# (PART) 線性迴歸 Linear Regression {-}

# 簡單線性迴歸 Simple Linear Regression

## 一些背景和術語

思考下面這些問題：


1. 脂肪攝入量增加，會導致體重增加嗎？
2. 兒童成年時的身高，可以用父母親的身高來預測嗎？
3. 如果其他條件都沒有變化，飲食習慣的改變，是否能影響血清膽固醇的水平？

上面的問題中，自變量 (預測變量)，和因變量 (反應量) 分別是什麼？

你可能還會碰到像下面這些稱呼，他們都是一個意思：

- 因變量  Dependent variable = 反應量 response variable = 結果變量 outcome variable;
- 自變量  independent variable = 預測變量 predictor variable = 解釋變量  explanatory variable = 共變量 covariate.


所有的非簡單統計模型 (non-trivial statistical models) 都包括以下三個部分：

1. 隨機變量 random variables：
    - 因變量永遠都是隨機變量；
    - 預測變量不一定是隨機變量；
    - 在相對簡單的模型中，我們討論的因變量和預測變量幾乎都來自於從人羣中抽取觀察樣本收集來的數據。

2. 人羣參數 population parameters：
    - 人羣參數，是我們希望通過收集樣本獲得的數據來估計 (estimate) 的參數。

3. 對不確定性的描述 representation of uncertainty：
    - 不確定性，意爲因變量的變動中，沒有被預測變量解釋的部分。


其他的術語問題：


- **單一因變量**的統計模型：**univariate model**;
- **多個因變量**的統計模型： **multivariate model**;
- **單一因變量**，含有**多個預測變量**的統計模型：**multivariable model**；
- 在線性迴歸中，單一因變量，單一預測變量的統計模型：**simple linear regression** (簡單線性迴歸)；
- 在線性迴歸中，單一因變量，多個預測變量的統計模型：**multiple linear regression** (多重線性迴歸)；

儘量避免將預測變量 (predictor variable) 寫作自變量 (independent variable)，因爲 "independent" 有自己的統計學含義 (獨立)。然而我們在線性迴歸中使用的預測變量，不一定都**互相獨立**，所以容易讓人混淆其意義。


## 簡單線性迴歸模型 simple linear regression model

即：**單一因變量，單一預測變量**的統計模型。


### 例1
下面的散點圖 \@ref(fig:age-wt) 展示的是一項橫斷面調查的結果，調查的是一些兒童的年齡 (月)，和他們的體重 (千克) 之間的關係。

```{r age-wt, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Age and weight of children in a cross-sectional survey', fig.align='center', out.width='80%'}
library(haven)
library(ggplot2)
library(ggthemes)
growgam1 <- read_dta("backupfiles/growgam1.dta")

ggplot(growgam1, aes(x=age, y=wt)) + geom_point(shape=20) +
  scale_x_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+
  scale_y_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) +
   theme_stata() +labs(x = "Age (Months)", y = "Weight (kg)")
```

### 例2

表 \@ref(tab:walk) 羅列的是11名兒童能夠自己獨立行走時的年齡。這些兒童在剛出生時被隨機分配到兩個組中 (積極鍛鍊走路，和對照組)。如果你熟悉均數比較，這樣的數據可以通過簡單 $t$ 檢驗來分析其均值的不同。但是實際上後面你會看到簡單 $t$ 檢驗和簡單線性迴歸是同一回事。


```{r walk, echo=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("backupfiles/walking.csv", header = T)
dt$Eight.Week.Control.Group..n.5.[6] <- " -- "
kable(dt, "html",  col.names = c("Active Exercise (n=6)","Eight Week Control (n=5)"), align = "c",caption = "Childen's ages at time of first walking aline by randomisation group") %>%
  kable_styling(bootstrap_options = c("striped", "bordered")) %>%
#  collapse_rows(columns = c(1)) %>%
  add_header_above(c("Age in months for walking alone" = 2))
```

## 區分因變量和預測變量

在簡單兩樣本 $t$ 檢驗中，我們不區分那兩個要比較的數據 $(X, Y)$。所以 $X$ 和 $Y$ 的關係，同分析 $Y$ 和 $X$ 的關係是一樣的。表 \@ref(tab:walk) 的例子中，視“直立行走的年齡”這一變量爲因變量十分直觀且自然。圖 \@ref(fig:age-wt) 的例子中我們顯然可以關心是否可以用兒童的年齡來推測他/她的體重。所以年齡被視爲預測變量 $(X)$，體重被視爲因變量或者叫結果變量 $(Y)$。

### 均值 (期待值) 公式 {#meanfunction}

圖 \@ref(fig:age-wt) 的例子中，當我們決定考察體重變化 $(Y)$ 和年齡的關係 $(X)$ 後，我們需要提出一個模型，來描述二者之間的關係。這個模型中，最重要的信息，是均值，或者叫期待值：

$$
E(Y|X=x), \text{ the expected value of } Y \text{ when } X \text{ takes the value } x
$$

在簡單線性迴歸模型中，我們認爲這個均值方程是線性關係：

$$
E(Y|X=x) = \alpha +\beta x
$$

所以這個線性關係中，有兩個參數 (parameters) 是我們關心的 $\alpha, \beta$。

- $\alpha$ 是截距 intecept。意爲當 $X$ 取 $0$時， $Y$ 的期待值大小；
- $\beta$ 是方程的斜率 slope。意爲當 $X$ 上升一個單位時，$Y$ 上升的期待值大小。

需要強調的是，這樣的線性模型，是我們提出，用來模擬真實數據時使用的。~~你如果作死~~當然還可以提出更加複雜的模型。如下面圖 \@ref(fig:age-wt-lm) 顯示的是線性迴歸直線， 而圖 \@ref(fig:age-wt-loess) 顯示的是較爲複雜的迴歸曲線。曲線方程可能更加擬合我們收集到的數據，然而這樣的連續的斜率變化很可能僅僅只解釋了這個樣本量數據，而不能解釋在人羣中年齡和體重的關係。





```{r age-wt-lm, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Linear mean function for age and weight of children in a cross-sectional survey', fig.align='center', out.width='80%'}
library(haven)
library(ggplot2)
library(ggthemes)
growgam1 <- read_dta("backupfiles/growgam1.dta")

ggplot(growgam1, aes(x=age, y=wt)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = lm, size = 0.3) +
  scale_x_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+
  scale_y_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) +
   theme_stata() +labs(x = "Age (Months)", y = "Weight (kg)")
```




```{r age-wt-loess, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Non-linear mean function for age and weight of children in a cross-sectional survey', fig.align='center', out.width='80%'}
library(haven)
library(ggplot2)
library(ggthemes)
growgam1 <- read_dta("backupfiles/growgam1.dta")

ggplot(growgam1, aes(x=age, y=wt)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = loess, se=T, size = 0.3) +
  scale_x_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+
  scale_y_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) +
   theme_stata() +labs(x = "Age (Months)", y = "Weight (kg)")
```

### 條件分佈和方差 the conditional distribution and the variance function

如果要完全明確一個統計模型，另一個重要的事情是，該模型能否準確描述因變量在預測變量的條件分佈 it is necessary to describe the ddistribution of the dependent variable conditional on the predictor variable。使用簡單線性迴歸模型有幾個前提假設，一是因變量對預測變量的條件分佈的方差是保持不變的 the variance of the dependent variable (conditional on the predictor variable) is constant。二是，這樣的條件分佈是一個正態分佈。有時候，這些假設條件並不能得到滿足。上面的散點圖 \@ref(fig:age-wt)看上去還算符合這兩個假設前提：在每一個年齡階段，體重的分佈沒有發生歪斜 (skew)，分散分佈 (方差) 也相對穩定。但是圖 \@ref(fig:diamond) 中的價格-克拉數據很明顯無法滿足上面的前提假設。在線性迴歸模型中，我們使用 $\sigma^2$ 表示殘差的方差 (residual variance)。

```{r diamond, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Relationship between diamond carat and price', fig.align='center', out.width='80%'}
library(ggplot2)
library(ggthemes)
ggplot(data=diamonds, aes(x=carat, y=price)) + geom_point(shape=20)+
  theme_stata()
```

### 定義簡單線性迴歸模型

用來描述一個隨機變量 $(Y)$ 和另一個變量 $(X)$ 之間關係的簡單線性迴歸模型，被定義爲：

$$
(Y|X=x) \sim N(\alpha+\beta x, \sigma^2)
$$

上面這個模型，十分清楚地描述了我們提出的模型，還有其中我們對數據的分佈的假設。同樣的模型，你可能更多得看到被寫成如下的方式：

$$
y=\alpha+\beta x+ \varepsilon \text{, where } \varepsilon\sim N(0,\sigma^2)
$$

假如，我們有一組樣本數據 $\underline{x}$， 樣本量爲 $n$。我們就可以把通過上面的迴歸模型實現的 $Y_i$ 和它對應的 $X_i (i=1,\cdots, n)$。描述爲如下的形式：

$$
\begin{equation}
  (Y_i|X_i=x_i) \sim NID(\alpha+\beta x, \sigma^2) \text{ where } i=1,\cdots,n
\end{equation}
  (\#eq:NID)
$$

此處的 $NID$ 意爲獨立且服從正態分佈 **(normally and independently distributed)**。這裏默認的一個重要前提是所有的觀察值 $X_i$ 是相互獨立互不影響的。例如上面圖 \@ref(fig:age-wt) 所示兒童的年齡和體重數據，就必須假設這些兒童都來自**沒有血緣關係的獨立家庭**。如果這以數據中的兒童，有些是兄弟姐妹的話，觀察數據互相獨立的前提就無法得到滿足。具體這類數據的分析方法會在 "Analysis of hierarchical and other dependent data" 中詳盡介紹。

公式 \@ref(eq:NID) 常被記爲：

$$
\begin{equation}
(Y_i|X_i=x_i) = \alpha + \beta x_i + \varepsilon_i, \text{ where } \varepsilon_i\sim NID(0,\sigma^2)
\end{equation}
 (\#eq:NID1)
$$

或者爲了簡潔表述寫成：

$$
\begin{equation}
y_i = \alpha + \beta x_i + \varepsilon_i, \text{ where } \varepsilon_i\sim NID(0,\sigma^2)
\end{equation}
 (\#eq:NID2)
$$

### 殘差 residuals

公式 \@ref(eq:NID1) 和 \@ref(eq:NID2) 其實已經包含了殘差的表達式：

$$
\varepsilon_i = y_i - (\alpha + \beta x_i)
$$

所以 $\varepsilon_i$ 的意義是第 $i$ 個觀察對象的隨機(偶然)誤差 (random error)，或者叫真實殘差 (true residual)。其實就是從線性迴歸計算獲得的估計值 $\alpha+\beta x_i$，和觀察值 $y_i$ 之間的差距大小。而且從其公式可見，殘差本身也是由人羣的參數 $(\alpha, \beta)$ 決定的。殘差也被定義爲迴歸模型的偏差值。當我們用樣本數據獲得的參數估計 $(\hat\alpha, \hat\beta)$ 來取代掉參數 $(\alpha, \beta)$ 時，這時的模型變成了估計模型，殘差也成了估計殘差或者叫觀察模型和觀察殘差。須和真實殘差加以區分。

## 參數的估計 estimation of parameters

簡單線性迴歸模型中有三個人羣參數 $(\alpha, \beta, \sigma^2)$。統計分析的目標，就是使用樣本數據 $Y_i, X_i, (i=1, \cdots, n)$ 來對總體參數做出推斷 (inference)。在線性迴歸中主要使用**普通最小二乘法 (ordinary least squares, OLS)** 作爲推斷的工具。在統計學中，我們習慣給希臘字母戴上“帽子”，作爲該參數的估計值，例如 $\hat\alpha, \hat\beta$ 是參數 $\alpha, \beta$ 的估計值。通過線性迴歸模型，給第 $i$ 個觀察值擬合的預測值，被叫做因變量的估計期望值 (estimated expectation)。用下面的式子來表示:


$$
\hat{y}_i=\hat\alpha+\hat\beta x_i
$$

此時，第 $i$ 名對象的觀察殘差 (observed or fitted or estimated residuals) 用下面的式子來表示：

$$
\hat{\varepsilon}_i = y_i-\hat{y}_i=y_i-(\hat\alpha+\hat\beta x_i)
$$

### 普通最小二乘法估計 $\alpha, \beta$

普通最小二乘法估計的 $\alpha, \beta$ 會最小化擬合迴歸直線的偏差 minimize the sum of squared deviations from the fitted regression line。其正式的定義爲：OLS估計值，指的是能夠使**殘差平方和 (residual sum of squares, $SS_{RES}$)**取最小值的 $\hat\alpha, \hat\beta$。

$$
\begin{equation}
SS_{RES} = \sum_{i=1}^n \hat{\varepsilon}^2_i = \sum_{i=1}^n (y_i-\hat\alpha-\hat\beta x_i)^2
\end{equation}
(\#eq:ssres)
$$

可以證明的是，OLS的 $\alpha, \beta$ 估計值的計算公式爲：

$$
\begin{equation}
\hat\alpha=\bar{y}-\hat\beta\bar{x}
(\#eq:hatalpha)
\end{equation}
$$

$$
\begin{equation}
\hat\beta=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
(\#eq:hatbeta)
\end{equation}
$$

其中 $\bar{y}=\frac{\sum_{i=1}^ny_i}{n}, \bar{x}=\frac{\sum_{i=1}^nx_i}{n}$

**證明**

求能最小化 $SS_{RES}$ 的 $\alpha$， 我們需要把公式 \@ref(eq:ssres) 對 $\hat\alpha$ 求導，然後將求導之後的式子等於 $0$ 之後求根即可：

$$
\begin{aligned}
& \frac{\text{d}SS_{RES}}{\text{d}\hat\alpha} =\sum_{i=1}^n -2(y_i-\hat\alpha-\hat\beta x_i) = 0\\
& \text{Since } \sum_{i=1}^n(y_i) = n\bar{y}; \sum_{i=1}^n (x_i) =n\bar{x} \\
& \Rightarrow -n\bar{y}+n\hat\alpha+n\hat\beta\bar{x} = 0 \\
& \Rightarrow \hat\alpha = \bar{y}-\hat\beta\bar{x}
\end{aligned}
$$

求能最小化 $SS_{RES}$ 的 $\beta$，求導之前我們先把公式 \@ref(eq:ssres) 中含有 $\hat\alpha$ 的部分替換掉：

$$
\begin{equation}
\begin{split}
SS_{RES} &= \sum_{i=1}^n\hat\varepsilon_i^2=\sum_{i=1}^n(y_i-(\bar{y}-\hat\beta\bar{x})-\hat\beta x_i)^2\\
         &= \sum_{i=1}^n((y_i-\bar{y})-\hat\beta(x_i-\bar{x}))^2 \\
\end{split}
(\#eq:ssres-rearrange)
\end{equation}
$$

接下來對上式 \@ref(eq:ssres-rearrange) 求導之後，用相同辦法求根：

$$
\begin{aligned}
&\frac{\mathrm{d} SS_{RES}}{\mathrm{d} \hat\beta} = \sum_{i=1}^n -2(x_i-\bar{x})(y_i-\bar{y}) + 2\hat\beta(x_i-\bar{x})^2 = 0\\
& \Rightarrow \hat\beta\sum_{i=1}^n(x_i-\bar{x})^2 = \sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) \\
& \hat\beta=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}
\end{aligned}
$$

這兩個式子 \@ref(eq:hatalpha) \@ref(eq:hatbeta) 同時也是參數 $\alpha \beta$ 的極大似然估計 (MLE)。

## 殘差方差的估計 Estimation of the residual variance $(\sigma^2)$

殘差方差等於殘差平方和除以樣本量。所以我們會把殘差方差的估計用下面的式子表示：

$$
\begin{equation}
\hat\sigma^2=\sum_{i=1}^n \frac{\hat\varepsilon^2}{n} = \sum_{i=1}^n \frac{(y_i-\hat\alpha-\hat\beta x_i)^2}{n}
\end{equation}
(\#eq:sigma2wrong)
$$

這的確是 $\sigma^2$ 的極大似然估計 (MLE)。然而我們知道，公式 \@ref(eq:sigma2wrong) 並不是殘差方差的無偏估計。類似與樣本方差低估了總體方差 (Section \@ref(samplevarbias))，那樣，這裏殘差方差的觀察值也是低估了總體殘差方差的。所以，殘差方差的無偏估計需要用下面的式子來校正：

$$
\begin{equation}
\hat\sigma^2=\sum_{i=1}^n \frac{\hat\varepsilon^2}{n-2} = \sum_{i=1}^n \frac{(y_i-\hat\alpha-\hat\beta x_i)^2}{n-2}
\end{equation}
(\#eq:sigma2right)
$$

公式 \@ref(eq:sigma2right) 被叫做殘差均方 (Residual Mean Squares, RMS)，常常被標記爲 $\text{MS_{RES}}$。分母的 $n-2$，表示進行殘差方差估計時用掉了兩個信息量 $\alpha, \beta$ (自由度減少了 2)，

## R 演示  例 1： 圖 \@ref(fig:age-wt) 數據


```{r }
library(haven)
growgam1 <- read_dta("backupfiles/growgam1.dta")

slm <- lm(wt~age, data=growgam1)

summary(slm) # basic default output of the summary
print(anova(slm), digits = 8) # show the sum of squares for the fitted model and residuals
```
也可以用 `stargazer` 包輸出很酷的表格報告：

```{r results='asis', message=FALSE}
library(stargazer)
stargazer(slm, type = "html")
```
其實結果都一樣。我們這裏詳細來看 $\alpha, \beta, \sigma^2$：

$\hat\alpha = 6.84$：當年齡爲 $0$ 時，體重爲 $6.84 kg$。本數據 \@ref(fig:age-wt) 中並沒有 $0$ 歲的兒童，所以這裏的截距的解釋需要非常小心是否合理。

$\hat\beta = 0.165$：這數據中兒童的體重估計隨着年齡升高 $1$ 個月增長 $0.165 kg$。所以使用這兩個估計值我們就可以來估計任意年齡時兒童的體重。圖 \@ref(fig:age-wt-lm) 就是擬合數據以後的簡單線性迴歸曲線。

$\hat\sigma^2 = 1.62, \hat\sigma=1.27$  就是默認輸出中最下面的 `Residual standard error: 1.274` 和 ANOVA 表格中 Residuals 的 `Mean Sq=1.62184` 部分。含義是，沿着擬合的直線，在每一個給定的年齡上兒童體重的分佈的標準差是 $1.27 kg$。

## R 演示 例 2： 表\@ref(tab:walk) 數據 {#binarylms}

如果在 `Stata` 聽說你還需要自己生成啞變量 (dummy variables) (應該是計算時，在想要變成啞變量的變量名前面加上 `i.`)。在 [R](https://www.r-project.org/) 裏面，分類變量被設置成因子 "factor" 時，你就完全可以忽略生成啞變量的過程。下圖 \@ref(fig:age-walk) 顯示了兩組兒童直立行走時的年齡。

```{r age-walk, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Age at walking by group', fig.align='center', out.width='80%', warning=FALSE}
library(ggplot2)
library(ggthemes)
dt <- read.csv("backupfiles/walking.csv", header = T)
age1 <- dt$Active.Exercise.Group..n.6.
age2 <- dt$Eight.Week.Control.Group..n.5.
Group <- c(rep("exercise", 6), rep("control", 6))
Walk <- data.frame(c(age1,age2), Group)
names(Walk)[1] <- "Age"
## Reordering Group
Walk$Group <- factor(Walk$Group,levels=c("exercise", "control"))

ggplot(Walk, aes(x=Group, y=Age)) + geom_point() +
  scale_y_continuous(breaks = seq(9, 14, 1),limits = c(9,14)) +
   theme_stata() +labs(x = "Randomised groups", y = "Age (Months)")
```

擬合簡單線性迴歸也是小菜一碟：

```{r}
wk_age <- lm(Age ~ Group, data=Walk)

summary(wk_age)
anova(wk_age)
```

這裏的 $\hat\alpha=10.125$，意爲參照組 (此處，"exercise" 被默認設定爲參照組，而 "control" 被默認拿來和參照組相比較) 的兒童也就是，積極練習走路的小朋友這組能夠獨立行走的平均年齡是 $10.125$ 個月。

$\hat\beta=2.225$，意爲和參照組 (積極練習組) 相比，對照組兒童能夠自己行走的年齡平均要晚 $2.225$ 個月。所以對照組兒童能夠直立行走的平均年齡就是 $10.125+2.225=12.35$ 個月。

上述結果，你如果拿來和下面的兩樣本 $t$ 檢驗的結果相比就知道，是完全一致的。其中統計量 $t^2=2.9285^2=F_{1,9}=8.58$。
```{r}
t.test(Age~Group, data=Walk, var.equal=TRUE)
```

## 練習 {#exeChol}

使用的數據內容爲：兩次調查同一樣本，99 名健康男性的血清膽固醇水平，間隔一年。


```{r message=FALSE, warning=FALSE, fig.align='center', out.width='80%'}
# 數據讀入
library(haven)

Chol <- read_dta("backupfiles/chol.dta")
summary(Chol)

# Alternative Descriptive Statistics using psych package
library(psych)
describe(Chol)

# 兩次膽固醇水平的直方圖 Distribution of the two measures
par(mfrow=c(1,2))
hist(Chol$chol1)
hist(Chol$chol2)

# 對兩次膽固醇水平作散點圖
ggplot(Chol, aes(x=chol1, y=chol2)) + geom_point(shape=20) +
  scale_x_continuous(breaks=seq(150, 400, 50),limits = c(150, 355))+
  scale_y_continuous(breaks=seq(150, 400, 50),limits = c(150, 355)) +
   theme_stata() +labs(x = "Cholesterol at visit 1 (mg/100ml)", y = "Cholesterol at visit 2 (mg/100ml)")
```

### 兩次測量的膽固醇水平分別用 $C_1, C_2$ 來標記的話，考慮這樣的簡單線性迴歸模型：$C_2=\alpha+\beta C_2 + \varepsilon$。我們進行這樣迴歸的前提假設有哪些？

- 每個觀察對象互相獨立。
- 前後兩次測量的膽固醇水平呈線性相關。
- 殘差值，在每一個給定的 $C_1$ 值處呈現正態分佈，且方差不變。

從散點圖來看這些假設應該都能得到滿足。

```{r}
# 計算兩次膽固醇水平的 均值，方差，以及二者的協方差
mean(Chol$chol1); mean(Chol$chol2)
var(Chol$chol1); var(Chol$chol2)
cov(Chol$chol1, Chol$chol2)
```

### 計算普通最小二乘法 (OLS) 下，截距和斜率的估計值 $\hat\alpha, \hat\beta$

$$
\begin{aligned}
\hat\beta &= \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}\\
&=\frac{\text{Cov}(C_1,C_2)}{\text{Var}(C_1)}\\
&=\frac{1661.061}{961.224}=0.578
\end{aligned}
$$

```{r}
cov(Chol$chol1, Chol$chol2)/var(Chol$chol1)
```

$$\hat\alpha=\bar{y}-\hat\beta\bar{x}=263.54-0.578\times264.59=110.425$$

```{r}
mean(Chol$chol2)-mean(Chol$chol1)*cov(Chol$chol1, Chol$chol2)/var(Chol$chol1)
```
### 和迴歸模型計算的結果作比較，解釋這些估計值的含義

```{r}
summary(lm(chol2~chol1, data=Chol))
```

- 截距的估計值是 110.4 mg/100ml: 意爲這組樣本，第一次採集數據時，膽固醇水平的平均值是 110.4。
- 斜率的估計值是 0.58：意爲第一次採集的膽固醇水平每高 1 mg/100ml，那麼第二次採集的膽固醇相應提高的值的期待量爲 0.58.

### 加上計算的估計值直線 (即迴歸直線)

```{r warning=FALSE}
ggplot(Chol, aes(x=chol1, y=chol2)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = lm, se=FALSE, size=0.5) +
   scale_x_continuous(breaks=seq(150, 400, 50),limits = c(150, 355))+
  scale_y_continuous(breaks=seq(150, 400, 50),limits = c(150, 355)) +
   theme_stata() +labs(x = "Cholesterol at visit 1 (mg/100ml)", y = "Cholesterol at visit 2 (mg/100ml)")
```

可以注意到，第一次訪問時膽固醇水平高的人，第二次被測量時膽固醇值高於平均值，但是卻沒有第一次高出平均值的部分多。
相似的，第一次膽固醇水平低的人，第二次膽固醇水平低於平均值，但是卻沒有第一次低於平均值的部分多。這一現象被叫做 “向均數迴歸-regression to the mean”


### 下面的代碼用於模型的假設診斷
```{r}
M <- lm(chol2~chol1, data=Chol)
par(mfrow = c(2, 2))  # Split the plotting panel into a 2 x 2 grid
plot(M)
```


好心人在 [github](https://gist.github.com/atyre2/ff4e1ec24e42adda8dbd43cda99d6282) 上共享了 `Check_assumption.R` 的代碼，可以使用 ggplot2 來獲取高逼格的模型診斷圖：

```{r message=FALSE}
source("checkassumptions.R")
check_assumptions(M)
```


# 最小二乘估計的性質和推斷 Ordinary Least Squares Estimators and Inference


前一章介紹了簡單線性迴歸模型中對總體參數 $\alpha, \beta, \sigma^2$ 的估計公式，分別是 \@ref(eq:hatalpha) \@ref(eq:hatbeta) \@ref(eq:sigma2right)。本章繼續介紹他們的統計學性質。下面的標記和統計量也會被用到：

1. $\bar{y}=\frac{\sum_{i=1}^n y_i}{n}$，因變量 $y$ 的樣本均值；
2. $\bar{x}=\frac{\sum_{i=1}^n x_i}{n}$，預測變量 $x$ 的樣本均值；
3. $SS_{yy}=\sum_{i=1}^n(y_i-\bar{y})^2$，因變量 $y$ 的校正平方和；
4. $SS_{xx}=\sum_{i=1}^n(x_i-\bar{x})^2$，預測變量 $x$ 的校正平方和；
5. $SD_y^2=\frac{\sum_{i=1}(y_i-\bar{y})^2}{n-1}=\frac{SS_{yy}}{n-1}$，因變量 $y$ 的樣本方差；
6. $SD_x^2=\frac{\sum_{i=1}(x_i-\bar{x})^2}{n-1}=\frac{SS_{xx}}{n-1}$，因變量 $y$ 的樣本方差；
7. $S_{xy}=\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})$，$x,y$ 的交叉乘積；
8. $CV_{xy}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{n-1}=\frac{S_{xy}}{n-1}$，樣本協方差；
9. $r_{xy}=\frac{CV_{xy}}{SD_xSD_y}$，$x,y$ 的樣本相關係數；
10. $SS_{RES}=\sum_{i=1}^n\hat\varepsilon^2=\sum_{i=1}^n(y_i-\hat\alpha-\hat\beta x_i)^2$，殘差的估計平方和。

## OLS 估計量的性質

1. 樣本估計的迴歸直線必定穿過數據的中心 $(\bar{x},\bar{y})$。

**證明**

由於樣本估計的截距和斜率公式  \@ref(eq:hatalpha) \@ref(eq:hatbeta) 可知：

$$
\begin{aligned}
\hat\alpha &= \bar{y} - \hat\beta\bar{x} \\
 \hat y_i  &= \hat\alpha + \hat\beta x_i \\
 \Rightarrow \hat y_i &= \bar{y}+\hat\beta(x_i-\bar{x})
\end{aligned}
(\#eq:lmcenter)
$$

所以，當 $\hat x_i=\bar{x}$ 時 $\hat y_i=\bar{y}$。即迴歸直線必然穿過中心點。

2. 如果擬合模型是正確無誤的， $\hat\alpha,\hat\beta,\hat\sigma^2$ 分別是各自的無偏估計。
3. $\hat\alpha, \hat\beta$ 是極大似然估計， $\hat\sigma^2$ 不是MLE。
4. $\hat\alpha, \hat\beta$ 是 $\alpha, \beta$ 最有效的估計量。

## $\hat\beta$ 的性質

$$
\begin{equation}
\hat\beta=\frac{S_{xy}}{SS_{xx}}=\frac{CV_{xy}}{SD_x^2}
\end{equation}
(\#eq:hatbetaalt)
$$

### $Y$ 對 $X$ 迴歸， 和 $X$ 對 $Y$ 迴歸 {#randbeta}

如果我們使用 $\hat\beta_{y|x}$ 表示預測變量 $x$，因變量 $y$ 的簡單線性迴歸係數，那麼我們就有：

$$
\begin{equation}
\hat\beta_{y|x} = \frac{CV_{xy}}{SD_x^2}  \text{ and } \hat\beta_{x|y} = \frac{CV_{xy}}{SD_y^2} \\
\text{Hence, } \hat\beta_{y|x}\hat\beta_{x|y} = r^2_{xy}
\end{equation}
(\#eq:r2)
$$

公式 \@ref(eq:r2) 也證明了：如果兩個變量相關係數爲 $1$ (100% 相關)， $Y$ 對 $X$ 迴歸的迴歸係數，是 $X$ 對 $Y$ 迴歸的迴歸係數的倒數。

### 例 1： 還是圖 \@ref(fig:age-wt) 數據

```{r}
library(haven)
growgam1 <- read_dta("backupfiles/growgam1.dta")

# regress wt on age
summary(lm(wt~age, data=growgam1))
print(anova(lm(wt~age, data=growgam1)), digits = 8)
# regress age on wt
summary(lm(age~wt, data=growgam1))
print(anova(lm(age~wt, data=growgam1)), digits = 8)
```

可以看到二者的輸出結果中統計檢驗量一樣，但是一個是將體重針對年齡迴歸，另一個則是反過來，所以迴歸係數和截距都不同。迴歸方程的含義也就發生了變化。如果把兩條迴歸曲線同時作圖可以更加直觀：


```{r age-wt-lm1, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Simple linear regression model line relating weight to age', fig.align='center', out.width='80%'}
library(haven)
library(ggplot2)
library(ggthemes)
growgam1 <- read_dta("backupfiles/growgam1.dta")

ggplot(growgam1, aes(x=age, y=wt)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = lm) +
  scale_x_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+
  scale_y_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) +
   theme_stata() +labs(x = "Age (Months)", y = "Weight (kg)")
```

```{r wt-age-lm, warning=FALSE, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Simple linear regression model line relating age to weight', fig.align='center', out.width='80%'}

ggplot(growgam1, aes(x=wt, y=age)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = lm) +
  scale_y_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+
  scale_x_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) +
   theme_stata() +labs(y = "Age (Months)", x = "Weight (kg)")+
coord_flip()
```

## 截距和迴歸係數的方差，協方差

假如簡單線性迴歸模型是正確的，那麼截距 $\hat\alpha$ 和迴歸係數 $\hat\beta$ 的方差分別是：

$$
\begin{equation}
V(\hat\alpha) = \sigma^2(\frac{1}{n}+\frac{\bar{x}^2}{SS_{xx}}) = \frac{\sigma^2}{(n-1)} (1-\frac{1}{n}+\frac{\bar{x}^2}{SD_x^2})
\end{equation}
(\#eq:varhatalpha)
$$


$$
\begin{equation}
V(\hat\beta) = \frac{\sigma^2}{SS_{xx}}=\frac{\sigma^2}{(n-1)SD_x^2}
\end{equation}
(\#eq:varhatbeta)
$$

從公式 \@ref(eq:varhatalpha) 和 \@ref(eq:varhatbeta) 也可以看出，兩個估計量的方差隨着殘差方差的增加而增加 (估計不精確)，隨着樣本兩的增加而減少 (估計更精確)。截距 $\hat\alpha$ 的方差會隨着樣本均值的增加而增加。

通常來說，截距和迴歸係數二者之間並非相互獨立。他們的協方差爲：

$$
\begin{equation}
Cov(\hat\alpha,\hat\beta) = -\frac{\sigma^2\bar{x}}{SS_{xx}}
\end{equation}
(\#eq:covaralphabeta)
$$

上面的公式 \@ref(eq:varhatalpha) \@ref(eq:varhatbeta) \@ref(eq:covaralphabeta) 都包含了真實的殘差方差 $\sigma^2$。這個量對於我們“人類”來說是未知的。

### 中心化 centring

簡單線性迴歸模型常用的一個技巧是將預測變量中心化。即，求預測變量的均值，然後將每個觀測值減去均值之後再用這個新的預測變量擬合簡單線性迴歸模型。這樣做其實完全不影響回顧係數，卻會影響截距的大小。此時新的迴歸直線的截距，就等於因變量 (體重) 的均值。

用圖 \@ref(fig:age-wt) 數據來解釋：

```{r}
# mean value of age
mean(growgam1$age)
growgam1$age_cen <- growgam1$age-mean(growgam1$age)
# regress wt on age
print(summary(lm(wt~age, data=growgam1)), digit=5)
print(summary(lm(wt~age_cen, data=growgam1)), digit=5)
```

很明顯，結果顯示中心化不會改變迴歸係數，也不會改變它的方差。但是“新”的截距，其實就等於因變量 (體重) 的均值。而且很多數據都集中在這個均值附近，因而，截距的方差比沒有中心化的迴歸方程要小。

## $\alpha, \beta$ 的推斷

$\hat\alpha, \hat\beta$ 都可以被改寫成關於因變量 $Y$ 的方程，因此同時也是隨機誤差的方程式：

$$
\begin{aligned}
\hat\beta &= \sum_{i=1}^n[\frac{(x_i-\bar{x})}{SS_{xx}}(y_i-\bar{y})] \\
\text{Substituting } &(y_i-\bar{y}) = \beta(x_i-\bar{x})+(\varepsilon_i-\bar{\varepsilon}) \\
          &= \beta + \sum_{i=1}^n[\frac{x_i-\bar{x}}{SS_{xx}}(\varepsilon_i-\bar{\varepsilon})]
\end{aligned}
$$

又因爲，$\varepsilon_i \sim NID(0,\sigma^2)$，估計量 $\hat\alpha, \hat\beta$ 均爲 $\varepsilon_i$ 的線性轉換，所以他們也都是服從正態分佈的。

### 對迴歸係數進行假設檢驗

對於迴歸係數 $\beta$，我們可以使用 Wald statistic (Section \@ref(Wald)) 進行零假設爲 $\text{H}_0: \beta=0$ 的假設檢驗。此時，替代假設爲 $\text{H}_1: \beta\neq0$。最佳檢驗統計量爲：

$$
\begin{equation}
t = \frac{\hat\beta-0}{SE(\hat\beta)} \\
\end{equation}
(\#eq:betattest)
$$

根據公式 \@ref(eq:varhatbeta) $SE(\hat\beta) = \sqrt{V(\hat\beta)} = \frac{\hat\sigma}{\sqrt{SS_{xx}}}$。用 $\hat\sigma^2$ 替換掉公式 \@ref(eq:varhatbeta) 中的 $\sigma^2$，意味着迴歸係數的檢驗統計量 $t$ 服從自由度爲 $n-2$ 的 $t$ 分佈。之後就可以根據 $t$ 分佈的性質求相應的 $p$ 值了，對相關係數是否爲 $0$ 進行檢驗。之所以我們可以在這裏使用 Wald 檢驗，是因爲前提條件：隨機誤差服從正態分佈，於是 $\beta$ 的對數似然比也是左右對稱的，當對數似然比的圖形左右對稱時，就可以使用二次方程來近似 (Wald 檢驗的實質)。

### 迴歸係數，截距的信賴區間

估計量 $\beta$ 的 $95\%$ 信賴區間的計算公式如下：

$$
\begin{equation}
\hat\beta \pm t_{n-2,0.975}SE(\hat\beta)
\end{equation}
(\#eq:CIbeta)
$$

其中，$t_{n-2, 0.975}$ 表示自由度爲 $n-2$ 的 $t$ 分佈的 $97.5\%$ 位點的值。繼續使用之前的實例，圖 \@ref(fig:age-wt) 中的數據。體重對年齡進行簡單線性迴歸之後，年齡的估計回顧係數 $\hat\beta=0.165, SE(\hat\beta)=0.0111$, 此例中 $n=190$，所以 $t_{188, 0.975}=1.973$。所以迴歸係數的 $95\%$ 信賴區間可以如此計算：$0.165\pm1.973\times0.0111=(0.143, 0.187)$。

類似的，估計截距 $\hat\alpha$ 的 $95\%$ 信賴區間的計算式便是： $\hat\alpha \pm t_{n-2, 0.975}SE(\hat\alpha)$。同樣的例子裏，$\hat\alpha=6.838, SE(\hat\beta) = 0.210, t_{188, 0.975}=1.973$。所以截距的 $95\%$ 信賴區間的計算方法就是： $6.838\pm1.973\times0.210=(6.42, 7.25)$

跟下面 R 計算的完全一樣：

```{r}
confint(lm(wt~age, data=growgam1))
```

### 預測值的信賴區間 (置信带) - 测量回归曲线本身的不确定性

這裏所謂的“預測值”其實並沒有拿來預測什麼新的數值，而是說我們希望通過線性迴歸找到因變量真實值的存在區間 (信賴區間)。所以這個預測值的真實含義其實應該是在預測變量取 $X=x$ 時，因變量的期待值，$E(Y|X=x)$。

這個預測值的方差公式如下：

$$
\begin{equation}
V(\hat y_{x}) = \sigma^2[\frac{1}{n}+\frac{(x_i-\bar{x})^2}{SS_{xx}}]
\end{equation}
(\#eq:predictvar)
$$

於是可以計算它的 $95\%$ 信賴區間公式是：

$$
\begin{equation}
\hat y_x \pm t_{n-2, 0.975} \hat\sigma \sqrt{[\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}}]}
\end{equation}
(\#eq:predictCI)
$$

其實在之前的圖 (圖 \@ref(fig:age-wt-lm)) 我們也已經展示過這個信賴區間的範圍。


### 预测带 Reference range - 包含了 95% 观察值的区间

此處的 $95\%$ 預測帶，其實是包含了 $95\%$ 觀察數據的區間。所以預測帶要比置信帶更寬。它的方差計算公式爲：

$$
\begin{equation}
V(\hat y_x)+\sigma^2 = \sigma^2[1+\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}}]
\end{equation}
(\#eq:refrangevar)
$$

區間計算公式爲：

$$
\begin{equation}
\hat{y}_x \pm t_{n-2, 0.975} \sqrt{1+\frac{1}{n}+\frac{(x-\bar{x})^2}{SS_{xx}}}
\end{equation}
(\#eq:refrangeCI)
$$


將置信帶和預測帶同時展現則如下圖：

```{r age-wt-lm-pred, fig.asp=.7, fig.width=4, fig.cap='Simple linear regression for age and weight of children in a cross-sectional survey with 95% CI of predicted values and 95% reference range', warning=FALSE, fig.align='center', out.width='80%'}
library(haven)
library(ggplot2)
library(ggthemes)
growgam1 <- read_dta("backupfiles/growgam1.dta")


Model <- lm(wt~age, data=growgam1)
temp_var <- predict(Model, interval="prediction")

new_df <- cbind(growgam1, temp_var)


ggplot(new_df, aes(x=age, y=wt)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = lm, se=FALSE, size = 0.3) +
  geom_line(aes(y=lwr), color = "red", linetype = "dashed")+
    geom_line(aes(y=upr), color = "red", linetype = "dashed")+
  scale_x_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+
  scale_y_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) +
   theme_stata() +labs(x = "Age (Months)", y = "Weight (kg)")
```


## 線性迴歸模型和 Pearson 相關係數 {#rsquare}

前面也推導過線性迴歸係數和 Pearson 相關係數之間的關係 (Section \@ref(randbeta))，這裏詳細再展開討論它們之間關係的另外兩個重要結論。

### $r^2$ 可以理解爲因變量平方和被模型解釋的比例

Pearson 相關係數，因變量的平方和，模型的殘差平方和之間有如下的關係：

$$
\begin{equation}
r^2 = \frac{SS_{yy}-SS_{RES}}{SS_{yy}} = 1-\frac{SS_{RES}}{SS_{yy}}
\end{equation}
(\#eq:rSSyySSres)
$$

**證明**

$$
\frac{SS_{RES}}{SS_{yy}} = \frac{\sum_{i=1}^n(y_i-\hat\alpha-\hat\beta x_i)^2}{\sum_{i=1}^n(y_i-\bar{y})^2}
$$

因爲 \@ref(eq:hatalpha) : $\hat\alpha=\bar{y}-\hat{\beta}\bar{x}$

$$
\begin{aligned}
\frac{SS_{RES}}{SS_{yy}} &= \frac{\sum_{i=1}^n[(y_i-\bar{y})-\hat\beta(x_i-\bar{x})]^2}{\sum_{i=1}^n(y_i-\bar{y})^2} \\
                  &=\frac{\sum_{i=1}^n(y_i-\bar{y})^2}{\sum_{i=1}^n(y_i-\bar{y})^2}-\frac{2\hat\beta\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(y_i-\bar{y})^2}+\frac{\hat\beta^2\sum_{i=1}^n(x_i-\bar{x})^2}{\sum_{i=1}^n(y_i-\bar{y})^2}\\
                  &=1-\frac{2\hat\beta SS_{xy}}{SS_{yy}} + \frac{\hat\beta^2SS_{xx}}{SS_{yy}}
\end{aligned}
$$

又因爲 $\hat\beta=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{S_{xy}}{SS_{xx}}, r^2=\frac{S_{xy}^2}{SS_{xx}SS_{yy}}$。

$$
\begin{aligned}
\frac{SS_{RES}}{SS_{yy}} &= 1-\frac{2S_{xy}^2}{SS_{yy}SS_{xx}}+\frac{S_{xy}^2}{SS_{xx}SS_{yy}}\\
&=1-2r^2+r^2\\
&=1-r^2\\
\Rightarrow r^2&=1-\frac{SS_{RES}}{SS_{yy}}
\end{aligned}
$$

因此，這裏就引出了非常重要的一個結論，**Pearson 相關係數的平方 $r^2$ 的統計學含義是，因變量的平方和 $SS_{yy}$ 中，模型的預測變量能夠解釋的部分 $1-SS_{RES}$ 的百分比。** 統計學結果的報告中，爲了和一般相關係數的意義區分，會用大寫的 $R^2$ 來表示這個模型解釋了因變量的百分比。(Section \@ref(Rsquare))

## Pearson 相關係數和模型迴歸係數的檢驗統計量 $t$ 之間的關係 {#t-r2-F}

$$
\begin{equation}
t=r\sqrt{\frac{n-2}{1-r^2}}
\end{equation}
(\#eq:t-r2)
$$

**證明**

由於前面推導的 $r^2$ 公式 \@ref(eq:rSSyySSres)，而且 $r^2=\frac{S_{xy}^2}{SS_{xx}SS_{yy}}$：

$$
\begin{aligned}
\frac{r^2}{1-r^2} & = \frac{\frac{S_{xy}^2}{SS_{xx}SS_{yy}}}{\frac{SS_{RES}}{SS_{yy}}} \\
                  & = \frac{S_{xy}^2}{SS_{xx}SS_{RES}} \\
                  & = \frac{S_{xy}^2}{SS_{xx}(n-2)\hat\sigma^2}
\end{aligned}
$$

由於公式 \@ref(eq:varhatbeta)，所以 $\hat\sigma^2=V(\hat\beta)SS_{xx}$

$$
\begin{aligned}
\frac{r^2}{1-r^2} & = \frac{S_{xy}^2}{SS^2_{xx}(n-2)V(\hat\beta)} \\
                  & = \frac{\hat\beta^2}{(n-2)V(\hat\beta)} \\
\Rightarrow t=r\sqrt{\frac{n-2}{1-r^2}}
\end{aligned}
$$

這個結論也被用於相關係數的假設檢驗。而且也正如 Section \@ref(randbeta) 證明過的那樣，在簡單線性迴歸裏因變量和預測變量的位置對調以後，對於回顧係數是否爲零的檢驗統計量不受影響。

## 練習

數據同前一章練習部分數據相同 \@ref(exeChol)：

```{r message=FALSE, warning=FALSE, fig.align='center', out.width='80%'}
# 數據讀入
library(haven)
library(ggplot2)
library(ggthemes)
Chol <- read_dta("backupfiles/chol.dta")
Model <- lm(chol2~chol1, data=Chol)
print(summary(Model), digit=6)
print(anova(Model), digit=6)

# 計算截距和迴歸係數的 P 值 HAND CALCULATIONS twosided p-value in R can be obtained by pt(t, df) function

## p value for intercept:

110.42466/20.01133 #=5.518107

2*pt(5.518107, 97, lower.tail = FALSE)

## p value for beta:

0.57868/0.07476 #= 7.740503

2*pt(7.740503, 97, lower.tail = FALSE)

# add fitted regression lines 95% CIs and reference range
temp_var <- predict(Model, interval="prediction")

new_df <- cbind(Chol, temp_var)

ggplot(new_df, aes(x=chol1, y=chol2)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = lm, se=TRUE, size=0.5)  +
  geom_line(aes(y=lwr), color = "red", linetype = "dashed")+
    geom_line(aes(y=upr), color = "red", linetype = "dashed")+
   scale_x_continuous(breaks=seq(150, 400, 50),limits = c(150, 355))+
  scale_y_continuous(breaks=seq(150, 400, 50),limits = c(150, 355)) +
   theme_stata() +labs(x = "Cholesterol at visit 1 (mg/100ml)", y = "Cholesterol at visit 2 (mg/100ml)")
```

圖中可見，95% 置信帶變化顯著，距離均值越遠的地方，置信帶越寬。然而預測帶基本是平行的沒有變化。因爲預測帶的涵義是，95%的觀察數據都在這個區間範圍內。




# 方差分析 Introduction to Analysis of Variance


## 背景

當我們用統計模型模擬真實數據的時候，我們常常會被問到這樣的問題：“兩個模型哪個能更好的擬合這個數據？”

本章我們先考慮簡單的情況，兩個模型互相比較時，其中一個稍微簡單些的模型使用的預測變量，同時也是另一個較複雜的模型的預測變量 (nested models)。所以，複雜模型的預測變量較多，而其中一個或者幾個預測變量又構成了新的較爲簡單的模型。這兩個模型之間的比較，就需要用到方差分析 Analysis of Variance (ANOVA)。

此處方差分析的原則是：如果複雜模型能夠更好的擬合真實實驗數據，那我們會認爲簡單模型無法解釋的大量殘差平方和，有效地被複雜模型解釋了。所以，這一原則下，可以推理，複雜模型計算獲得的殘差平方和，會顯著地小於簡單模型計算獲得的殘差平方和。ANOVA 就提供了這個殘差平方和變化的定量比較方法。

## 簡單線性迴歸模型的方差分析

其實從線性迴歸的第一章節開始，我們都在使用方差分析的思想。圖 \@ref(fig:age-wt) 數據的迴歸模型中，我們其實比較了以下兩個模型：

1. 零假設模型：null model, 即認爲年齡和體重之間沒有任何關係 (水平直線)；
2. 替代模型： alternative model, 認爲年齡和體重之間有一定的線性關係 (擬合後的直線)。



```{r age-wt-lm-anova, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='NULL (red) and Alternative models (blue) for the data', fig.align='center', out.width='80%'}
library(haven)
library(ggplot2)
library(ggthemes)
growgam1 <- read_dta("backupfiles/growgam1.dta")

ggplot(growgam1, aes(x=age, y=wt)) + geom_point(shape=20, colour="grey40") +
  stat_smooth(method = lm, se=FALSE, size = 0.3) +
     geom_hline(yintercept = 10, colour = "red", size=0.3) +
  scale_x_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+
  scale_y_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) +
   theme_stata() +labs(x = "Age (Months)", y = "Weight (kg)")
```

### 兩個模型的參數估計

無論是零假設模型，還是替代假設模型，都需要通過最小化殘差來獲得其參數估計：

$$
SS_{RES} = \sum_{i=1}^n \hat\varepsilon^2= \sum_{i=1}^n(y_i-\hat y_i)^2
$$

替代假設模型，在線性迴歸第一部分 (Section \@ref(meanfunction)) 已經提到過，均值方程是 $E(Y|X=x) = \alpha+\beta x$，且這個方程的參數 $\alpha, \beta$ 以及殘差方差 $\sigma^2$ 的估計值計算公式也已經推導完成 \@ref(eq:hatalpha) \@ref(eq:hatbeta) \@ref(eq:sigma2right)。

零假設模型，它的均值方程是 $E(Y|X=x)=\alpha$。所以需要將它的殘差最小化：

$$
SS_{RES} = \sum_{i=1}^n(y_i-\hat\alpha)^2
$$

由於 \@ref(eq:hatalpha) ：$\hat\alpha=\bar{y}-\hat\beta$，所以 $\hat\alpha = \bar{y}$。

所以對於零假設模型來說：

$$
SS_{RES} = \sum_{i=1}^n(y_i-\bar{y})^2 =SS_{yy}
$$

因此，沒有預測變量的零假設模型，它的殘差平方和，就等於因變量的平方和。

### 分割零假設模型的殘差平方和

ANOVA，方差分析的原則，其實就是將較簡單模型 (零假設模型) 的殘差平方和 $(SS_{RES_{NULL}})$，分割成下面兩個部分：

1. 替代假設的複雜模型能夠說明的模型平方和  $(SS_{REG})$；
2. 替代假設的複雜模型的殘差平方和 $(SS_{RES_{ALT}})$。

用數學表達式表示爲：

$$
\begin{equation}
\sum_{i=1}^n(y_i-\bar{y})^2 = \sum_{i=1}^n(\hat{y}-\bar{y})^2 + \sum_{i=1}^n(y_i-\hat{y}_i)^2 \\
SS_{RES_{NULL}}(SS_{yy}) = SS_{REG} + SS_{RES_{ALT}}
\end{equation}
(\#eq:SSres-partition)
$$



**證明**

$$
\begin{aligned}
\sum_{i=1}^n(y_i-\bar{y})^2 &= \sum_{i=1}^n[(\hat{y}-\bar{y})+(y_i-\hat{y})]^2\\
                            &= \sum_{i=1}^n(\hat{y}-\bar{y})^2+\sum_{i=1}^n(y_i-\hat{y})^2+2\sum_{i=1}^n(\hat{y}_i-\bar{y})(y_i-\hat{y}) \\
                            &= SS_{REG} + SS_{RES_{ALT}} + 2\sum_{i=1}^n(\hat{y}_i-\bar{y})(y_i-\hat{y})
\end{aligned}
$$

接下來就是要證明 $\sum_{i=1}^n(\hat{y}_i-\bar{y})(y_i-\hat{y})=0$

因爲公式 \@ref(eq:lmcenter) $\hat{y}_i=\bar{y}+\hat{\beta}(x_i-\bar{x})$ 所以公式變形如下：

$$
\begin{aligned}
\sum_{i=1}^n(\hat{y}_i-\bar{y})(y_i-\hat{y}) &=  \sum_{i=1}^n(\bar{y}+\hat\beta(x_i-\bar{x})-\bar{y})(y_i-\bar{y}-\hat\beta(x_i-\bar{x})) \\
&= \sum_{i=1}^n\hat\beta(x_i-\bar{x})[y_i-\bar{y}-\hat\beta(x_i-\bar{x})] \\
&= \hat\beta\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y}) - \hat\beta^2\sum_{i=1}^n(x_i-\bar{x}) \\
&= \frac{S_{xy}}{S_{xx}}S_{xy} - (\frac{S_{xy}}{S_{xx}})^2SS_{xx}\\
&= 0 \\
\Rightarrow  SS_{RES_{NULL}}(SS_{yy}) &= SS_{REG} + SS_{RES_{ALT}}
\end{aligned}
$$

### $R^2$ -- 我的名字叫**決定係數** coefficient of determination {#Rsquare}

在公式 \@ref(eq:SSres-partition) 中，因變量的平方和被分割成了兩個部分：$SS_{REG}$ 迴歸模型能說明的部分，和 $SS_{RES_ALT}$ 迴歸模型的殘差平方和。所以，我們定義迴歸模型能說明的部分，佔因變量平方和的百分比 $\frac{SS_{REG}}{SS_{yy}}$，爲決定係數 $R^2$。

這個決定係數之前 (Section \@ref(rsquare)) 也出現過：

$$
\begin{equation}
R^2 = \frac{SS_{REG}}{SS_{yy}} = \frac{\sum_{i=1}^n(\hat{y}_i-\bar{y})^2}{\sum_{i=1}^n(y_i-\bar{y})^2} = 1-\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{\sum_{i=1}^n(y_i-\bar{y})^2}
\end{equation}
(\#eq:R-square)
$$

再一次回到數據 (\@ref(fig:age-wt)) 的線性迴歸來看：

```{r}
library(haven)
growgam1 <- read_dta("backupfiles/growgam1.dta")
Model <- lm(wt~age, data=growgam1)
print(summary(Model), digit=6)
```

R 輸出的結果中最下面的部分 `Multiple R-squared:  0.5408`。我們就可以用“人話”來解釋其意義：假定年齡和體重成直線關係，那麼年齡解釋了這組數據中兒童體重變化 (平方和) 的 54%。

### 方差分析表格 the ANOVA table

一般情況下一個簡單線性迴歸，通過 ANOVA 對因變量平方和的分割，會被彙總成下面這樣的表格：

```{r anova, echo=FALSE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("backupfiles/anova.csv", header = T)
kable(dt, "html", align = "c",caption = "Analysis of Variance table for a simple liear regression model") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))
```

<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption>表 26.1: Analysis of Variance table for a simple liear regression model</caption>
 <thead><tr>
<th style="text-align:center;"> Source of <br>Variation </th>
   <th style="text-align:center;"> Sum of <br>Squares </th>
   <th style="text-align:center;"> Degrees of <br>Freedom </th>
   <th style="text-align:center;"> Mean Sum of <br>Squares </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:center;"> Regression (model) </td>
   <td style="text-align:center;"> $SS_{reg}$ </td>
   <td style="text-align:center;"> $1$ </td>
   <td style="text-align:center;"> $MS_{reg} = \frac{SS_{reg}}{1}$ </td>
  </tr>
<tr>
<td style="text-align:center;"> Residual </td>
   <td style="text-align:center;"> $SS_{res}$ </td>
   <td style="text-align:center;"> $n-2$ </td>
   <td style="text-align:center;"> $MS_{res} = \frac{SS_{res}}{(n-2)}$ </td>
  </tr>
<tr>
<td style="text-align:center;"> Total </td>
   <td style="text-align:center;"> $SS_{yy}$ </td>
   <td style="text-align:center;"> $n-1$ </td>
   <td style="text-align:center;"> $\frac{SS_{yy}}{(n-1)}$ </td>
  </tr>
</tbody>
</table>


表格中最右邊一列是平均平方和 (mean sum of squares)。它的定義是將平方和除以各自的自由度。其中殘差的平均平方和 $MS_{RES}=\frac{SS_{RES}}{(n-2)}$ 是替代模型下殘差方差的無偏估計。總體平均平方和 (total mean sum of squares)，則是零假設模型時的殘差方差估計。在 R 裏面也已經演示過多次 `anova(model)` 是調取方差分析表格的代碼：

```{r}
Model <- lm(wt~age, data=growgam1)
print(anova(Model), digit=8)
```

注意到 R 省略掉第三行總體平方和的部分，不過其實也不太需要。檢驗統計量 F 的計算也很簡單，就是359.06320/1.62184=221.39。

### 用 ANOVA 進行假設檢驗

在 ANOVA 中使用的檢驗手段是 $F$ 檢驗。這裏用 $F$ 檢驗來比較**模型解釋的因變量平方和部分** $(SS_{REG})$ 和**這個模型不能解釋的殘差平方和部分** $SS_{RES}$ 經過自由度校正以後比值的大小。

此時我們需要知道零假設和替代假設 $\text{H}_0: \beta=0 \text{ v.s. H}_1: \beta\neq0$ 時，$SS_{REG}, SS_{RES}$ 的分佈。

1. 零假設和替代假設時，$SS_{RES}$ 均服從自由度爲 $n-2$ 的卡方分佈：

$$
\begin{equation}
\text{Because } SS_{RES} = \sum_{i=1}^n \varepsilon \sim N(0, \sigma^2)\\
\frac{SS_{RES}}{\sigma^2} \sim \chi^2_{n-2}
\end{equation}
(\#eq:distributionSSres)
$$

2. 零假設時， $SS_{REG}$ 服從自由度爲 $1$ 的卡方分佈，且與 $SS_{RES}$ 相互獨立：

$$
\begin{equation}
\frac{SS_{REG}}{\sigma^2} \sim \chi^2_1
\end{equation}
(\#eq:distributionSSreg)
$$

3. 替代假設時，$SS_{REG}$ 服從一個非中心化的卡方檢驗，且與 $SS_{RES}$ 相互獨立：

$$
\begin{equation}
SS_{REG} = \beta^2 SS_{xx} + U \text{ where }\frac{U}{\sigma^2} \sim \chi_1^2
\end{equation}
(\#eq:distributionSSregh1)
$$

### 簡單線性迴歸時的 $F$ 檢驗

如果兩個隨機變量各自服從相應自由度的卡方分佈，他們的每個元素的比值服從 $F$ 分佈：

$$
A\sim \chi_a^2 \text{ and } B\sim \chi_b^2\\
\Rightarrow \frac{A/a}{B/b} \sim F_{a,b}
$$

因此，目前爲止的推導過程我們也可以看到，在零假設條件下，$MS_{REG}$ 和 $MS_{RES}$ 的比值會服從 $F$ 分佈，自由度爲 $(1, n-2)$：

$$
\begin{equation}
F=\frac{SS_{REG}/1}{SS_{RES}/(n-2)} = \frac{MS_{REG}}{MS_{RES}} \sim F_{1,n-2}
\end{equation}
(\#eq:Fdistri)
$$

在替代假設條件下 $(\text{H}_1: \beta\neq0)$，$SS_{REG}$ 的期望值是 $\sigma^2+\beta^2SS_{xx}$，所以替代假設條件下的 $F$ 檢驗量總是會大於零假設時的 $F$。因此你可以看到，這是一個雙側檢驗 ($\text{H}_0: \beta=0 \text{ v.s. H}_1: \beta\neq0$)，但是由於替代假設的 $F$ 總是較大，所以只需要 $F$ 的右半部分的概率密度積分 (單側 $p$ 值)。

### 簡單線性迴歸時 $F$ 檢驗和 $t$ 檢驗的一致性 {#F-t-same}

**證明**

$$
\begin{aligned}
&F=\frac{SS_{REG}/1}{SS_{RES}/(n-2)} = \frac{SS_{REG}}{(SS_{yy}-SS_{REG})/(n-2)} \\
&\text{Since } r^2 = \frac{SS_{REG}}{SS_{yy}} \\
&F=(n-2)\frac{SS_{yy}r^2}{SS_{yy}-SS_{yy}r^2}=(n-2)(\frac{r^2}{1-r^2})=t^2
\end{aligned}
$$

最後一步用到 (Section \@ref(t-r2-F)) 證明過的，迴歸係數檢驗統計量 $t$，和 Pearson 相關係數 $r$ 之間的關係。

## 分類變量用作預測變量時的 ANOVA

方差分析的應用是如此的廣泛，你可以在多重迴歸中使用，也可以在模型中有分類變量時使用，甚至是同時有連續性變量和分類變量的迴歸模型中得到應用。

之前也遇到過二分類變量的簡單線性迴歸模型，當時我們的做法是使用一個啞變量來表示一個二分類變量。同樣的方法也可以用到多組分類變量上來，然後繼續使用線性迴歸。

### 一個二分類預測變量

在前面的例子 (Section \@ref(binarylms)) 中也已經展示過，可以通過線性迴歸來分析一個二分類變量 (實驗組對照組)，和一個連續型變量 (能直立行走時的兒童年齡)兩個變量之間的關係。而且其結果同兩樣本 $t$ 檢驗的結果完全一致。

繼續回到之前用過的這個兒童行走數據 (表 \@ref(tab:walk))：



```{r echo=FALSE}
print(summary(wk_age), digits = 5)
print(anova(wk_age), digits = 5)
```

之前分析這個數據的時候也說明過了，這裏的迴歸係數 $2.225$ 的含義是兩組之間均值的差異。而且注意看，這個迴歸係數是否爲零的檢驗統計量$(t-test)$獲得的 $p$ 值和 ANOVA 的檢驗結果 $(F-test)$ 也是一致的。正驗證了我們前面證明的結果。(Section \@ref(F-t-same))

### 一個模型，兩種表述

上面這個例子中，一個二分類的預測變量和一個因變量之間的關係，實際上可以用兩種數學模型來表達：

1. 令 $y_i, x_i$ 分別是第 $i$ 名觀察對象的因變量 (“直立行走的年齡”)，和預測變量 (“實驗組或者對照組”) $(i=1,\cdots,n)$。那麼**迴歸模型**可以寫作：

$$
\begin{equation}
y_i = \alpha+\beta x_i + \varepsilon_i, \text{ where } \varepsilon_i \sim NID(0, \sigma^2)
\end{equation}
(\#eq:regremodel)
$$

其中，
- $x_i=0$ 時，表示第 $i$ 名觀察對象在實驗組；
- $x_i=1$ 時，表示第 $i$ 名觀察對象在對照組。

在這樣的迴歸模型標記下，零假設和替代假設分別是 $\text{H}_0: \beta=0 \text{ v.s. H}_1: \beta\neq0$

2. 另一種模型的表達方式，被叫做 ANOVA 表達方式。是如此描述上面的關係的：令 $y_{ki}$ 表示第 $i$ 名觀察對象，他在第 $k$ 組 $(i=1,\cdots, n_k; k=1,2)$，此時的模型被寫作：

$$
\begin{equation}
y_{ki} = \mu_k + \varepsilon_{ki}, \text{ where } \varepsilon_{ki} \sim NID(0, \sigma^2)
\end{equation}
(\#eq:anovamodel)
$$

此時，$\mu_k$ 表示第 $k$ 組因變量的均值。零假設和替代假設分別是 $\text{H}_0: \mu_k=\mu \text{ v.s. H}_1: \mu_k\neq\mu$。這裏的 $\mu$ 表示，每個組的平均值等於一個共同的均值 $\mu$。

### 分組變量的平方和

對於預測變量只有一個分組變量的模型，擬合後的數值就是兩組的因變量均值 $(\bar{y}_k)$。在零假設條件下，兩組均值相等，均等於總體均值 $\bar{y}$。這就導致了，殘差平方和，模型平方和在分組變量的 ANOVA 分析時要使用與連續型變量不同的術語。

- 殘差平方和表示爲：

$$
\begin{equation}
SS_{RES} = \sum_{k=1}^k\sum_{i=1}^{n_k} (y_{ki}-\bar{y}_k)^2
\end{equation}
(\#eq:withingroupSS)
$$

其實這就是**組內平方和** (within group sum of squares)。

- 模型平方和表示爲：

$$
\begin{equation}
SS_{REG} = \sum_{k=1}^k\sum_{i=1}^{n_k}(\bar{y}_k-\bar{y})^2=\sum_{k=1}^kn_k(\bar{y}_k-\bar{y})^2
\end{equation}
(\#eq:betweengroupSS)
$$

其實這就是**組間平方和** (between group sum of squares)

```{r}
Mdl0 <- aov(Age ~ Group, data = Walk) # fit a one-way ANOVA
print(summary(Mdl0), digits = 6)
```

其實這跟之前的 `anova(Model)` 給出的結果完全一致。

```{r}
bartlett.test(Age ~ Group, data=Walk)
```

FYI. 上面的代碼 `bartlett.test()` 利用的是另外一個叫做 Bartlett 檢驗法的方差比較公式。(在 STATA 的 `oneway` 命令中也會默認給出 Bartlett 檢驗的方差是否一致的檢驗結果)

### 簡單模型的分組變量大於兩組的情況

公式 \@ref(eq:anovamodel), \@ref(eq:withingroupSS), 和 \@ref(eq:betweengroupSS) 在兩組以上分組變量作預測變量時也是適用的。但是當組數爲 $K$ 時，組內平方和 (殘差平方和 $SS_{RES}$) 的自由度需要修改成 $n-K$ (這是因爲模型中使用了 $K$ 個參數)。此時方差分析 ANOVA 的彙總表格就變爲了下面這樣：

```{r 1way-anova, echo=FALSE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("backupfiles/anova2.csv", header = T)
kable(dt, "html", align = "c",caption = "One-way ANOVA table") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))
```

<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption>表 26.2: One-way ANOVA table</caption>
 <thead><tr>
<th style="text-align:center;"> Source of <br> variation </th>
   <th style="text-align:center;"> Sum of <br> Squares </th>
   <th style="text-align:center;"> Degrees of <br> Freedom </th>
   <th style="text-align:center;"> Mean Sum of <br> Squares </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:center;"> Between groups </td>
   <td style="text-align:center;"> $SS_{between}$ </td>
   <td style="text-align:center;"> $K-1$ </td>
   <td style="text-align:center;"> $\frac{SS_{between}}{(K-1)}$ </td>
  </tr>
<tr>
<td style="text-align:center;"> Within groups </td>
   <td style="text-align:center;"> $SS_{within}$ </td>
   <td style="text-align:center;"> $n-K$ </td>
   <td style="text-align:center;"> $\frac{SS_{within}}{(n-K)}$ </td>
  </tr>
<tr>
<td style="text-align:center;"> Total </td>
   <td style="text-align:center;"> $SS_{yy}$ </td>
   <td style="text-align:center;"> $n-1$ </td>
   <td style="text-align:center;"> $\frac{SS_{yy}}{(n-1)}$ </td>
  </tr>
</tbody>
</table>

此時，檢驗統計量 $F$ 的計算公式爲：

$$
\begin{equation}
F=\frac{SS_{between}/(K-1)}{SS_{within}/(n-K)} \sim F_{(K-1),(n-K)}
\end{equation}
(\#eq:F1way-anova)
$$

在解釋兩組以上分組變量的分析結果時，要注意的是如果 $p$ 值很小，檢驗結果告訴我們的是，各組中因變量的均值**不全相等**，而**不是全部都不相等**。其實就是，即使做了這個檢驗，我們也不知道到底那兩組之間是有差異的。如果此時我們發現結果提示均值不全相等，通常我們還會再作進一步的分析，使用類似成對比較法等等 (以後再繼續詳述)。不過提前要記住，如果使用成對比較法時 (pair-wise comparisons)，**多重比較的問題 (multiple comparisons)**會凸顯出來，主要的結果是增加統計檢驗的假陽性 (false-positive) 概率，此時再繼續使用 $p<0.05$ 作爲統計學意義的標準則是不妥當的。


# 多元模型分析 Multivariable Models

## 背景

簡單線性迴歸描述的是一個連續型的因變量 $(y)$，和一個單一的預測變量 $(x)$ 之間的關係。我們考慮把這個模型擴展成包含多個預測變量，單一因變量的模型。例如，我們可以考慮建立一個模型使用生活習慣 (包括“年齡，性別，運動，飲食習慣等”) 來預測收縮期血壓。此時多重迴歸的思想就可以幫我們理解一些我們**更加關心的因子**，與因變量之間的關係，同時控制或者叫調整了其他的**混雜因子** (control or adjust confounders)。有時候這樣的模型也可以直接應用到生活中去，比如上面的例子，我們可以通過瞭解一個人的生活習慣，用建立好的模型來估計這個人的收縮期血壓。

建立模型之前，必須明確研究的目的是什麼。例如我們關心一個**新發現的因子**可能與高血壓有關係，那麼模型中我們放進去調整的其他因子 (如年齡，性別，運動) 等和因變量 (血壓) 之間的關係就變得不那麼重要。

多重線性迴歸，或者叫多元模型分析 (multiple linear regression or multivariable linear regression) 是研究一個連續型因變量和多個預測變量之間關係的重要模型。本章還會着重討論**混雜 (confounding)**的概念。

## 兩個預測變量的線性迴歸模型

### 數學標記法和解釋

這裏假設我們研究一個因變量 $Y$，和兩個預測變量 $(X_1,X_2)$ 的模型。那麼此時兩個預測變量的線性迴歸模型可以記爲：

$$
\begin{equation}
y_i = \alpha + \beta_1 x_{1i} + \beta_2 x_{2i} + \varepsilon_i, \text{ where } \varepsilon_i \sim NID(0, \sigma^2)
\end{equation}
(\#eq:2varmultilm)
$$

其中，

  - $y_i$ 是第 $i$ 名研究對象的因變量數據 (例如體重)；
  - $x_{1i}$ 是第 $i$ 名研究對象的第一個預測變量數據 (例如年齡)， $X_1$；
  - $x_{2i}$ 是第 $i$ 名研究對象的第二個預測變量數據 (例如身高)， $X_1$；
  - $\alpha$ 的涵義是，當兩個預測變量均爲 $0$ 時，因變量的期望值；
  - $\beta_1$ 的涵義是，當 $X_2$ 不變時，$X_1$ 每升高一個單位，因變量的期望值；
  - $\beta_2$ 的涵義是，當 $X_1$ 不變時，$X_2$ 每升高一個單位，因變量的期望值。

$\beta_1, \beta_2$ 叫做偏迴歸係數 (partial regression coefficient)。它們測量的是兩個預測變量中，當一個被控制 (保持不變) 時，另一個對因變量的影響。

這個模型也可以用矩陣的形式來表示：

$$
\begin{equation}
\textbf{Y} = \textbf{X}\beta+\varepsilon, \text{ where } \varepsilon \sim N(0, \textbf{I}\sigma^2) \\
\left(
\begin{array}{c}
y_1\\
y_2\\
\vdots\\
y_n
\end{array}
\right) = \left(
\begin{array}{c}
1&  x_{11} & x_{21}  \\
1&  x_{12} & x_{22} \\
\vdots &   \vdots& \vdots \\
1&   x_{1n}& x_{2n} \\
\end{array}
\right)\left(
\begin{array}{c}
\alpha \\
\beta_1\\
\beta_2
\end{array}
\right)+\left(
\begin{array}{c}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_n\\
\end{array}
\right)
\end{equation}
(\#eq:matrixlm)
$$

此時上面的表達式中，$\textbf{X}​$ 是一個矩陣，$\textbf{Y}, \beta, \varepsilon​$ 均為向量。殘差被認為服從多變量正態分佈 **(Multivariate normal distribution)** ，這個多變量正態分佈的協方差矩陣為 $\sigma^2​$ 和單位矩陣 $\textbf{I}​$ 的乘積來描述。這等價於假設殘差是獨立同分佈且方差 $\sigma^2​$ 不變。

### 最小平方和估計 Least Squares Estimation

跟簡單線性回歸相似地，我們需要通過對殘差平方和最小化，來獲得此時多重線性回歸的各項參數估計：

$$
\begin{equation}
SS_{RES} = \sum_{i=1}^n \hat\varepsilon_{i}^2 = \sum_{i=1}^n(y_i-\hat{y})^2=\sum_{i=1}^n(y_i-\hat\alpha-\hat\beta_1x_{1i}-\hat\beta_2x_{2i})^2
\end{equation}
(\#eq:se)
$$

求能讓這個殘差平方和取最小值的參數估計 $\hat\alpha,\hat\beta_1,\hat\beta_2$ 我們會在下一章用矩陣標記法來解釋。此處要強調的是，這些估計量都是無偏估計量，且可以被證明的是殘差方差可以用下面的式子來定義：

$$
\begin{equation}
\hat\sigma^2=\sum_{i=1}^n\frac{\hat\varepsilon_i^2}{(n-3)}=\frac{\sum_{i=1}^n(y_i-\hat\alpha-\hat\beta_1x_{1i}-\hat\beta_2x_{2i})^2}{(n-3)}
\end{equation}
(\#eq:multivar)
$$



## 線性回歸模型中使用分組變量

之前我們已展示過，分組變量可以使用啞變量來表示分組變量。分組變量多於兩組時，即可用多個啞變量同時來表示。現在假設變量 $X$ 有三個分組分別用 $1,2,3$ 來表示。那麼用啞變量來描述含有這個分組變量的數學方法可以標記為：


$$
\begin{equation}
y_i  = \alpha+\beta_1u_{1i}+\beta_2u_{2i}+\varepsilon_i, \text{ where } \varepsilon_i \sim NID (0,\sigma^2)
\end{equation}
(\#eq:dummy3)
$$

其中


$$
\begin{aligned}
u_{1i}=\left\{
 \begin{array}{ll}
 1 \text{ if } x_i=2 \\
 0 \text{ if } x_i\neq2 \\
 \end{array}
\right. ;
u_{2i}=\left\{
 \begin{array}{ll}
 1 \text{ if } x_i=3 \\
 0 \text{ if } x_i\neq3 \\
 \end{array}
\right.
\end{aligned}
$$

其實如果你願意，你也可以把公式 \@ref(eq:dummy3) 寫成下面這樣：


$$
\begin{aligned}
\begin{array}{ll}
y_i = \alpha + \varepsilon_i   & \text{if }  x_i=1 \\
y_i = \alpha +\beta_1+ \varepsilon_i   & \text{if }  x_i=2 \\
y_i = \alpha +\beta_2+ \varepsilon_i   & \text{if }  x_i=3 \\
\end{array}
\end{aligned}
$$
所以，

- $\alpha$ 是 $X=1$ 時因變量的期待值；
- $\alpha+\beta_1$ 是 $X=2$ 時因變量的期待值，所以 $\beta_1$ 是分組變量 $X$ 前兩組之間因變量的期待值的差； 
- $\alpha+\beta_2$ 是 $X=3$ 時因變量的期待值，所以 $\beta_2$ 是分組變量 $X$ 前兩組之間因變量的期待值的差。

此時的 $X=1$ 這個組通常被當作是分組變量中的基準組，也就是參照組 (reference group)。實際情況下你可能可以改變這個參照組為其他組的任意一個。



##  協方差分析模型 the Analysis of Covariance (ANCOVA)  Model 



協方差分析模型用來分析一個連續型的因變量 $Y$ ，與一個連續型的預測變量 $(X_1)$和一個二分類的預測變量 $(X_2= 1,2)$，模型被標記為：


$$
\begin{equation}
y_i=\alpha+\beta_1x_{1i}+\beta_2u_{2i}+\varepsilon_i, \text{ where } \varepsilon_i \sim NID(0,\sigma^2)
\end{equation}
(\#eq:ancova)
$$
其中， 

- $y_{i}$ 為第 $i$ 名研究對象的因變量數據 (連續型)； 
- $x_{1i}$ 為第 $i$ 名研究對象的第一個預測變量 (也是連續型)；
- $u_i =\left\{ \begin{array}{ll} 1 \text{ if } x_{2i}=2 \\ 0 \text{ if } x_{2i}=1 \\ \end{array}\right.$ 

此模型中用到的參數有：

- $\alpha$ 是截距，意為當 $X_1=0$ 且 $X_2=1 \; (u=0)$ 時的因變量期待值；
- $\beta_1$ 是當 $X_2$ 保持不變時，$X_1$ 每升高一個單位時，因變量 $Y$ 的期待值；
- $\beta_2$ 是當 $X_1$ 保持不變時，分組變量 $X_2$ 的兩組之間因變量 $Y$ 的期待值差異大小。

所以理解了上面的解釋之後，就可以將表達式 \@ref(eq:ancova) 描述為：


$$
\begin{array}{ll}
y_i=\alpha+\beta_1x_{1i}+\varepsilon_i & \text{ if } x_{2i}=1 \\
y_i=\alpha+\beta_2+\beta_1x_{1i}+\varepsilon_i & \text{ if } x_{2i} = 2
\end{array}
$$

所以，在一個二維圖形中繪製這兩條回歸直線，你會發現他們之間是**平行的**。因為他們之間相差的只有截距，決定直線斜率的回歸係數，都是 $\beta_1$。再用之前用過的數據，兒童的體重和年齡，如果此時考慮了性別因素的話，多重線性回歸的輸出結果和圖形分別應該是：

```{r}
library(haven)
growgam1 <- read_dta("backupfiles/growgam1.dta")
growgam1$sex <- as.factor(growgam1$sex)

Model1 <- lm(wt ~ age + sex, data=growgam1)
print(summary(Model1), digits = 5)
print(anova(Model1), digits = 5)
```



```{r age-wt-mlm, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Data and fitted values from a regression model relating age and gender to data from a cross-sectional survey. For male children data points shown as circles and fitted values linked by a solid line. For female children data points shown as triangles and fitted values linked by a dashed line.', fig.align='center', out.width='80%'}
library(ggplot2)
library(ggthemes)
library(ggsci)

ggplot(growgam1, aes(x=age, y=wt, shape=sex, colour=sex)) + geom_point(size=2, alpha=2/3) + 
  scale_x_continuous(breaks = seq(0,42,6), limits = c(0,36)) + 
  scale_y_continuous(breaks = seq(0,30,5), limits = c(0,20)) + 
  labs(x = "Age (months)", y= "Weight (kg)") + theme_bw() + 
  geom_abline(intercept = Model1$coefficients[1], 
              slope = Model1$coefficients[2], colour = "#BC3C29FF") + 
  geom_abline(intercept = Model1$coefficients[1]+Model1$coefficients[3], 
              slope = Model1$coefficients[2], colour = "#0072B5FF", linetype=2) +
  scale_color_nejm() + theme(legend.position = "none")
 
```



## 偏回歸係數的變化

在增加不同的預測變量進入線性回歸模型中時，原先在方程中的預測變量的偏回歸係數發生了怎樣的變化？

我們先從最簡單的開始入手。先只考慮一個簡單先行回歸模型的情況。當我們新加入一個預測變量，模型發生了什麼變化？


$$
\begin{aligned}
& \text{Model 1: } y_i = \alpha^*+\beta_1^*x_{1i}+\varepsilon^*_i \\
& \text{Model 2: } y_i = \alpha + \beta_1x_{1i} + \beta_2 x_{2i}+\varepsilon_i
\end{aligned}
$$
$\beta_1, \beta_1^*$ 表示的其實是完全不同的含義。$\beta_1^*$ 被稱為粗回歸係數 (crude)，或者叫做調整前回歸係數，$\beta_1$ 被稱為調整後回歸係數 (adjusted coefficient)。二者之間的差異，其實是可以通過對這兩個變量進行簡單線性回歸來度量的：


$$
\text{Model 3: } x_{2i} = \gamma+\delta_1x_{1i}+\omega_i
$$
將 Model 2 中的 $x_{2i}$ 用 Model 3 來替換掉：
$$
\begin{aligned}
\text{Model 2: }y_i  &= \alpha + \beta_1 x_{1i} + \beta_2(\gamma + \delta_1x_{1i}+\omega_i) +\varepsilon_i \\
       &= \alpha + \beta_2\gamma+(\beta_1+\beta_2\delta_1)x_{1i}+\beta_2\omega_i + \varepsilon_i
\end{aligned}
$$
比較 Model 1 和變形過後的 Model 2 中 $x_{1i}$ 的係數就不難發現：


$$
\beta_1^* = \beta_1 + \beta_2\delta_1
$$
由此可見，調整前後 $x_{1i}$ 的回歸係數的變化 $\beta_1^*, \beta_1$ 之間的差異，取決於兩個部分的大小：

- $\beta_2$ 的大小和它的符號；
- $X_1, X_2$ 這兩個預測變量之間有多大關聯，用 Model 3 的 $\delta_1$ 來度量。

所以，當調整後的 $\beta_1 > 0$ 時，要分三種情況來討論



### 情況1： $\beta_1 > \beta_1^*$

此時，$\beta_2\delta_1<0$ 所以，二者之間一正一負。如下圖所示：

```{r echo=FALSE, fig.asp=.7, fig.width=4, fig.align='center', out.width='50%'}
knitr::include_graphics("img/lr4confounding1.png")
```
按圖所示，當 $X_2$ 保持不變，$X_1$ 與因變量 $Y$ 正相關 ($\beta_1>0$)。但是，兩個預測變量之間 $X_1, X_2$ 也呈正相關關係 $\delta_1 >0$。而同時，$X_2$ 的升高會導致因變量 $Y$ 的下降 ($\beta_2 <0 $)。這種情況就意味著，如果，我們不調整 $X_2$ (使之保持不變)，那麼 $X_1$ 每升高一個單位，$Y$ 的變化會**低於**調整 $X_2$ 時，$X_1$ 的變化所引起的 $Y$ 的變化。如果這時候 $\beta_2,\delta_1$ 較大，那麼對於 $X_1$ 來說，調整 $X_2$ 前後，回歸係數的變化較大，如果大到一定程度，甚至調整前後的回歸係數的方向 (正負) 都會發生變化。

### 情況2：$\beta_1<\beta_1^*$ 

本情況下，$\beta_2\delta_1>0$ 是正的。所以二者要麼同時爲正，要麼同時爲負。如下圖所示：

```{r echo=FALSE, fig.asp=.7, fig.width=4, fig.align='center', out.width='50%'}
knitr::include_graphics("img/lr4confounding2.png")
```



當 $X_2$ 保持不變時， $X_1$ 同 $Y$ 呈正關係。但是，$X_1$ 的升高也會引起 $X_2$ 的升高，同時通過 $X_2$ 和 $Y$ 之間的正關係升高 $Y$。所以假設在模型裏我們不對 $X_2$ 進行控制 (controld or adjust)，那麼 $X_1$ 和 $Y$ 之間的關係就被誇大了。

所以，當 $X_1\rightarrow X_2\rightarrow Y$ 的這條通路大大超過 $X_1\rightarrow Y$ 的話，調整後的迴歸係數 $\beta_1$ 就會變得很小。

### 情況3： $\beta_1 = \beta_1^*$

這種情況只有當 $\beta_2\delta_1=0$ 時才會出現。所以，二者至少有一個是 $0$。 如下圖所示：

```{r echo=FALSE, fig.asp=.7, fig.width=4, fig.align='center', out.width='50%'}
knitr::include_graphics("img/lr4confounding3.png")
```



$X_1$ 與 $Y$ 呈正關係，$X_1$ 與 $X_2$ 呈正關係。但是 $X_2$ 與 $Y$ 無關聯。所以此時無論模型是否調整了 $X_2$ 都不會影響 $X_1$ 和 $Y$ 之間關係的計算。

## 混雜 confounding

流行病學家最喜歡的詞彙恐怕要屬混雜 (confounding) 了 (interaction, 交互作用也要算一個，(笑))。他們常用混雜來解釋爲什麼調整其他因子前後迴歸係數發生了變化。當有其他因子 (測量了或者甚至是未知的) 對我們關心的預測變量和因變量之間的關係產生了影響 (加強或是減弱) 時，就叫做發生了混雜。

對於一個預測變量是否夠格被叫做混雜因子，它必須滿足下面的條件：

-   與關心的預測變量相關 (i.e. $\delta_1 \neq 0$)；
-   與因變量相關 (當關心的預測變量不變時，$\beta_2\neq0$ )；
-   不在預測變量和因變量的因果關係 (如果有的話) 中作媒介。Not be on the causal pathway between the predictor of interest and the dependent variable.

有時，判斷一個因子是否對我們關心的預測變量和因變量之間的關係構成了混雜並不容易，也不直觀。所以，有太多太多的情況下，我們無法準確地 100% 地確定我們關心的關係是否被別的因子混雜。所以，莫要用 “混雜” 一詞簡單糊弄人。

### 考慮因果關係的通路

多數情況下，我們也無法從數據判斷一個變量是否在我們關心的預測變量和因變量之間關係的通路上。此時要做的是離開你的電腦，去學習他們之間的生物學知識，看是否真的有關係。

但是有些例子就很簡單啦。比如說，服用降血壓藥物可以預防發生中風。那麼此時血壓的降低，就處在了這二者因果關係的通路上。因爲藥物通過降低了血壓，從而預防了中風的發生。這一關係中，我們不能說血壓是混雜因子，它是一個媒介 (mediator)。但是多數的橫斷面研究 (cross-sectional study) 中我們無法下結論。

# 多元模型分析的解釋