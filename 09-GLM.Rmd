# (PART) 廣義線性迴歸模型 {-}

# 重要概念複習

## 概率論學派統計推斷要點複習

下面我們一起用二項分佈的概念 ($n$ 個對象中 $K$ 個“事件”)，來複習概率論學派的統計推斷要點。

1. 模型，the Model。一個統計模型，描述的不僅僅是我們研究的人羣的一些特徵，而且通常一個模型還可提供如何從人羣中收集該樣本的信息。<br> 用二項分佈的概念來解釋，人羣是衆多個體的集合，他們中的一部分佔比 $\pi$ 的人身上發生了某個事件。從這個人羣的集合中，我們隨機抽取 $n$ 個對象作爲**研究樣本**，該樣本中有 $K$ 個人身上發生了事件。此時，我們說 $K$ 服從人羣比例爲 $\pi$ 的二項分佈：$K \sim \text{Bin}(n,\pi)$。
2. 參數，parameters。模型中的參數反映了人羣的某些特徵。在實際應用中，從來沒有“人類”能知道人羣參數的真實值，渺小的我們從人羣中抽取樣本，用於推斷 “上帝才知道的” 這些代表了人羣特徵的參數。<br> 在二項分佈的情境下，有且只有一個人羣參數，人羣中事件的比例 $\pi$。
3. 參數估計量，parameter estimators。估計量是樣本的統計量，被用來估計未知的總體參數。估計量 estimator，是一個隨機變量，是我們計算估計值的一般形式。估計值 estimate，是每個樣本通過統計模型計算獲得的估計量的真實值，每採樣一次，計算獲得的估計值理論上會略有不同。<br> 二項分佈的上下文中，人羣事件比例 -- 這一參數 $\pi$ 的天然估計量是 $\hat\pi = \frac{K}{n}$，當一個樣本中發現 $K = k$，該樣本給出的估計值是 $\frac{k}{n}$。
4. 研究假設，hypotheses。研究假設是實驗前我們提出的要被檢驗的一些關於人羣某些特徵參數的 “陳述 statement”。可以是猜想參數等於某個特定值，或者多個參數大小相同。<br> 二項分佈的數據裏，只有一個人羣參數，$\pi$。可能提出的零假設和替代假設有很多，$\pi = 0.5 \text{ v.s. } \pi \neq 0.5$ 是其中之一的複合型假設。

## 似然

如果一個模型只有一個參數 $\theta$，樣本數據已知的話，該參數的似然爲：

$$\text{L}(\theta | \text{data}) = \text{Pr}(\text{data}|\theta)$$

其中，$\text{Pr}(\text{data}|\theta)$ 對於離散型變量，是概率方程 probability function；對於連續型變量，則是概率密度方程 probability density function (PDF)。

對數似然，就是上面的似然方程取自然底數的對數方程：

$$\ell(\theta | \text{data}) = \text{ln}\{ \text{L}(\theta | \text{data}) \}$$


## 極大似然估計

當數據收集完畢，從獲得的數據中計算獲得的能夠使似然方程/或對數似然方程取得極大值的 $\theta$ 的大小，被叫做極大似然估計 $\text{(MLE)}$，且通常數學標記會在參數上加一頂帽子： $\hat\theta$。收集不同的樣本，在相同的似然方程或對數似然方程下，極大似然估計不同。

1. 許多問題，我們獲得極大似然估計的方法是先定義好模型的似然方程，然後求該方程的一階導數之後計算使之等於零的參數值大小就是 $\text{MLE } \hat\theta$。此時，你還要記得再求一次二階導數，看是否小於零，以確保前一步計算獲得的值給出的似然方程是極大值。
2. 更多的時候我們用對數似然方程以簡化計算過程：

$$
\begin{aligned}
\left.\frac{\text{d}}{\text{d } \theta}\ell (\theta | \text{data})\right\vert_{\theta=\hat{\theta}}  &= \ell^\prime(\hat\theta) = 0 \\
\left.\frac{\text{d}^2}{\text{d } \theta^2}\ell (\theta | \text{data})\right\vert_{\theta=\hat{\theta}}  &= \ell^{\prime\prime}(\hat\theta) < 0
\end{aligned}
$$

3. 我們只關心似然方程的形狀，所以方程中不包含參數的部分可全部忽略掉。
4. $\text{MLE}$ 的一些關鍵性質：
    1. 漸進無偏 asymptotically unbiased：當 $n\rightarrow \infty$ 時，$E(\hat\theta) \rightarrow \theta$；
    2. 一致性 consistency：隨着樣本量的增加，$\hat\theta$ 收斂於 (converges) 總體參數 $\theta$；
    3. 漸進正態分佈 asymptotically normality：隨着樣本量增加，$\hat\theta$ 的樣本分佈收斂於 (converges) 正態分佈，方差爲  $$E[-\ell^{\prime\prime}(\theta)]^{-1}=[-\ell^{\prime\prime}(\hat\theta)]^{-1}$$
    4. 恆定性 invariance：如果 $\hat\theta$ 是 $\text{MLE}$，那麼 $\theta$ 被數學轉換以後 $g(\theta)$ 的方程的 $\text{MLE}$ 是 $g(\hat\theta)$
5. 似然理論可以直接拓展到多個參數的情況。一般地，如果一個模型有 $p$ 個參數 $\mathbf{\theta} = (\theta_1, \theta_2, \cdots, \theta_p)^T$，這些參數在給定數據的條件下的似然方程爲：$$\text{L}(\mathbf{\theta} | \text{data}) = \text{Pr}(\text{data} | \mathbf{\theta})$$ 其中，概率 (密度) 方程在多個參數時變成聯合 (joint) 概率 (密度) 方程。似然，也是各個參數的聯合似然方程。此時，參數向量 $\mathbf{\theta} = (\theta_1, \theta_2, \cdots, \theta_p)^T$ 的方差協方差矩陣的估計量爲：

$$
\hat{\text{Var}}(\mathbf{\hat\theta}) = - \left(
\begin{array}{c}
\frac{\partial^2\ell}{\partial\theta^2_1} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_1} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_1}  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_2} & \frac{\partial^2\ell}{\partial\theta^2_2} & \cdots & \frac{\partial^2\ell}{\partial\theta_k\partial\theta_2}  \\
\vdots & \vdots & \ddots & \vdots  \\
\frac{\partial^2\ell}{\partial\theta_1\partial\theta_k} & \frac{\partial^2\ell}{\partial\theta_2\partial\theta_k} & \cdots & \frac{\partial^2\ell}{\partial\theta^2_k}  \\
\end{array}
\right)^{-1}_{\theta=\hat\theta}
$$

Tips: typing `vcov(Modelname)` command in R will display this estimated variance-covariance matrix for the parameter estimates.

回到二項分佈數據的例子：

$$
K \sim \text{Bin}(n, \pi)
$$

如果我們樣本的觀測數據是 $K=k$，對數似然方程一次微分等於零以後求得的參數 $\pi$ 的 $\text{MLE}$ 是 $\hat\pi = \frac{k}{n}$。所以參數 $\pi$ 的估計量是 $\frac{K}{n}$。$\hat\pi$ 的方差估計量是：

$$
\hat{\text{Var}} (\hat\pi) = \frac{\hat\pi(1-\hat\pi)}{n} \text{ for } \hat\pi = \frac{k}{n}
$$


## 關於假設檢驗的複習

極大似然估計可以有三大類檢驗方法：似然比檢驗法 likelihood ratio test；Wald 檢驗 Wald test；Score 檢驗 Score test。

- 似然比檢驗法 likelihood ratio test (LRT) (Section \@ref(LRT))：

$$
-2llr(\theta_0) = -2\{ \ell(\theta_0) - \ell(\hat\theta) \}
$$

零假設條件下 (Under $\text{H}_0$:)

$$
-2llr(\theta_0) \sim \chi_1^2
$$

這個對數似然比的統計量可以和自由度爲 1 的卡方分佈作比較，計算反對零假設的證據的強度大小。如果顯著性水平是 $\alpha$，那麼下面條件成立時，可以認爲反對零假設的證據強度大到足以拒絕零假設。

$$
-2llr(\theta_0) > \chi^2_{1, 1-\alpha}
$$

- Wald 檢驗 (Section \@ref(Wald)) 是一種利用二次方程近似法對似然比檢驗進行近似的手段。其檢驗統計量是

$$
\begin{aligned}
  (\frac{M-\theta_0}{S})^2 & \sim \chi^2_1 \\
 \text{Where } M  & = \hat\theta \\
              S^2 & = \frac{1}{-\ell^{\prime\prime}(\hat\theta)}
\end{aligned}
$$

- Score 檢驗 (Section \@ref(Score)) 是另一種利用二次方程近似法對似然比檢驗進行近似的手段。其檢驗統計量是

$$
\begin{aligned}
\frac{U^2}{V} & \sim \chi^2_1 \\
\text{Where } U  & = \ell^\prime(\theta_0) \\
             V & = -\ell^{\prime\prime}(\theta_0)
\end{aligned}
$$

如果對數似然方程本身就是一個二次方程 (數據服從完美正態分佈狀態，且總體方差已知時)，這三大類的檢驗法其實計算獲得完全一樣的 $p$ 值，提供完全一致的證據。多數情況下，三大類檢驗法的結果是近似的。關於三種檢驗法的比較可以參考過去總結的章節 (Section \@ref(LRTwaldScore-Compare))

### 子集似然函數

當統計模型中的部分參數是噪音參數 (nuisance parameters) 時，我們需要用到子集似然函數法 (Section \@ref(profile-log-likelihood)) 來去除噪音參數的影響,，只檢驗我們感興趣的那部分參數。

## 線性迴歸複習

### 簡單線性迴歸

假設對於 $n$ 名研究對象，我們測量個兩個觀測值 $(y_i, x_i)$，那麼用線性迴歸模型來表示這兩個測量值估計的參數之間的關係就是：

$$
\begin{aligned}
y_i &  = \alpha + \beta x_i + \varepsilon_i \\
\text{Where } & \varepsilon_i \sim \text{NID}(0,1)
\end{aligned}
$$

或者用另一個標記法：

$$
Y_i | x_i \sim N(\alpha + \beta x_i, \sigma^2)
$$

### 多元線性迴歸

如果預測變量有兩個或者兩個以上 $(x_i, \;\&\; z_i)$，那麼描述這兩個預測變量和因變量之間的多元線性迴歸模型可以寫作：

$$
y_i = \alpha + \beta x_i + \gamma z_i + \varepsilon_i
$$

此時， $\beta$ 的含義是，當保持 $z$ 不變時，$x$ 每增加一個單位，$y$ 的變化量。用這個模型，我們默認 $z$ 保持不變的同時無論取值爲多少， $x, y$ 之間的關係是不會變化的，我們用這個模型來調整 (adjust) $z$ 的混雜效應 (confounding effect) (Section \@ref(confounding))。

當然我們也可以考慮當 $z$ 取值不同時， $x, y$ 之間的關係發生改變，只要在上面的多元線性迴歸方程中加入一個交互作用項即可 (Section \@ref(interaction))。

$$
y_i = \alpha + \beta x_i + \gamma z_i + \delta x_i z_i + \varepsilon_i
$$

增加了交互作用項最大的變化是，$x_i$ 的迴歸係數 $\beta$ 的含義發生了改變：當且僅當 $z = 0$ 且保持不變時，$x$ 每增加一個單位，$y$ 的變化量。如果 $z = k \neq 0$ 且保持不變，那麼 $x$ 每增加一個單位，$y$ 的變化量則是 $\beta + k\delta$。

### 簡單線性迴歸的統計推斷

一個給定的樣本 $(y_i, x_i), i = 1, \cdots, n$ ，其對數似然方程是

$$
\ell(\alpha, \beta, \sigma^2 | \mathbf{y, x}) = -\frac{1}{2\sigma^2}\sum^n_{i=1}(y_i - \alpha - \beta x_i)^2
$$

分別對 $\alpha, \beta$ 求微分之後可以獲得他們各自的 $\text{MLE}$：

$$
\begin{aligned}
U(\alpha) & = \ell^\prime(\alpha) = \frac{1}{\sigma^2}\sum_{i=1}^n (y_i - \alpha - \beta x_i) \\
U(\beta)  & = \ell^{\prime}(\beta) = \frac{1}{\sigma^2}\sum_{i=1}^n x_i(y_i - \alpha - \beta x_i) \\
U(\hat\alpha) & = 0 \Rightarrow \hat\alpha = \bar{y} - \hat\beta\bar{x} \\
U(\hat\beta)  & = 0 \Rightarrow \hat\beta=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{\sum x_iy_i - n\bar{x}\bar{y}}{\sum x_i^2 - n\bar{x}^2}
\end{aligned}
$$

注意到和線性迴歸章節中推導的過程不同 (Section \@ref(MLEalphabeta))，當時我們用的是最小二乘法，這裏我們用的是光明正大的極大似然法，同時也證明了最小二乘法獲得的 $\hat\alpha,\hat\beta$ 是他們各自的 $\text{MLE}$。

另外，殘差方差的 $\text{MLE}$ 也可以用上面的方法推導出來，同樣和之前的方法 (Section \@ref(ResidualVar)) 做個對比吧：

$$
\begin{aligned}
U(\sigma^2) & = \ell^\prime(\sigma^2) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4}\sum_{i=1}^n(y_i - \alpha - \beta x_i)^2 \\
U(\hat\sigma^2) & \Rightarrow \hat\sigma^2 = \frac{\sum_{i=1}^n(y_i - \hat\alpha - \hat\beta x_i)^2}{n}
\end{aligned}
$$

這個殘差方差的 $\text{MLE}$ 其實不是一個無偏估計，它只是一個漸進無偏的估計 (需要除以 $\frac{n-2}{n}$)，所以，當一個線性迴歸模型中有 $p$ 個參數時：

$$
\hat\sigma^2 = \frac{\sum_{i=1}^n(y_i - \hat\alpha - \hat\beta_1 x_{i1} - \hat\beta_2 x_{i2}\cdots)^2}{n - p}
$$

線性迴歸時殘差方差的檢驗統計量服從 $F$ 分佈 (Section \@ref(lm-Ftest))。

# 廣義線性迴歸入門

線性迴歸方法是十分強大的建模工具，可惜的是它只能適用與因變量爲連續型變量的情況。廣義線性迴歸模型 (或者叫一般化線性迴歸模型 generalised linear models, GLM) 是一大類將線性迴歸模型拓展到因變量可以使用二分類，計數，分組型變量的建模工具。

## 指數分佈家族

一個服從正態分佈的隨機變量 $Y$ 的概率密度方程 (probability density function, PDF) 可以寫作

$$
f(y) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(y-\mu)^2}{2\sigma^2}}
$$

給 PDF 的左右兩邊同時取自然底數的對數，方程變形爲

$$
\begin{aligned}
\text{ln}\{f(y)\} & = -\frac{y^2}{2\sigma^2} + \frac{y\cdot\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2} -\frac{1}{2}\text{ln}(2\pi\sigma^2) \\
                & = \frac{y\cdot\mu - \frac{\mu^2}{2}}{\sigma^2} - [\frac{y^2}{2\sigma^2} + \frac{1}{2}\text{ln}(2\pi\sigma^2) ]
\end{aligned}
(\#eq:glm2-0)
$$

如果令

$$
\begin{aligned}
\theta & = \mu  \\
\phi   & = \sigma^2 \\
b(\theta) & = \frac{\mu^2}{2} \\
c(y, \theta) & = \frac{y^2}{2\sigma^2} + \frac{1}{2}\text{ln}(2\pi\sigma^2)
\end{aligned}
$$

那麼上面的式子 \@ref(eq:glm2-0) 可以被整理爲：

$$
\begin{equation}
\text{ln}\{f(y)\} = \frac{y\cdot\theta - b(\theta)}{\phi} - c(y, \theta)
\end{equation}
(\#eq:glm2-1)
$$

**此處有重要結論：** 凡是分佈的概率密度方程的對數方程能夠轉換整理成 \@ref(eq:glm2-1) 形式的分佈，都隸屬於指數分佈家族 (the Exponential Family of distributions)。

### 泊松分佈和二項分佈的指數分佈家族屬性

- 泊松分佈 Poisson Distribution

$$
\begin{aligned}
     f(y) & = \text{Pr}(Y = y) = \frac{\mu^y e^{-\mu}}{y!}, y = 0,1,2,\cdots \\
\text{ln}\{ f(y) \} & = y\cdot\text{ln}(\mu) - \mu - \text{ln}(y!) \\
\text{Let } &\color{red}{\boxed{\theta = \text{ln}(\mu), \phi = 1, b(\theta) = \mu, c(y,\phi) = \text{ln}(y!)}} \\
\Rightarrow \text{ln}\{f(y)\} & = \frac{y\cdot\theta - b(\theta)}{\phi} - c(y, \theta) \\
\end{aligned}
$$

所以，**泊松分佈屬於指數分佈家族成員**。

- 二項分佈 Binommial Distribution

$$
\begin{aligned}
f(y) & = \text{Pr}(Y = y) = \binom{n}{y}\pi^y(1-\pi)^{n-y}, y = 0,1,2,\cdots\\
\text{ln}\{ f(y) \} & = y\cdot \text{ln}(\pi) + (n - y)\text{ln}(1-\pi) + \text{ln}\{\binom{n}{y}\} \\
                    & = y\cdot \text{ln}(\frac{\pi}{1-\pi}) + n\text{ln}(1-\pi) +  \text{ln}\{\binom{n}{y}\} \\
\text{Let } &\color{red}{\boxed{\theta = \text{ln}(\frac{\pi}{1-\pi}), \phi = 1,}} \\
            &\color{red}{\boxed{b(\theta) = -n\text{ln}(1-\pi), c(y, \phi) = -\text{ln}\{\binom{n}{y}\}}}\\
\Rightarrow \text{ln}\{f(y)\} & = \frac{y\cdot\theta - b(\theta)}{\phi} - c(y, \theta) \\
\end{aligned}
$$

所以，**二項分佈也屬於指數分佈家族成員**。

指數分佈家族成員的數學表達式  \@ref(eq:glm2-1)  中，

- $\theta$ 被叫做標準 (或者叫自然) 參數 (**canonical or natural parameter**)，相關的函數被叫做標準鏈接函數 (canonical link function)，如上面所列舉的例子中：泊松分佈時用的對數函數 $\text{ln}(\mu)$，二項分佈時用的邏輯函數 (logit function) $\text{ln}(\frac{\pi}{1-\pi})$，鏈接函數可能還有別的選擇，(例如，二項分佈數據的另一種標準鏈接函數是概率函数 (probit function $\Phi^{-1}(P)$))，同時它對於條件推斷 conditional inference 至關重要，因爲它還提示我們應該用什麼樣的算法去估計我們苦苦尋找的人羣參數。
- $\phi$ 被命名爲**尺度參數 (scale or dispersion parameter)**，泊松分佈和二項分佈的尺度參數是 $1$。但是正態分佈的尺度參數是方差 $\sigma^2$，且常常是未知的，需要從樣本數據中估計。尺度參數是否需要從樣本中獲取其估計值，對於實際統計推斷或者假設檢驗的過程有重大影響。

廣義線性迴歸就是這個指數分佈家族數據共通的一種統計建模過程，所以，在這一“屋檐”下，它衍生出衆多種類的統計模型。

------------------

### Exercise. Exponential distribution

證明指數分佈本身也屬於指數分佈家族，定義其標準鏈接函數和標準參數。

**證明**

$$
\begin{aligned}
Y \sim \text{exp}(\lambda) & \rightarrow f(y) = \lambda \text{exp}(-y\lambda), y > 0\\
\Rightarrow \text{ln}\{ f(y) \} & = - y \lambda + \text{ln}(\lambda) \\
\text{Let } & \color{red}{\theta = -\lambda, b(\theta) = - \text{ln}(\lambda), \phi = 1, c(y, \phi) = 0} \\
\Rightarrow \text{ln}\{f(y)\} & = \frac{y\cdot\theta - b(\theta)}{\phi} - c(y, \theta) \\
\text{Because } E(Y) & = \frac{1}{\lambda}, \text{ the canonical link is } g(\lambda) = -\frac{1}{\lambda}\\
\end{aligned}
$$

------------------


## 廣義線性迴歸模型之定義

一個四肢健全的廣義線性模型包括三個部分：

1. 因變量分佈 (或者叫響應變量分佈，response distribution)：$Y_i, i = 1,\cdots,n$ 可以被認爲是互相獨立且服從指數家族分佈，設其期望值 (均值) $E(Y_i) = \mu_i$；
2. 線性預測方程 (linear predictor)：**預測變量及其各自的參數以線性迴歸形式進入模型**，其中第 $i$ 個觀測值的線性預測值爲：<br> $$\eta_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}$$
3. 鏈接函數 (link function)：鏈接函數連接的是線性預測方程 $\eta_i$ 和其期待值 (均值) 之間 $\mu_i$ 的關係。<br> $$g(\mu_i) = \eta_i$$


簡單線性迴歸模型本身當然也數據廣義線性迴歸模型：

1. 因變量分佈是正態分佈；
2. 線性預測值也是線性迴歸形式；
3. 鏈接函數是它因變量本身 (the **identity** function)。


## 注意

1. 廣義線性迴歸的線性預測方程部分的意義，需要澄清的是它指的是 **參數 parameter** 之間呈線性關係，預測變量本身可以有二次方，三次方，多次方，因爲這些多項式線性迴歸本身仍然是**線性的**如： $$\eta_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + \cdots + \beta_p x_i^p$$ <br> 然而，這樣的形式 $$\eta_i = \alpha (1- e^{\beta_1 x_{i1}})$$ <br> 就不能說是一個線性預測方程。
2. 除了有很少的特例。廣義線性迴歸擬合後的參數估計，推斷，模型評價和比較時使用的原理都一樣，不同的只有各自的分佈和鏈接函數。
3. 通常選用的鏈接方程，要能夠使線性預測方程的取值範圍達到所有實數 $-\infty,+\infty$。
4. “模型的似然函數 the log likelihood of the model”，只是我們偷懶縮短了原文 “在給定數據的前提下，當所有參數均爲 $\text{MLE}$ 時模型的對數似然函數 (the log likelihood function of the model for the given data evaluated at the MLE's of the parameters)”，就是對數似然函數的極大值的意思 (i.e. the maximum of the log likelihood function)。
5. 從本節開始往後的章節中 “模型，model”，“廣義線性模型，generalized linear model”，和 "GLM" 將被視爲同義詞。

## 如何在 R 裏擬合 "GLM"

這裏我們僅僅討論用極大似然法擬合 "GLM" 模型的方法，。前面一章節的複習也是在告訴我們，利用極大似然法簡單說就是找到模型參數，使得似然函數能夠取到極大值。對於線性迴歸來說， $\text{MLE}$ 可以用一個封閉式函數來計算；但是廣義線性迴歸模型則必須使用[迭代法計算 (iterative methods)](https://www.youtube.com/watch?v=JZIeX3eVyf4)。

在 R 裏面擬合廣義線性模型的命令及其格式是：

```{r eval=FALSE}
glm(response variable ~ explanatory variables to form linear predictor, family=name of distribution(link=link function), data=dataset)
```

Tips: See `help(glm)` for other modeling options. See `help(family)` for other allowable link functions for each family.

下面的數據來自一個心理學臨牀實驗，比較的是和安慰劑組相比，注射嗎啡組，注射海洛因組對象的精神病檢測指數的前後變化。

```{r cache = TRUE}
Mental <- read.table("backupfiles/MENTAL.DAT", header =  FALSE, sep ="", col.names = c("treatment", "Before", "After"))
Mental$treatment[Mental$treatment == 1] <- "placebo"
Mental$treatment[Mental$treatment == 2] <- "morphine"
Mental$treatment[Mental$treatment == 3] <- "heroin"
Mental$treatment <- factor(Mental$treatment, levels = c("placebo", "morphine", "heroin"))
head(Mental)
```

我們來比較一下簡單線性迴歸的代碼輸出結果和廣義線性迴歸代碼輸出結果是否一致：

用 `lm` 命令，擬合因變量爲注射後精神病檢測指數，預測變量爲治療方式和注射錢精神病檢測指數，及兩者的交互作用項：

```{r}
Model1 <- lm(After ~ treatment*Before, data = Mental)
summary(Model1)
```

同樣的模型也可以用 `glm` 命令擬合：

```{r}
Model2 <- glm(After ~ treatment*Before, family = gaussian(link = "identity"), data = Mental)
summary(Model2)
```

可以看到，`glm` 命令的輸出結果略多，但是參數估計的部分是完全相同的。**但是如果你用的是坑爹的 STATA，那裏面的 `glm` 命令中的統計檢驗量和 $p$ 值用的則是正態分佈近似法。所以在 STATA 裏面簡單線性迴歸模型最好不要使用 `glm` 命令：**

```
 glm After i.treatt##c.Before, family(gaussian) link(identity)

Iteration 0:   log likelihood = -185.70711

Generalized linear models                         No. of obs      =         72
Optimization     : ML                             Residual df     =         66
                                                  Scale parameter =   11.10799
Deviance         =  733.1276068                   (1/df) Deviance =   11.10799
Pearson          =  733.1276068                   (1/df) Pearson  =   11.10799

Variance function: V(u) = 1                       [Gaussian]
Link function    : g(u) = u                       [Identity]

                                                  AIC             =   5.325197
Log likelihood   =  -185.707106                   BIC             =   450.8676

------------------------------------------------------------------------------
             |                 OIM
        After|      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       treat |
          2  |  -1.211742   1.750342    -0.69   0.489    -4.642349    2.218865
          3  |  -1.461968   1.771855    -0.83   0.409    -4.934741    2.010805
             |
      Before |   .5939394   .1834682     3.24   0.001     .2343483    .9535305
             |
 treat#Before|
          2  |  -.0895258   .2483459    -0.36   0.718    -.5762749    .3972233
          3  |  -.3129855   .2503829    -1.25   0.211     -.803727    .1777561
             |
       _cons |    1.97803   1.294069     1.53   0.126    -.5582981    4.514359
------------------------------------------------------------------------------
```

回到 R 來， 當儲存了一個 `Model2` 向量在 R 裏之後，你可以用下面的各種命令獲取你想要的各種有用的信息。

```{r message=FALSE}
confint(Model2) # 95% CI for the coefficients
exp(coef(Model2)) # exponentiated coefficients
exp(confint(Model2)) # 95% CI for exponentiated coefficients
head(predict(Model2, type="response")) # predicted values
head(residuals(Model2, type="deviance")) # residuals
```

### `margins` 命令

一個在 STATA 裏面十分有用的用於**預測**的命令 `margins`，在 R 裏，下載了 `margins` 包以後就可以調用它的類似命令。

假如我們用擬合的模型預測當注射前精神病檢測值分別是 0，6，12 分時三組之間的注射後精神病檢測值差，可以這樣求：

```{r}
library(margins)
summary(margins(Model2, at = list(Before=c(0,6,12))))
```


對比 STATA 裏的結果：

```
 margins, dydx(trt) at(pre = (0 6 12))

Conditional marginal effects                    Number of obs     =         72
Model VCE    : OIM

Expression   : Predicted mean post, predict()
dy/dx w.r.t. : 2.trt 3.trt

1._at        : pre             =           0

2._at        : pre             =           6

3._at        : pre             =          12

------------------------------------------------------------------------------
             |            Delta-method
             |      dy/dx   Std. Err.      z    P>|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
1.trt        |  (base outcome)
-------------+----------------------------------------------------------------
2.trt        |
         _at |
          1  |  -1.211742   1.750342    -0.69   0.489    -4.642349    2.218865
          2  |  -1.748897    .963025    -1.82   0.069    -3.636391    .1385977
          3  |  -2.286051   1.797717    -1.27   0.204    -5.809513     1.23741
-------------+----------------------------------------------------------------
3.trt        |
         _at |
          1  |  -1.461968   1.771855    -0.83   0.409    -4.934741    2.010805
          2  |  -3.339881   .9623512    -3.47   0.001    -5.226054   -1.453707
          3  |  -5.217794   1.796264    -2.90   0.004    -8.738406   -1.697181
------------------------------------------------------------------------------
Note: dy/dx for factor levels is the discrete change from the base level.
```

### `ggplot2::geom_smooth(method = "loess")` 命令

類似 STATA 作散點圖時的 `lowess` 命令，在 R 裏，你可以用 `ggplot2` 包裏自帶的 `geom_smooth(method = "loess")` 選項命令，給散點圖添加平滑曲線。把觀測數據中變量之間的關係視覺化，用於輔助判斷一個模型是否可以被擬合爲線性關係。全稱是 “locally weighted scatterplot smoothing”，縮寫成 "lowess/loess"。[LOWESS 的原理](https://en.wikipedia.org/wiki/Local_regression)簡略說是，通過把預測變量分成幾個部分，分別在各個小區間內擬合迴歸各自的迴歸曲線，如此便可以將**每個觀測值都以各自不同的加權值放入整個模型**中，然而正如我們在簡單線性模型中提到過的，這樣的曲線更加擬合觀測數據，而不能說明觀測值來自的人羣中，兩個變量之間的關係。此方法的靈活性在於，你可以選擇平滑的程度，該平滑程度用 `bandwith`(STATA) 或者 `span`(R) 表示，取值範圍是 $0 \sim 1$ 之間的任意值，越靠近 $1$，Lowess 曲線越接近簡單線性直線，越靠近 $0$，那麼每個觀測點本身的權重越大，擬合的 Lowess 曲線越接近觀測數據本身。下圖 \@ref(fig:loess-smoother1) 提示，選用的平滑程度 $= 0.8$ 時，精神病測量分數在 (安慰劑組中) 實驗前後的關係接近線性關係。當我們降低平滑程度，Lowess 曲線接近觀測數據本身，其實是太接近觀測數據本身，反而無法提供太多的信息。


```{r loess-smoother1, echo=TRUE, fig.width=7, fig.height=10, fig.cap="Lowess smoother, with bandwith/span set to 0.8, for the menetal data", fig.align='center', out.width='100%', cache=TRUE}
library(ggplot2)
ggplot(Mental, aes(Before, After)) + geom_point() +
  geom_smooth(method = "loess",  span = 0.8, se = FALSE) +
  facet_grid(treatment ~ .) + theme_bw()
```


```{r loess-smoother2, echo=TRUE, fig.width=7, fig.height=10, fig.cap="Lowess smoother, with bandwith/span set to 0.4, for the menetal data", fig.align='center', out.width='100%', cache=TRUE, message=FALSE, warning=FALSE}
library(ggplot2)
ggplot(Mental, aes(Before, After)) + geom_point() +
  geom_smooth(method = "loess",  span = 0.4, se = FALSE) +
  facet_grid(treatment ~ .) + theme_bw()
```

# 二項分佈數據的廣義線性迴歸模型 logistic regression model

二項分佈數據在醫學研究中很常見，例子有千千萬，下面這些只是作爲拋磚引玉：

1. 心臟搭橋手術和血管成形術兩組病人比較療效時，結果變量可以是：死亡 (是/否)；心肌梗死發作 (是/否)；
2. 機械心臟瓣膜手術結果：成功/失敗；
3. 用小鼠作不同劑量二硫化碳暴露下的毒理學實驗，結果變量是：小鼠死亡 (是/否)；
4. 隊列研究中追蹤對象中出現心肌梗死病例，結果變量是：心肌梗死發作 (是/否)。

## 分組/個人 (grouped / individual) 的二項分佈數據

下面的數據，來自某個毒理學實驗，不同劑量的二硫化碳暴露下小鼠的死亡數和總數的數據：

```{r cache=TRUE, echo=FALSE}
Insect <- read.table("backupfiles/INSECT.RAW", header =  FALSE, sep ="", col.names = c("dose", "n_deaths", "n_subjects"))
print(Insect)
```

很容易理解這是一個典型的分組二項分佈數據 (grouped binary data)。每組不同的劑量，第二列，第三列分別是死亡數和實驗總數。另外一種個人二項分佈數據 (individual binary data) 的形式是這樣的：

```{r echo=FALSE}
dose <- c(rep("49.06", 10), rep("." , 3))
death <- c(rep(1, 6), rep(0,4), rep(".", 3))
data.frame(dose, death)
```
個人二項分佈數據其實就是把每個觀察對象的事件發生與否的信息都呈現出來。通常個人二項分佈數據又被稱爲**伯努利數據**，分組型的二項分佈數據被稱爲**二項數據**。兩種表達形式，但是存儲的是一樣的數據。

## 二項分佈數據的廣義線性迴歸模型

而所有的 GLM 一樣，二項分佈的 GLM 包括三個部分：

1. 因變量的分佈 Distribution：因變量應相互獨立，且服從二項分佈 <br> $$\begin{aligned} Y_i &\sim \text{Bin}(n_i, \pi_i), i = 1, \cdots, n \\ E(Y_i) &= \mu_i = n_i\pi_i\end{aligned}$$
2. 線性預測方程 Linear predictor：第 $i$ 名觀測對象的預測變量的線性迴歸模型 <br> $$\eta_i = \alpha + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}$$
3. 鏈接方程 Link function：鏈接方程連接的是 $\mu_i = n\pi_i$ 和線性預測方程。一個二項分佈因變量數據，可以有許多種鏈接方程：
    - $\mathbf{logit}:$ $$\text{logit}(\pi) = \text{ln}(\frac{\pi}{1-\pi})$$
    - $\mathbf{probit}:$ $$\text{probit}(\pi) = \Phi^{-1}(\pi)$$
    - $\mathbf{complementary\; log-log}:$ $$\text{cloglog}(\pi) = \text{ln}\{ - \text{ln}(1-\pi) \}$$
    - $\mathbf{log:}$ $$\text{log}(\pi) = \text{ln}(\pi)$$

## 注

1. 概率鏈接方程 $\text{probit}$，$\Phi$ 被定義爲標準正態分佈的累積概率方程 (Section \@ref(standardNormal))： $$\Phi(z) = \text{Pr}(Z \leqslant z), \text{ for } Z\sim N(0,1)$$
2. 二項分佈數據的標準參數 (canonical parameter) $\theta_i$ 的標準鏈接方程是 $\theta_i = \text{logit}(\pi_i)$。
3. $\text{logit, probit, complementary log-log}$ 三種鏈接方程都能達到把閾值僅限於 $0 \sim 1$ 之間的因變量概率映射到線性預測方程的全實數閾值 $(-\infty,+\infty)$ 的目的。但是最後一個 $\text{log}$ 鏈接方程只能映射全部的非零負實數 $(-\infty,0)$。
4. $\text{logit, probit}$ 鏈接方程都是以 $\pi= 0.5$ 爲對稱軸左右對稱的。但是 $cloglog$ 則沒有對稱的性質。
5. 鏈接方程 $\text{log}$ 具有可以直接被解讀爲對數危險度比 (log Risk Ratio)，所以也常常在應用中見到。對數鏈接方程還有其他的優點 (非塌陷性 non-collapsibility)，但是它的最大缺點是，有時候利用這個鏈接方程的模型無法收斂 (converge)。
6. $\text{logit}$ 鏈接方程是我們最常見的，也最直觀易於理解。利用這個鏈接方程擬合的模型的迴歸係數能夠直接被理解爲對數比值比 (log Odds Ratio)。
7. 如果是個人數據 (individual data)，那麼 $n_i = 1$，$i$ 是每一個觀測對象的編碼。那麼 $Y_i = 0\text{ or }1$，代表事件發生或沒發生/成功或者失敗。如果是分組數據 (grouped data)，$i$ 是每個組的編號，$n_i$ 指的是第 $i$ 組中觀測對象的人數，$Y_i$ 是第 $i$ 組的 $n$ 名對象中事件發生的次數/成功的次數。


------------------------

### Exercise. Link functions.

推導出鏈接參數分別是

1) $\text{log}$
2) $\text{logit}$
3) $\text{complementary log-log}$

時，用參數 $\alpha， \beta_1, \cdots, \beta_p$ 表達的參數 $\pi_i=?, E(Y_i)=\mu_i=?$

**解**

1) $\text{log}$
$$
\begin{aligned}
\text{ln}(\pi_i) & = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
\Rightarrow \pi_i & = e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}} \\
            \mu_i & = n_i\pi_i = n_i e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}}
\end{aligned}
$$


2) $\text{logit}$

$$
\begin{aligned}
\text{logit}(\pi_i) & = \text{ln}(\frac{\pi_i}{1-\pi_i})  \\
                    & = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
\Rightarrow \pi_i   & = \frac{e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}}}{1+e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}}} \\
              \mu_i & = \frac{n_i e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}}}{1+e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}}}
\end{aligned}
$$


3) $\text{complementary log-log}$

$$
\begin{aligned}
\text{cloglog}(\pi_i) & = \text{ln}\{ - \text{ln}(1-\pi) \} \\
                      & = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} \\
\Rightarrow \pi_i     & = 1 - e^{-e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}}} \\
            \mu_i     & = n_i\pi_i = n_i(1-e^{-e^{\alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}}})
\end{aligned}
$$

----------------------------------

## 邏輯迴歸模型迴歸係數的實際意義

邏輯迴歸 (logistic regression) 的模型可以寫成是

$$
\text{logist}(\pi_i) = \text{ln}(\frac{\pi_i}{1-\pi_i}) = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ips}
$$

假如觀察對象 $j$ 和 $i$ 兩人中，其餘的預測變量都相同，二者之間有且僅有最後一個預測變量相差一個單位：

$$
\begin{aligned}
\text{logit}(\pi_j) & = \text{ln}(\frac{\pi_j}{1-\pi_j}) = \alpha + \beta_1 x_{j1} + \beta_2 x_{j2} + \cdots + \beta_p x_{jps} \\
\text{logit}(\pi_i) & = \text{ln}(\frac{\pi_i}{1-\pi_i}) = \alpha + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ips} \\
\text{Because they are} & \text{ in the same model share the same parameters, and } x_{jps} = x_{ips} + 1\\
\Rightarrow \text{logit}(\pi_j) - \text{logit}(\pi_i) & = \beta_p (x_{jps} + 1 - x_{jps}) = \beta_p \\
\Rightarrow \beta_p & =  \text{ln}(\frac{\pi_j}{1-\pi_j})  -  \text{ln}(\frac{\pi_i}{1-\pi_i})  \\
                    & = \text{ln}(\frac{\frac{\pi_j}{1-\pi_j}}{\frac{\pi_i}{1-\pi_i}}) \\
                    & = \text{ln}(\text{Odds Ratio})
\end{aligned}
$$

所以迴歸係數 $\beta_p$ 可以被理解爲是 $j$ 與 $i$ 相比較時的對數比值比 log Odds Ratio。我們只要對迴歸係數求反函數，即可求得比值比。

## 邏輯迴歸實際案例



# 模型比較和擬合優度

# 計數型因變量

# 率的廣義線性迴歸

# 混雜，調整，交互作用

# 邏輯迴歸

# 分析策略

# 模型檢查

# Assessing model performance

# Matched studies

# 條件邏輯迴歸


# Multinomial Logistic Regression

# Ordinal Logistic Regression
