[
["index.html", "醫學統計學 前言", " 醫學統計學 王 超辰 Chaochen Wang 最近更新於 2017-12-08 前言 We are drowning in information and starving for knowledge. — Rutherford D. Roger 尚未想好寫什麼作前言。我只是默默地想留下一些筆記和思考。 本書用了兩個 R 包編譯，分別是 knitr (Xie 2015) 和 bookdown (Xie 2017)。以下是我的 R 進程信息： sessionInfo() ## R version 3.4.3 (2017-11-30) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 15063) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Chinese (Simplified)_China.936 ## [2] LC_CTYPE=Chinese (Simplified)_China.936 ## [3] LC_MONETARY=Chinese (Simplified)_China.936 ## [4] LC_NUMERIC=C ## [5] LC_TIME=Chinese (Simplified)_China.936 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets ## [6] methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_3.4.3 backports_1.1.1 bookdown_0.5 ## [4] magrittr_1.5 rprojroot_1.2 tools_3.4.3 ## [7] htmltools_0.3.6 rstudioapi_0.7 yaml_2.1.14 ## [10] Rcpp_0.12.14 stringi_1.1.6 rmarkdown_1.8 ## [13] knitr_1.17 stringr_1.2.0 digest_0.6.12 ## [16] evaluate_0.10.1 王超辰 2017年9月於倫敦 参考文献 "],
["author.html", "我是誰", " 我是誰 歡迎參觀我的個人主頁。 "],
["intro.html", "第 1 章 概率論入門：定義與公理 1.1 三個概率公理： 1.2 條件概率 Conditional probability 1.3 獨立 (independence) 的定義 1.4 賭博問題 1.5 賭博問題的答案", " 第 1 章 概率論入門：定義與公理 1.1 三個概率公理： 對於任意事件 \\(A\\)，它發生的概率 \\(P(A)\\) 滿足這樣的不等式： \\(0 \\leqslant P(A) \\leqslant 1\\) \\(P(\\Omega)=1\\) , \\(\\Omega\\) 是全樣本空間 (total sample space) 對於互斥（相互獨立）的事件 \\(A_1, A_2, \\dots, A_n\\) 有如下的等式關係： \\(P(A_1\\cup A_2 \\cup \\cdots \\cup A_n)=P(A_1)+P(A_2)+\\cdots+P(A_n)\\) 你是不是覺得上面三條公理都是廢話。 不用擔心，我也是這麼覺得的。因爲所有人都認同的道理，才能成爲公理 (axiom)，因爲它們是不需要證明的自然而然形成的人人都接受的觀念。(axiom: a saying that is widely accepted on its own merits; its truth is assumed to be self-evident) 然而，正是這樣顯而易見的道理，確是拿來建築理論的基石，千萬不能小看了他們。例如，我們看下面這個看似也應該成爲公理的公式，你能證明嗎： \\(P(A_1\\cup A_2) = P(A_1) + P(A_2) - P(A_1 \\cap A_2)\\) 證明： 先考慮 \\(A_1 \\cup A_2\\) 是什麼（拆分成三個互斥事件） \\(A_1 \\cup A_2 = (A_1\\cap \\bar{A_2})\\cup(\\bar{A_1}\\cap A_2)\\cup(A_1\\cap A_2)\\) 運用上面的公理2 3 \\(\\therefore P(A_1 \\cup A_2) = P(A_1\\cap \\bar{A_2}) + P(\\bar{A_1}\\cap A_2) + P(A_1\\cap A_2) \\;\\;\\;\\;\\;\\;(1)\\) 再考慮 \\(A_1=(A_1\\cap A_2)\\cup(A_1\\cap\\bar{A_2})\\) 繼續拆分成兩個互斥事件 \\(\\therefore P(A_1)=P(A_1\\cap A_2)+P(A_1\\cap\\bar{A_2})\\) 整理一下： \\(P(A_1\\cap\\bar{A_2})=P(A_1)-P(A_1\\cap A_2)\\) 同理可得: \\(P(\\bar{A_1}\\cap A_2)=P(A_2)-P(A_1\\cap A_2)\\) 代入上面第(1)式可得： \\(P(A_1 \\cup A_2) =P(A_1)-P(A_1\\cap A_2)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+P(A_2)-P(A_1\\cap A_2)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+P(A_1\\cap A_2)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;=P(A_1) + P(A_2) - P(A_1 \\cap A_2)\\) 1.2 條件概率 Conditional probability \\(P(A|S)=\\frac{P(A\\cap S)}{P(S)}\\) \\(P(A\\cap S) = P(A|S)P(S)\\) 1.3 獨立 (independence) 的定義 兩個事件定義爲互爲獨立時 (\\(A\\) and \\(B\\) are said to be independent if and only if) \\[P(A\\cap B)=P(A)P(B)\\] 因爲從條件概率的概念我們已知 \\(P(A\\cap B) = P(A|B)P(B)\\) 所以\\(P(A|B)=P(A)\\) 即：事件 \\(B\\) 無法提供事件 \\(A\\) 的任何有效訊息 (\\(A, B\\) 互相獨立) 1.4 賭博問題 終於來到本次話題的“重點”了。 假設你在一個電視遊戲節目。有上圖一樣的三扇門。其中一扇門後面有一輛保時捷，另兩扇門後面則是(味道奇特的)山羊。遊戲規則是主持人會讓你先選擇其中一扇門（先不打開你選的這扇門）。主持人隨後打開另外兩扇門中的一扇沒有保時捷的門。主持人問你，你要堅持選擇之前選中的那扇門，還是要改變主意換一扇門去猜是否可以猜中保時捷。 請問，堅持選擇之前選中的門猜中保時捷的概率高，還是主持人打開一扇門以後改變主意猜中保時捷的概率更高呢？ 1.5 賭博問題的答案 答案是：必須改變主意才能提高中獎概率。 上述情況下，最簡單的是用概率樹 (probability tree) 來做決定： 解說一下： 假定保時捷在1號門後，你第一次選擇了1號門，那麼此時主持人可以任意打開2號或者三號門（因爲他們後面都沒有保時捷）。 假定保時捷在1號門後，你第一次選了2號門，那麼此時主持人只能打開3號門（因爲一號門後是保時捷，按照遊戲規則主持人不能打開）。 假定保時捷在1號門後，你第一次選了3號門，那麼此時主持人只能打開2號門（因爲一號門後是保時捷，按照遊戲規則主持人不能打開）。 所以按照圖中給出的計算概率樹的過程可以得到: \\[P[change]=\\frac{1}{3}+\\frac{1}{3}=\\frac{2}{3}\\\\ P[not\\; change]=\\frac{1}{6}+\\frac{1}{6}=\\frac{1}{3}\\] 你是否選擇了改變主意了呢？ "],
["Bayes-Definition.html", "第 2 章 Bayes 貝葉斯理論的概念", " 第 2 章 Bayes 貝葉斯理論的概念 許多時候，我們需要將概率中的條件相互對調。 例如： 在已知該人羣中有20%的人有吸菸習慣(\\(P(S)\\))，吸菸的人有9%的概率有哮喘(\\(P(A|S)\\))，不吸菸的人有7%的概率有哮喘(\\(P(A|\\bar{S})\\))的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有多大的概率是一個菸民？也就是要求 \\(P(S|A)\\) 這裏先引入貝葉斯的概念： 我們可以將 \\(P(A\\cap S)\\) 寫成： \\[P(A\\cap S)=P(A|S)P(S)\\\\or\\\\ P(A\\cap S)=P(S|A)P(A)\\] 這兩個等式是完全等價的。我們將他們連起來： \\[P(S|A)P(A)=P(A|S)P(S)\\\\ \\Rightarrow P(S|A)=\\frac{P(A|S)P(S)}{P(A)}\\] 是不是看起來又像是寫了一堆廢話？ 沒錯，你看出來是一堆廢話的時候，證明你也同意這背後的簡單邏輯。 再繼續，我們可以利用另外一個廢話：\\(\\because S+\\bar{S}=1\\\\ \\therefore P(A)=P(A\\cap S)+P(A\\cap\\bar{S})\\) 用上面的公式替換掉 \\(P(A\\cap S)+P(A\\cap\\bar{S}） \\\\ \\therefore P(A)=P(A|S)P(S)+P(A|\\bar{S})P(\\bar{S})\\) 可以得到貝葉斯理論公式： \\[P(S|A)=\\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\\bar{S})P(\\bar{S})}\\] 回到上面說到的哮喘人中有多少比例吸菸的問題。可以繼續使用概率樹來方便的計算： \\[\\begin{align} P(S|A) &amp;= \\frac{P(A|S)P(S)}{P(A|S)P(S)+P(A|\\bar{S})P(\\bar{S})} \\\\ &amp;= \\frac{0.09\\times0.2}{0.09\\times0.2+0.07\\times0.8} \\\\ &amp;= 0.24 \\end{align}\\] 所以我們的結論就是，在已知該人羣中有20%的人有吸菸習慣(\\(P(S)\\))，吸菸的人有9%的概率有哮喘(\\(P(A|S)\\))，不吸菸的人有7%的概率有哮喘(\\(P(A|\\bar{S})\\))的前提下，有個人前來門診，發現是哮喘患者，那麼這個人有24% 的概率是一個菸民(\\(P(S|A)\\))。 "],
["-expectation-or-mean-variance.html", "第 3 章 期望 Expectation (或均值 or mean) 和 方差 Variance 3.1 方差的性質：", " 第 3 章 期望 Expectation (或均值 or mean) 和 方差 Variance 期望（或均值）是用來描述一組數據中心位置的指標（另一個是中位數 Median）。 對於離散型隨機變量 \\(X\\) (discrete random variables)，它的期望被定義爲： \\[E(X)=\\sum_x xP(X=x)\\] 所以就是將所有 \\(X\\) 可能取到的值乘以相應的概率後求和。這個期望（或均值）常常用希臘字母 \\(\\mu\\) 來標記。 方差 Variance 是衡量一組數據變化幅度(dispersion/variability)的指標之一。 方差的定義是： \\[Var(X)=E((X-\\mu)^2)\\\\其中，\\mu=E(x)\\] 實際上我們更加常用的是它的另外一個公式： \\[Var(X)=E(X^2)-E(X)^2\\] 證明 上面兩個方差公式相等 \\[\\begin{align} Var(x) &amp;= E((X-\\mu)^2) \\\\ &amp;= E(X^2-2X\\mu+\\mu^2)\\\\ &amp;= E(X^2) - 2\\mu E(X) + \\mu^2\\\\ &amp;= E(X^2) - 2\\mu^2 + \\mu^2 \\\\ &amp;= E(X^2) - \\mu^2 \\\\ &amp;= E(X^2) - E(X)^2 \\end{align}\\] 3.1 方差的性質： \\(Var(X+b)=Var(X)\\) \\(Var(aX)=a^2Var(X)\\) \\(Var(aX+b)=a^2Var(X)\\) "],
["-bernoulli-distribution.html", "第 4 章 伯努利分佈 Bernoulli distribution", " 第 4 章 伯努利分佈 Bernoulli distribution 伯努利分佈，說的就是一個簡單的二分變量 (1, 0)，它取1時的概率如果是 \\(\\pi\\)。那麼我們可以計算這個分佈的期望值: \\[\\begin{align} E(X) &amp;=\\sum_x xP(X=x) \\\\ &amp;=1\\times\\pi + 0\\times(1-\\pi)\\\\ &amp;=\\pi \\end{align}\\] 由於 \\(x=x^2\\)，因爲 \\(x=0,1\\), 所以 \\(E[X^2]=E[X]\\)，那麼方差爲： \\[\\begin{align} Var(X) &amp;=E[X^2]-E[X]^2 \\\\ &amp;=E[X]-E[X]^2 \\\\ &amp;=\\pi - \\pi^2 \\\\ &amp;=\\pi(1-\\pi) \\end{align}\\] 證明，\\(X,Y\\) 爲互爲獨立的隨機離散變量時，a) \\(E(XY)=E(X)E(Y)\\) ; b) \\(Var(X+Y)=Var(X)+Var(Y)\\) 證明 \\[\\begin{align} E(XY) &amp;= \\sum_x\\sum_y xyP(X=x, Y=y) \\\\ \\because &amp;\\; X,Y are\\;independent\\;to\\;each\\;other \\\\ \\therefore &amp;= \\sum_x\\sum_y xyP(X=x)P(Y=y)\\\\ &amp;=\\sum_x xP(X=x)\\sum_y yP(Y=y)\\\\ &amp;=E(X)E(Y) \\end{align}\\] 證明 根據方差的定義： \\[\\begin{align} Var(X+Y) &amp;= E((X+Y)^2)-E(X+Y)^2 \\\\ &amp; \\; Expand \\\\ &amp;=E(X^2+2XY+Y^2)-(E(X)+E(Y))^2\\\\ &amp;=E(X^2)+E(Y^2)+2E(XY)\\\\ &amp;\\;\\;\\; - E(X)^2-E(Y)^2-2E(X)E(Y)\\\\ &amp;\\; We\\;just\\;showed\\; E(XY)=E(X)E(Y)\\\\ &amp;=E(X^2)-E(X)^2+E(Y^2)-E(Y)^2 \\\\ &amp;=Var(X)+Var(Y) \\end{align}\\] "],
["-binomial-distribution.html", "第 5 章 二項分佈的概念 Binomial distribution 5.1 二項分佈的期望和方差 5.2 超幾何分佈 hypergeometric distribution 5.3 樂透中獎概率問題：", " 第 5 章 二項分佈的概念 Binomial distribution 二項分佈在醫學研究中至關重要，一組二項分佈的數據，指的通常是 \\(n\\) 次相互獨立的成功率爲 \\(\\pi\\) 的伯努利實驗 (\\(n\\) independent Bernoulli trials) 中成功的次數。 當 \\(X\\) 服從二項分佈，記爲 \\(X \\sim binomial(n, \\pi)\\) 或\\(X \\sim bin(n, \\pi)\\)。它的(第 \\(x\\) 次實驗的)概率被定義爲： \\[\\begin{align} P(X=x) &amp;= ^nC_x\\pi^x(1-\\pi)^{n-x} \\\\ &amp;= \\binom{n}{x}\\pi^x(1-\\pi)^{n-x} \\\\ &amp; for\\;\\; x = 0,1,2,\\dots,n \\end{align}\\] 5.1 二項分佈的期望和方差 期望 \\(E(X)\\) 若 \\(X \\sim bin(n,\\pi)\\)，那麼 \\(X\\) 就是這一系列獨立伯努利實驗中成功的次數。 用 \\(X_i, i =1,\\dots, n\\) 標記每個相互獨立的伯努利實驗。 那麼我們可以知道 \\(X=\\sum_{i=1}^nX_i\\)。 \\[\\begin{align} E(X) &amp;= E(\\sum_{i=1}^nX_i)\\\\ &amp;= E(X_1+X_2+\\cdots+X_n) \\\\ &amp;= E(X_1)+E(X_2)+\\cdots+E(X_n)\\\\ &amp;= \\sum_{i=1}^nE(X_i)\\\\ &amp;= \\sum_{i=1}^n\\pi \\\\ &amp;= n\\pi \\end{align}\\] 方差 \\(Var(X)\\) \\[\\begin{align} Var(X) &amp;= Var(\\sum_{i=1}^nX_i) \\\\ &amp;= Var(X_i+X_2+\\cdots+X_n) \\\\ &amp;= Var(X_i)+Var(X_2)+\\cdots+Var(X_n) \\\\ &amp;= \\sum_{i=1}^nVar(X_i) \\\\ &amp;= n\\pi(1-\\pi) \\\\ \\end{align}\\] 5.2 超幾何分佈 hypergeometric distribution 假設我們從總人數爲 \\(N\\) 的人羣中，採集一個樣本 \\(n\\)。假如已知在總體人羣中(\\(N\\))有 \\(M\\) 人患有某種疾病。請問採集的樣本 \\(X=n\\) 中患有這種疾病的人，服從怎樣的分佈？ 從人羣(\\(N\\))中取出樣本(\\(n\\))，有 \\(^NC_n\\) 種方法。 從患病人羣(\\(M\\))中取出患有該病的人(\\(x\\))有 \\(^MC_x\\) 種方法。 樣本中不患病的人(\\(n-x\\))被採樣的方法有 \\(^{N-M}C_{n-x}\\) 種。 採集一次 \\(n\\) 人作爲樣本的概率都一樣。因此： \\[P(X=x)=\\frac{\\binom{M}{x}\\binom{N-M}{n-x}}{\\binom{N}{n}}\\] 5.3 樂透中獎概率問題： 從數字 \\(1\\sim59\\) 中選取 \\(6\\) 個任意號碼 開獎時從 \\(59\\) 個號碼球中隨機抽取 \\(6\\) 個 如果六個號碼全部猜中(不分順序)，你可以成爲百萬富翁。請問一次猜中全部 \\(6\\) 個號碼的概率是多少？ 從 \\(59\\) 個號碼中隨機取出任意 \\(6\\) 個號碼的方法有 \\(^{59}C_6\\) 種。 \\[^{59}C_6=\\frac{59!}{6!(59-6)!}=45,057,474\\] 每次選取六個號碼做爲一組的可能性相同，所以，你買了一組樂透號碼，能中獎的概率就是 \\(1/45,057,474 = 0.00000002219\\)。你還會再去買彩票麼？ 5.3.1 如果我只想中其中的 \\(3\\) 個號碼，概率有多大？ 用超幾何分佈的概率公式： \\[\\begin{align} P(X=3) &amp;= \\frac{^6C_3\\times ^{53}C_3}{^{59}C_6} \\\\ &amp;= 0.010 \\end{align}\\] 你有 \\(1\\%\\) 的可能中獎。換句話說，如果中三個以上的數字算中獎的話，你買的彩票中獎的概率低於 \\(1\\%\\)。是不是覺得下次送錢給博彩公司的時候還不如跟我一起喝一杯咖啡划算？ "],
["-poisson-distribution.html", "第 6 章 泊松分佈 Poisson Distribution", " 第 6 章 泊松分佈 Poisson Distribution 當一個事件，在一段時間 (\\(T\\)) 中可能發生的次數是 \\(\\lambda\\) 。那麼我們可以認爲，經過時間 \\(T\\)，該時間發生的期望次數是 \\(E(X)=\\lambda T\\)。 利用微分思想，將這段時間 \\(T\\) 等分成 \\(n\\) 個時間段，當 \\(n\\rightarrow\\infty\\) 直到每個微小的時間段內最多發生一次該事件。 那麼 每個微小的時間段，可以視爲是一個伯努利實驗（有事件發生或者沒有） 那麼這整段時間 \\(T\\) 內發生的事件可以視爲是一個二項分佈實驗。 令 \\(X=\\) 一次事件發生時所經過的所有時間段。 \\(X \\sim Bin(n, \\pi)\\)，其中 \\(n\\rightarrow\\infty\\)，\\(n\\) 爲時間段。 在每個分割好的時間段內，事件發生的概率都是：\\(\\pi=\\frac{\\lambda T}{n}\\) 期望 \\(\\mu=\\lambda T \\Rightarrow \\pi=\\mu/n\\) 所以 \\(X\\) 的概率方程就是： \\[\\begin{align} P(X=x) &amp;= \\binom{n}{x}\\pi^x(1-\\pi)^{n-x} \\\\ &amp;= \\binom{n}{x}(\\frac{\\mu}{n})^x(1-\\frac{\\mu}{n})^{n-x} \\\\ &amp;= \\frac{n!}{x!(n-x)!}(\\frac{\\mu}{n})^x(1-\\frac{\\mu}{n})^{n-x} \\\\ &amp;=\\frac{n!}{n^x(n-x)!}\\frac{\\mu^x}{x!}(1-\\frac{\\mu}{n})^{n-x}\\\\ when\\; n\\rightarrow\\infty &amp;\\; x \\ll n\\\\ \\frac{n!}{n^x(n-x)!} &amp;=\\frac{n(n-1)\\dots(n-x+1)}{n^x} \\rightarrow 1\\\\ (1-\\frac{\\mu}{n})^{n-x} &amp;\\approx (1-\\frac{\\mu}{n})^n \\rightarrow e^{-\\mu}\\\\ the\\;probability\\;function&amp;\\;of\\;a\\;Poisson\\;distribution \\\\ P(X=x) &amp;\\rightarrow \\frac{\\mu^x}{x!}e^{-\\mu} \\end{align}\\] 當數據服從泊松分佈時，記爲 \\(X\\sim Poisson(\\mu=\\lambda T)\\;\\; or\\;\\; X\\sim Poi(\\mu)\\) 證明泊松分佈的參數特徵： \\(E(X)=\\mu\\) \\[\\begin{align} E(X) &amp;= \\sum_{x=0}^\\infty xP(X=x) \\\\ &amp;= \\sum_{x=0}^\\infty x\\frac{\\mu^x}{x!}e^{-\\mu} \\\\ &amp;= 0+ \\sum_{x=1}^\\infty x\\frac{\\mu^x}{x!}e^{-\\mu} \\\\ &amp;= \\sum_{x=1}^\\infty \\frac{\\mu^x}{(x-1)!}e^{-\\mu} \\\\ &amp;= \\mu\\sum_{x=1}^\\infty \\frac{\\mu^{x-1}}{(x-1)!}e^{-\\mu} \\\\ replace\\; &amp;x\\; with\\; all\\; i=x-1 \\\\ &amp;= \\mu\\sum_{i=0}^\\infty \\frac{\\mu^{i}}{i!}e^{-\\mu} \\\\ notice\\; that\\; &amp;the\\; right\\; side \\sum_{i=0}^\\infty \\frac{\\mu^{i}}{i!}e^{-\\mu}=1 is \\\\ the\\;sum\\;of\\;all\\;&amp;probability\\;of\\;a\\;Poisson\\;distribution\\\\ &amp;= \\mu \\end{align}\\] \\(Var(x)=\\mu\\) 爲了找到 \\(Var(X)\\)，我們用公式 \\(Var(X)=E(X^2)-E(X)^2\\) 我們需要找到 \\(E(X^2)\\) \\[\\begin{align} E(X^2) &amp;= \\sum_{x=0}^\\infty x^2\\frac{\\mu^x}{x!}e^{-\\mu} \\\\ &amp;= \\mu \\sum_{x=1}^\\infty x\\frac{\\mu^{x-1}}{(x-1)!}e^{-\\mu} \\\\ replace\\; &amp;x\\; with\\; all\\; i=x-1 \\\\ &amp;= \\mu \\sum_{i=0}^\\infty (i+1)\\frac{\\mu^{i}}{i!}e^{-\\mu} \\\\ &amp;= \\mu(\\sum_{i=0}^\\infty i\\frac{\\mu^i}{i!}e^{-\\mu} + \\sum_{i=0}^\\infty \\frac{\\mu^i}{i!}e^{-\\mu}) \\\\ &amp;= \\mu(E(X)+1) \\\\ &amp;= \\mu^2+\\mu \\\\ Var(X) &amp;= E(X^2) - E(X)^2 \\\\ &amp;= \\mu^2 + \\mu -\\mu^2 \\\\ &amp;= \\mu \\end{align}\\] "],
["section-7.html", "第 7 章 正態分佈 7.1 概率密度曲線 probability density function， PDF 7.2 正態分佈 7.3 標準正態分佈", " 第 7 章 正態分佈 7.1 概率密度曲線 probability density function， PDF 一個隨機連續型變量 \\(X\\) 它的性質由一個對應的概率密度方程 (probability density function, PDF) 決定。 在給定的範圍區間內，如 \\(a\\sim b, (a &lt; b)\\)，它的概率滿足: \\[P(a\\leqslant X \\leqslant b) = \\int_a^bf(x)dx\\] 這個相關的方程，在 \\(a\\sim b\\) 區間內的積分，就是這個連續變量在這個區間內取值的概率。 # R codes for drawing a standard normal distribution by using ggplot2 library(ggplot2) p &lt;- ggplot(data.frame(x=c(-3,3)), aes(x=x)) + stat_function(fun = dnorm) p + annotate(&quot;text&quot;, x=2, y=0.3, parse=TRUE, label=&quot;frac(1, sqrt(2*pi)) * e ^(-z^2/2)&quot;) + theme(plot.subtitle = element_text(vjust = 1), plot.caption = element_text(vjust = 1), axis.text.x = element_text(size = 12), axis.text.y = element_text(size = 12), plot.title = element_text(size = 10, face = &quot;bold&quot;, hjust = 0.5), panel.background = element_rect(fill = &quot;ivory&quot;)) + labs(title = &quot;Probability density functions \\n for standard normal distribution&quot;, x = NULL, y = NULL) + stat_function(fun = dnorm, xlim = c(-1.3,0.4), geom = &quot;area&quot;,fill=&quot;#00688B&quot;, alpha= 0.2) 图 7.1: Probability Density Function of a Standard Normal Distribution 注意：整個方程的曲線下面積等於 \\(1\\)： \\[\\int_{-\\infty}^\\infty f(x)dx=1\\] 期望 \\(E(X)=\\int_{-\\infty}^\\infty xf(x)dx\\) 方差 \\(Var(X)=\\int_{-\\infty}^\\infty (x-\\mu)^2f(x)dx\\) 7.2 正態分佈 如果一組數據服從正態分佈，我們通常用它的期望（或者叫平均值）\\(\\mu\\)，和它的方差 \\(\\sigma^2\\)，來描述這組數據。記爲： \\[X \\sim N(\\mu, \\sigma^2)\\] 它的概率密度方程可以表述爲： \\[f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\] \\(E(x) =\\mu\\) \\(Var(x)=\\sigma^2\\) 7.3 標準正態分佈 標準正態分佈的期望（或者均值）爲 \\(0\\)，方差爲 \\(1\\) 記爲：\\(Z \\sim N(0,1)\\) 它的概率密度方程表述爲： \\[\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{z^2}{2})\\] 它的累積分佈方程 (cumulative distribution function， CDF)，是將概率密度方程 (PDF) 積分以後獲得的方程。通常我們記爲 \\(\\Phi(z)\\) 再看一下標準正態分佈的概率密度方程曲線： 图 7.2: Probability Density function of a Standard Normal Distribution 95% 的曲線下面積在標準差 standard deviation \\(-1.96\\sim1.96\\) 之間的區域。 而且，\\(\\phi(-x)=1-\\phi(x)\\) 任何一個正態分佈都可以通過下面的公式，標準化成爲標準正態分佈： \\[Z=\\frac{X-\\mu}{\\sigma}\\] "],
["CLT.html", "第 8 章 中心極限定理 the Central Limit Theorem 8.1 協方差 Covariance 8.2 相關 Correlation 8.3 中心極限定理 the Central Limit Theorem 8.4 二項分佈的正態分佈近似 8.5 泊松分佈的正態分佈近似 8.6 正態分佈模擬的校正：continuity corrections 8.7 兩個連續隨機變量 8.8 兩個連續隨機變量 例子： 8.9 條件分佈和邊緣分佈的概念 8.10 條件分佈和邊緣分佈的例子", " 第 8 章 中心極限定理 the Central Limit Theorem 最近明顯可以感覺到課程的步驟開始加速。看我的課表： 手機畫面太小了。早上都是9點半開始，下午基本都是到5點。週一更慘，到7點。週二-週五中午都被統計中心的講座佔據。簡直是非人的生活。 這周概率論基礎結束。中心極限定理講完以後我們正式進入了 Inference 統計推斷的課程。我們花了一天時間講什麼是樣本估計 (Estimation)，什麼是參數精確度 (Precision)，什麼是自由度 (degree of freedom)，怎樣進行不偏的估計 (unbiased inference)。然後還有似然方程 (likelihood function)。 今天的更新還是簡單的把概率論掃尾一下。感受一下中心極限定理的偉大。 8.1 協方差 Covariance 之前我們定義過，兩個獨立連續隨機變量 \\(X,Y\\) 之和的方差 Variance ： \\[Var(X+Y)=Var(X)+Var(Y)\\] 然而如果他們並不相互獨立的話： \\[\\begin{aligned} Var(X+Y) &amp;= E[((X+Y)-E(X+Y))^2] \\\\ &amp;= E[(X+Y)-(E(X)+E(Y))^2] \\\\ &amp;= E[(X-E(X)) - (Y-E(Y))^2] \\\\ &amp;= E[(X-E(X))^2+(Y-E(Y))^2 \\\\ &amp; \\;\\;\\; +2(X-E(X))(Y-E(Y))] \\\\ &amp;= Var(X)+Var(Y)+2E[(X-E(X))(Y-E(Y))] \\end{aligned}\\] 可以發現在兩者和的方差公式展開之後多了一部分 \\(E[(X-E(X))(Y-E(Y))]\\)。 這個多出來的一部分就說明了二者 \\((X, Y)\\) 之間的關係。它被定義爲協方差 (Covariance): \\[Cov(X,Y) = E[(X-E(X))(Y-E(Y))]\\] 所以： \\[Var(X+Y)=Var(X)+Var(Y)+2Cov(X,Y)\\] 要記住，協方差只能用於評價\\(X,Y\\)之間的線性關係 (Linear Association)。 以下是協方差 (Covariance) 的一些特殊性質： \\(Cov(X,X)=Var(X)\\) \\(Cov(X,Y)=Cov(Y,X)\\) \\(Cov(aX,bY)=ab\\:Cov(X,Y)\\) \\(Cov(aR+bS,cX+dY)=ac\\:Cov(R,X)+ad\\:Cov(R,Y)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+bc\\:Cov(S,X)+bd\\:Cov(S,Y)\\) \\(Cov(aX+bY,cX+dY)=ac\\:Var(X)+ad\\:Var(Y)\\\\ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+(ad+bc)Cov(X,Y)\\) \\(Cov(X+Y,X-Y)=Var(X)-Var(Y)\\) If \\(X, Y\\) are independent. \\(Cov(X,Y)=0\\) But not vise-versa ! 8.2 相關 Correlation 協方差雖然\\(Cov(X,Y)\\) 的大小很大程度上會被他們各自的單位和波動大小左右。 我們將協方差標準化(除以各自的標準差 s.d.) (standardization) 之後，就可以得到相關係數 Corr (\\(-1\\sim1\\)): \\[Corr(X,Y)=\\frac{Cov(X,Y)}{SD(X)SD(Y)}=\\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\\] 8.3 中心極限定理 the Central Limit Theorem 如果從人羣中多次選出樣本量爲 \\(n\\) 的樣本，並計算樣本均值, \\(\\bar{X}_n\\)。那麼這個樣本均值 \\(\\bar{X}_n\\) 的分佈，會隨着樣本量增加 \\(n\\rightarrow\\infty\\)，而接近正態分佈。 偉大的中心極限定理告訴我們： 當樣本量足夠大時，樣本均值 \\(\\bar{X}_n\\) 的分佈爲正態分佈，這個特性與樣本來自的人羣的分佈 \\(X_i\\) 無關。 再說一遍： 如果對象是獨立同分佈 i.i.d (identically and independently distributed)。那麼它的總體期望和方差分別是: \\(E(X)=\\mu;\\;Var(X)=\\sigma^2\\)。 根據中心極限定理，可以得到： 當樣本量增加，樣本均值的分佈服從正態分佈： \\[\\bar{X}_n\\sim N(\\mu, \\frac{\\sigma^2}{n})\\] 也可以寫作，當樣本量增加： \\[\\sum_{i=1}^nX_i \\sim N(n\\mu,n\\sigma^2)\\] 有了這個定理，我們可以拋開樣本空間(\\(X\\))的分佈，也不用假定它服從正態分佈。 但是樣本的均值，卻總是服從正態分佈的。簡直是太完美了！！！！！！ 8.4 二項分佈的正態分佈近似 假設我們有大量(\\(n\\rightarrow\\infty\\))的二項分佈實驗 \\(X\\sim Bin(n, \\pi)\\) 根據二項分佈的概率公式，計算將會變得很繁瑣複雜。 解決辦法：應用中心極限定理。 中心極限定理告訴我們，當樣本量足夠大時: \\[X\\sim N(n\\pi, n\\pi(1-\\pi))\\] 問題在於，多大的 \\(n\\) 才能算大樣本呢？ 當且僅當 (only and if only) \\(n&gt;20\\) AND \\(n\\pi&gt;5\\) AND \\(n(1-\\pi)&gt;5\\) 8.5 泊松分佈的正態分佈近似 假設時間 \\(t\\) 內某事件的發生次數服從泊松分佈 \\(X\\sim Po(\\mu)\\)。 考慮將這段時間 \\(t\\) 等分成 \\(n\\) 個時間段。那麼第 \\(i\\) 時間段內事件發生次數依舊服從泊松分佈 \\(X_i\\sim Po(\\frac{\\mu}{n})\\)。且 \\(E(X_i)=\\mu/n, Var(X_i)=\\mu/n\\)。 那麼原先的 \\(X\\) 可以被視爲是將這無數的小時間段的 \\(X_i\\) 相加。應用中心極限定理： \\[X=\\sum_{i=1}^nX_i\\sim N(\\frac{n\\mu}{n}, \\frac{n\\mu}{n})\\] 需要注意的是，這段時間 (\\(t\\)) 內發生的事件次數 (\\(\\lambda\\)) : \\(\\lambda t =\\mu&gt;10\\) ，這樣的正態分佈模擬才能成立。 8.6 正態分佈模擬的校正：continuity corrections 如果我們使用正態分佈來模擬離散變量的分佈，常常需要用到正態分佈模擬的矯正。 例如：我們如果用正態分佈模擬來計算 \\(P(X=15)\\)，那麼實際上我們應該計算的是 \\(P(14.5&lt;X&lt;15.5)\\)。 8.6.1 例題 已知 \\(X\\sim Bin(100,0.5)\\)，求 \\(P(X&gt;60)\\) 解 \\[\\begin{aligned} \\because X&amp;\\sim Bin(100, 0.5) \\\\ \\therefore E(X) &amp;=n\\pi=50 \\\\ Var(X) &amp;= n\\pi(1-\\pi) =25=5^2\\\\ P(X&gt;60) &amp;= 1-P(X\\leqslant60) \\\\ &amp;= 1-P(Z\\leqslant\\frac{60.5-50}{\\sqrt{25}}) \\\\ &amp;= 1-P(Z\\leqslant2.1) \\\\ &amp;= 1-\\Phi(2.1) \\\\ &amp;= 1-0.982 = 0.018 \\end{aligned}\\] # 快來看實際用傻瓜算法計算獲得的概率： 1-pbinom(60, size=100, prob=0.5) ## [1] 0.0176 # 快來看用中心極限定理模擬正態分佈獲得的概率： 1-pnorm((60.5-50)/sqrt(25)) ## [1] 0.01786 ## Warning in library(package, lib.loc = lib.loc, ## character.only = TRUE, logical.return = TRUE, : there ## is no package called &#39;ggfortify&#39; 图 8.1: Probability of 60 successes out of 100 Binomial trials, probability of success = 0.75 已知 \\(X\\sim Bin(48, 0.75)\\), 求 \\(P(30&lt;X&lt;39)\\) 解 \\[ \\begin{aligned} \\because B &amp;\\; \\sim Bin(48, 0.75) \\\\ \\therefore E(X) &amp;\\; =n\\pi=36 \\\\ Var(X) &amp;\\; =n\\pi(1-\\pi)=9=3^2 \\\\ P(30&lt;X&lt;39) &amp;\\; = P(31\\leqslant X\\leqslant 38)\\\\ &amp;\\; = P(30.5\\leqslant Y \\leqslant 38.5) \\\\ Y\\;is\\;the&amp;\\;normal\\;approximation \\\\ &amp;\\;= P(Y&lt;38.5) - P(Y&lt;30.5) \\\\ &amp;\\;= P(Z\\leqslant\\frac{38.5-36}{3})- P(Z\\leqslant\\frac{30.5-36}{3}) \\\\ &amp;\\;= P(Z\\leqslant0.833) - P(Z\\leqslant-1.833) \\\\ &amp;\\;= \\Phi(0.833)-\\Phi(-1.833) \\\\ &amp;\\;= 0.798-0.033 = 0.764 \\end{aligned} \\] # 快來看實際用傻瓜算法計算獲得的概率： pbinom(38, size=48, prob=0.75)-pbinom(30, size=48, prob=0.75) ## [1] 0.7578 # 快來看用中心極限定理模擬正態分佈獲得的概率： pnorm((38.5-36)/sqrt(9)) - pnorm((30.5-36)/sqrt(9)) ## [1] 0.7643 图 8.2: Probability of 30-39 successes out of 48 Binomial trials, probability of success = 0.75 從上面兩個例題也能看出，\\(n\\) 越小，正態分佈模擬的誤差就越大。 已知 \\(X \\sim Poisson(30)\\) 求 \\(P(X\\leqslant20)\\)。 解 \\[\\because E(X)=\\mu=30, \\;Var(X)=\\mu=30=(\\sqrt{30})^2 \\\\ \\begin{aligned} Pr(X\\leqslant20) &amp;= P(Z\\leqslant\\frac{20.5-30}{\\sqrt{30}}) \\\\ &amp;= P(Z\\leqslant-1.734) \\\\ &amp;= \\Phi(-1.734) \\\\ &amp;= 0.0414 \\end{aligned} \\] # 快來看實際用傻瓜算法計算獲得的概率： ppois(20, lambda=30) ## [1] 0.03528 # 快來看用中心極限定理模擬正態分佈獲得的概率： pnorm((20.5-30)/sqrt(30)) ## [1] 0.04142 這兩個其實有些小差距。不過看下圖，其模擬還是很到位的。只是正態分佈的面積明顯確實比泊松分佈的小柱子面積要大一些。 图 8.3: Probability of less than 20 events happen when the expectation is 30 已知 \\(X_1, X_2 \\stackrel{i.i.d}{\\sim} Poi(30)\\) 求 \\(P(X_1+X_2\\leqslant40)\\)。 解 \\[ \\begin{aligned} E(X_1+X_2) &amp;\\;= E(X_1)+E(X_2) = 30+30 = 60\\\\ Var(X_1+X_2) &amp;\\;= Var(X_1)+Var(X_2) = 30+30 \\\\ &amp;\\;= (\\sqrt{60})^2 \\\\ P(X_1+X_2\\leqslant 40) &amp;\\;= P(Z \\leqslant \\frac{40.5-60}{\\sqrt{60}}) \\\\ &amp;\\;= P(Z\\leqslant-2.517) \\\\ &amp;\\;= \\Phi(-2.517) \\\\ &amp;\\;= 0.006 \\end{aligned} \\] # 快來看實際用傻瓜算法計算獲得的概率： ppois(40, lambda=60) ## [1] 0.003983 # 快來看用中心極限定理模擬正態分佈獲得的概率： pnorm((40.5-60)/sqrt(60)) ## [1] 0.005911 图 8.4: Probability of 2 identically and independently observed results of less or equal to 40 events happen in total when the expectation of each observation is 30 又一次，正態分佈的面積比泊松分佈的小柱子面積要大一些。 8.7 兩個連續隨機變量 假定 \\(X_1, X_2\\) 是兩個連續隨機變量： \\[E(X_1)=\\mu_1, Var(X_1)=\\sigma_1^2 \\\\ E(X_2)=\\mu_2, Var(X_2)=\\sigma_2^2 \\\\ Corr(X_1, X_2)=\\rho \\Rightarrow Cov(X_1, X_2)=\\rho\\sigma_1\\sigma_2=\\sigma_{12}\\] 利用矩陣的標記法，可以將 \\(X_1, X_2\\) 標記爲 \\(\\textbf{X}=(X_1, X_2)^T\\), 即： \\[\\textbf{X}=\\left( \\begin{array}{c} X_1\\\\ X_2\\\\ \\end{array} \\right)\\] 上面的所有內容都可以標記爲： \\[E(\\textbf{X})=\\mathbf{\\mu}=\\left( \\begin{array}{c} \\mu_1\\\\ \\mu_2\\\\ \\end{array} \\right)\\\\ Covariance \\;matrix: \\\\ Var(\\textbf{X})=\\mathbf{\\Sigma}=\\left( \\begin{array}{c} \\sigma_1^2 &amp; \\sigma_{12}\\\\ \\sigma_{12} &amp; \\sigma_1^2\\\\ \\end{array} \\right)\\] 8.8 兩個連續隨機變量 例子： 假如要看收縮期血壓 (\\(SBP\\)) 和舒張期血壓 (\\(DBP\\)) 之間的關係： 下列爲已知條件： \\(SBP\\) 的均值爲 \\(130\\)， 標準差爲 \\(15\\); \\(DBP\\) 的均值爲 \\(90\\), 標準差爲 \\(10\\); \\(SBP\\) 和 \\(DBP\\) 之間的相關係數爲 \\(0.75\\)。 那麼， 我們可以把這些信息用下面的方法來標記： \\[E(\\textbf{X})=\\mathbf{\\mu}=\\left( \\begin{array}{c} 130\\\\ 90\\\\ \\end{array} \\right)\\\\ Var(\\textbf{X})=\\mathbf{\\Sigma}=\\left( \\begin{array}{c} 225 &amp; 112.5\\\\ 112.5 &amp; 225\\\\ \\end{array} \\right)\\] 8.9 條件分佈和邊緣分佈的概念 如果 \\(\\textbf{X}=(X_1, X_2)^T\\) 的兩個變量都服從正態分佈； 那麼這兩個變量的邊緣分佈 (marginal distribution) 也服從正態分佈: \\[X_1\\sim N(\\mu_1,\\sigma_1^2), X_2\\sim N(\\mu, \\sigma_2^2)\\] 同樣的，\\(X_1\\) 的給出 \\(X_2\\) 的條件分佈 (condition distribution) 也服從正態分佈： \\[E(X_1|X_2)=\\mu_1+\\frac{\\rho\\sigma_1}{\\sigma_2}(X_2-\\mu_2) \\\\ Var(X_1|X_2)=\\sigma_1^2(1-\\rho^2)\\] 反之亦然。 8.10 條件分佈和邊緣分佈的例子 上面的概念過於抽象，用血壓的例子： 收縮期血壓和舒張期血壓各自服從正態分佈。那麼可以用上面的概念來寫出已知舒張期血壓時，收縮期血壓的分佈。 條件期望: \\[E(SBP|DBP)=130+\\frac{0.75\\times15}{10}(DBP-90)\\] 實際如果來了一個病人，他說他只記得自己測的舒張期血壓是95： 他的收縮期血壓的期望值就可以用上面的式子計算： \\[E(SBP|DBP=95)=136\\] 條件方差爲： \\[Var(SBP|DBP)=15^2(1-0.75^2)=98.4\\approx9.92^2&lt;15^2\\] 所以當我們知道了這個人的一部分信息以後，推測他的另一個相關連的變量變得更加準確(方差變小)了。 8.10.1 例題 有 (閒) 人記錄了 \\(1494\\) 名兒童在 \\(2, 4, 6\\) 歲時的腿長度。已知在記錄的這三個年齡時的平均腿長度分別爲 \\(85 cm, 103cm, 114cm\\)。協方差矩陣如下: \\[\\left( \\begin{array}{c} 22.2 &amp; 11.8 &amp; 13.7\\\\ 11.8 &amp; 26.3 &amp; 21.5\\\\ 13.7 &amp; 21.5 &amp; 29.0 \\end{array} \\right)\\] 假定，這三個年齡記錄的這些兒童的腿長度數據（聯合分佈, joint distribution）服從三個變量正態分佈。 求 \\(2\\) 歲時這些兒童的腿長度的邊緣分佈 (marginal distribution) 解 \\[X_{age=2} \\sim N(85, \\sigma_{age=2}^2=22.2)\\] 求他們 \\(6\\) 歲時腿長度的 \\(2\\) 歲時的條件分佈。(Find the distribution of leg length age 6 conditional on leg length at age 2.) 解 \\(6\\) 歲時和 \\(2\\) 歲時腿長的相關係數 (correlation, \\(\\rho_{6,2}\\)) 爲： \\[ \\begin{aligned} \\rho_{6,2} &amp;= \\frac{Cov_{6,2}}{\\sqrt{Var(length_6)}\\sqrt{Var(length_2)}}\\\\ &amp;= \\frac{13.7}{\\sqrt{22.2}\\sqrt{29}}=0.54 \\end{aligned} \\] 條件分佈套用上面提到的公式： \\[ \\begin{aligned} E(length_6 | length_2) &amp;= \\mu_6+\\frac{\\rho_{6,2}\\sigma_6}{\\sigma_2}(length_2-\\mu_2) \\\\ &amp;= 114+\\frac{0.54\\times\\sqrt{29.0}}{\\sqrt{22.2}}(length_2-85)\\\\ Var(length_6 | length_2) &amp;= \\sigma_6^2(1-\\rho_{6,2}^2) \\\\ &amp;= 29.0\\times(1-0.54^2) =20.5 \\end{aligned} \\] "],
["section-9.html", "第 9 章 統計推斷的概念 9.1 人羣與樣本 (population and sample) 9.2 樣本和統計量 (sample and statistic) 9.3 估計 Estimation 9.4 信賴區間 confidence intervals", " 第 9 章 統計推斷的概念 9.1 人羣與樣本 (population and sample) 討論樣本時，需考慮下面幾個問題： 樣本是否具有代表性？ 人羣被準確定義了嗎？ 我們感興趣的“人羣”是否可以是無限大 (多) 的？ 我們研究的樣本，是僅僅用來觀察，亦或是計劃對之進行某種干預呢？ 我們從所有可能的人羣中抽樣了嗎？ 9.2 樣本和統計量 (sample and statistic) 通常我們在進行實驗或觀察時只是獲得了樣本的數據。而希望從樣本數據去推斷 (inference) 總體 (或人羣) 的一些特徵。我們也許只是想用樣本的平均值來估計整體人羣的某個特徵的平均值。不管是何種估計和推斷，都是基於對樣本數據的計算，從樣本中獲得想要推斷總體的統計量 (statistics)。我們用已知樣本去推斷未知總體的過程就叫做估計 (estimate)。這個想要被推斷的總體或人羣的值，被叫做參數 (parameter)，常常使用希臘字母來標記。用來估計總體或人羣的，從樣本數據計算得來的統計量，叫做估計量 (estimator)。 所有的統計量，都有樣本分佈 (sampling distributions，意爲重複無限次取樣後獲得的無限次統計量的分佈)。推斷的過程歸納如下： 從總體或人羣中抽樣 (樣本量 \\(n\\)) 計算這個樣本的合適統計量，從而用於估計它在整體或人羣中的值。 我們還需要決定計算獲得的統計量的樣本分佈 (假定會抽樣無數次) 。 一旦可以精確地確認樣本分佈，我們就可以定量地計算出使用步驟2中獲得的統計量估計總體或人羣的參數時的準確度。 9.3 估計 Estimation 從樣本的均值，推斷總體或人羣的均值是一種估計。我們的目的是，從已知樣本中計算一個儘可能接近那個未知的總體或人羣參數的值。一個估計量有兩個與生俱來的性質 (properties)：1) 偏倚 (bias); 2) 精確度 (precision)。這兩個性質都可以從樣本分佈和估計量獲得。 偏倚： 偏倚簡單說就是樣本分佈的均值，也就是我們從樣本中計算獲得的估計量，和我們想要拿它來估計的總體或人羣的參數之間的差距。(The bias is the difference between the mean of the sampling distribution – the expected or average value of the estimator – and the population parameter being estimated.) 一個小的偏倚，確保了我們從樣本中計算獲得的估計值 (假設我們抽樣無數次，計算無數個樣本估計值) 均勻地分佈在總體或人羣參數的左右兩邊。偏倚本身並不是太大的問題，但是假如樣本量增加，偏倚依然存在 (估計量不一致, inconsistent) ，那常常意味着是抽樣過程出現了問題。例如：用簡單隨機抽樣法獲得的樣本均值，就是總體或人羣均值的無偏估計 (unbiased estimator)。如果抽樣時由於某些主觀客觀的原因導致較小的樣本很少被抽樣 (抽樣過程出了問題，脫離了簡單隨機抽樣原則) ，那麼此時得到的樣本均值就會是一個過高的估計值 (upward biased estimator)。 精確度：估計值的精確度可以通過樣本分佈的方差或標準差來評價 (簡單說是樣本分佈的方差越低，波動越小，精確度越高) 。樣本分佈的標準差被定義爲估計值的標準誤。假如估計量是樣本均值，那麼樣本分佈的標準差 (估計量的標準誤) 和樣本數據之間有如下的關係： \\[true\\; stantdard\\; error\\;of\\;the\\;mean = \\frac{true\\;standard\\;deviation}{\\sqrt{sample\\;size}}\\] 在一些簡單的情況下，通常估計值的選用不言自明 (例如均值，或者百分比) 。但是在複雜的情況下，我們可能可以有多個不同類型的估計量可以選擇，他們也常常各有利弊，需要我們做出取捨。 9.4 信賴區間 confidence intervals 從樣本中計算估計量獲得的一個估計值，只是一個點估計 (point estimate)。對比之下，信賴區間就是一個對這個點估計的精確度的體現。信賴區間越窄，說明我們對於總體或人羣的參數的可能取值的範圍估計越精確。 信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平 (\\(95\\%\\)) 。 常用的這個概率值就是 \\(95\\%, 90\\%, 99\\%\\)。 當從樣本數據計算獲得的估計量的信賴區間很寬，說明了這個收集來的數據提供了很少的參數信息，導致估計變得很不精確。 看到這裏的都是好漢一條啊！ 我不知道你暈了麼有，反正我是已經暈了。。。 "],
["-estimation-and-precision.html", "第 10 章 估計和精確度 Estimation and Precision 10.1 估計量和他們的樣本分佈 10.2 估計量的特質 10.3 總體方差的估計，自由度 10.4 樣本方差的樣本分佈", " 第 10 章 估計和精確度 Estimation and Precision 10.1 估計量和他們的樣本分佈 例子： 最大呼氣量 (Forced Expoiratory Volume in one second, FEV1) 用於測量一個人的肺功能，它的測量值是連續的。我們從前來門診的人中隨機抽取 \\(n\\) 人作爲樣本，用這個樣本的 FEV1 平均值來估計這個診所的患者的平均肺功能。 模型假設： 在這個例子中，我們的假設有如下：每個隨機抽取的 FEV1 測量值都是從同一個總體 (人羣) 中抽取，每一個觀察值 \\(Y_i\\) 都互相獨立互不影響。我們用縮寫 iid 表示這些隨機抽取的樣本是服從獨立同分佈 (independent and identically distributed)。另外，總體的分佈也假定爲正態分佈，且總體均值爲 \\(\\mu\\)，總體方差爲 \\(\\sigma^2\\)。那麼這個模型可以簡單的被寫成： \\[Y_i \\stackrel{i.i.d}{\\sim} N(\\mu, \\sigma^2), i=1,2,\\dots,n\\] 總體均值 \\(\\mu\\) 的估計量： 顯然算術平均值: \\(\\bar{Y}=\\frac{1}{n}\\sum_{i=1}^ny_i\\) 是我們用於估計總體均值的估計量。 估計量的樣本分佈： \\[\\bar{Y}\\stackrel{i.i.d}{\\sim}N(\\mu, \\frac{\\sigma^2}{n})\\] 證明 $$ \\[\\begin{aligned} E(\\bar{Y}) &amp;= E(\\frac{1}{n}\\sum Y_i) \\\\ &amp;= \\frac{1}{n}E(\\sum Y_i) \\\\ &amp;= \\frac{1}{n}\\sum E(Y_i) \\\\ &amp;= \\frac{1}{n}n\\mu = \\mu \\\\ Var(\\bar{Y}) &amp;= Var(\\frac{1}{n}\\sum Y_i) \\\\ \\because Y_i \\;are &amp;\\; independent \\\\ &amp;= \\frac{1}{n^2}\\sum Var(Y_i) \\\\ &amp;= \\frac{1}{n^2} n Var(Y_i) \\\\ &amp;= \\frac{\\sigma^2}{n} \\end{aligned}\\] $$ 證明當 \\(Z=\\frac{\\bar{Y}-\\mu}{\\sqrt{Var(\\bar{Y})}}\\) 時， \\(Z\\sim N(0,1)\\): 由式子可知， \\(Z\\) 只是由一組服從正態分佈的數據 \\(\\bar{Y}\\) 線性轉換 (linear transformation) 而來，所以 \\(Z\\) 本身也服從正態分佈 \\[ \\begin{aligned} E(Z) &amp;= \\frac{1}{\\sqrt{Var(\\bar{Y})}}E[\\bar{Y}-\\mu] \\\\ &amp;= \\frac{1}{\\sqrt{Var(\\bar{Y})}}[\\mu-\\mu] = 0 \\\\ Var(Z) &amp;= \\frac{1}{Var(\\bar{Y})}Var[\\bar{Y}-\\mu] \\\\ &amp;= \\frac{1}{Var(\\bar{Y})}Var(\\bar{Y}) =1 \\\\ \\therefore Z \\;&amp;\\sim N(0,1) \\end{aligned} \\] 均值 \\(\\mu\\) 的信賴區間： 上節說道， 信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平(\\(95\\%\\)) 。 常用的這個概率值就是 \\(95\\%, 90\\%, 99\\%\\)。 假定我們用 \\(95\\%\\) 作爲信賴區間的水平。那麼下面我們嘗試推導一下信賴區間的計算公式。從長遠來說 (也就是假設我們從總體中抽樣無數次，每次都進行信賴區間的計算，也獲得無數個信賴區間) ，這些信賴區間中有 \\(95\\%\\) 是包含了總體的真實均值 (但是卻是未知) 的，而且這些信賴區間由於是從一個服從正態分佈的數據而來，它們也服從正態分佈 (對真實均值左右對稱) 。所以我們有理由相信，可以找到一個數值 \\(c\\)： \\[Prob(\\bar{Y} &gt; \\mu+c) = 0.025 \\\\ Prob(\\bar{Y} &lt; \\mu-c) = 0.025\\] 因此，我們可以定義 \\(95\\%\\) 信賴區間的上限和下限分別是： \\[L=\\bar{Y}-c \\Rightarrow Prob(L&gt;\\mu)=0.025 \\\\ U=\\bar{Y}+c \\Rightarrow Prob(U&lt;\\mu)=0.025\\] 接下來就是推倒 (故意的) \\(c\\) 的過程啦： \\[ \\begin{aligned} Prob(\\bar{Y}&gt;\\mu+c)=Prob(\\bar{Y}-\\mu&gt;c) \\;&amp;= 0.025 \\\\ \\Rightarrow Prob(\\frac{\\bar{Y}-\\mu}{\\sqrt{Var(\\bar{Y})}} &gt; \\frac{c}{\\sqrt{Var(\\bar{Y})}}) \\;&amp;= 0.025 \\\\ \\Rightarrow Prob(Z&gt;\\frac{c}{\\sqrt{Var(\\bar{Y})}}) \\;&amp;= 0.025 \\\\ we\\;have\\;proved\\; Z\\sim N(0,1) \\\\ we\\;also\\;know\\; Prob(Z&gt;1.96) \\;&amp;= 0.025 \\\\ so\\;let\\; \\frac{c}{\\sqrt{Var(\\bar{Y})}} =1.96 \\\\ \\Rightarrow c=1.96\\sqrt{Var(\\bar{Y})} \\\\ the\\;95\\%\\;confidence\\;interval \\;of\\; &amp;the\\;population\\;mean\\;is\\\\ \\mu = \\bar{Y}\\pm1.96\\sqrt{Var(\\bar{Y})}=\\bar{Y}\\pm &amp; 1.96\\frac{\\sigma}{\\sqrt{n}} \\end{aligned} \\] 其中，\\(\\sqrt{Var(\\bar{Y})}\\) 就是我們熟知的估計量 \\(\\bar{Y}\\) 的標準誤。 10.2 估計量的特質 考慮以下的問題： 什麼因素決定了一個估計量 (estimator) 的好壞，是否實用？ 如果有其他的可選擇估計量，該如何取捨呢？ 當情況複雜的時候，我們該如何尋找合適的估計量？ 10.2.1 偏倚 假設 \\(T\\) 是我們估計總體參數 \\(\\theta\\) 的一個估計量。一般來說我們希望估計量的樣本分佈可以在 “正確的位置” 左右均勻分佈。換句話說我們希望： \\[E(T)=\\theta\\] 如果實現了這個條件，我們說這樣的估計量是無偏的 (unbiased)。然而，天下哪有這等好事，我們叫真實值和估計量之間的差距爲偏倚： \\[bias(T) = E(T)-\\theta\\] 其實偏倚完全等於零並不是最重要，許多常見的估計量都是有偏倚的。重要的是，這個偏倚會隨着樣本量的增加而逐漸趨近於零。所以我們就可以認爲這樣的估計量是漸進無偏的 (asymptotically unbiased)： \\[T\\;is\\;an\\;\\textbf{unbiased}\\;estimator\\;for\\;\\theta\\;if\\;\\\\E(T)=\\theta\\\\ T\\;is\\;an\\;\\textbf{asymptotically unbiased}\\;estimator\\;for\\;\\theta\\;if\\;\\\\lim_{n\\rightarrow\\infty}E(T)=\\theta\\] 10.2.2 估計量的效能 Efficiency 通常，我們希望一個估計量 (estimator) 的偏倚要小，同時，它的樣本分佈也希望能儘可能的不要波動太大。換句話說，我們還希望估計量的方差越小越好。 如果說，兩個估計量有相同的偏倚，均可以選擇來推斷總體，我們說，其中樣本分佈的方差小的那個 (波動幅度小) 的那個估計量是相對更好的。因爲樣本分佈方差越小，說明可以更加精確的估計總體參數。這兩個估計量的方差之比：\\(Var(S)/Var(T)\\) 被叫做這兩個估計量的相對效能 (relative efficiency)。所以我們用估計量去推斷總體時，需要選用效能最高，精確度最好的估計量 (the minimum variance unbiased estimator/an efficient estimator)。 10.2.3 均值和中位數的相對效能 在一個服從 \\(N(\\mu,\\sigma^2)\\) 正態分佈的數據中，中位數和均值是一樣的，也都同時等於總體均值參數 \\(\\mu\\)。而且，樣本均數 \\(\\bar{Y}\\) 和樣本中位數 \\(\\dot{Y}\\) 都是對總體均值的無偏估計量。那麼應該選用中位數還是平均值呢？ 之前證明過當 \\(Y_i \\sim N(\\mu,\\sigma^2)\\) 時， \\(Var(\\bar{Y}=\\sigma^2/n)\\)。然而，當 \\(n\\) 較大的時候，可以證明的是： \\[Var(\\dot{Y})=\\frac{\\pi}{2}\\frac{\\sigma^2}{n}\\approx1.571\\frac{\\sigma^2}{n}\\] 因此，這兩個估計量的相對效能就是： \\[\\frac{Var(\\dot{Y})}{Var(\\bar{Y})}\\approx1.571\\] 所以總體是正態分佈時，平均值就是較中位數更適合用來估計總體的估計量。 10.2.4 均方差 mean square error (MSE) 兩個估計量的偏倚不同時，可以比較他們和總體參數之間的差距，這被叫做均方差, Mean Square Error (MSE)。 \\[MSE(T)=E[(T-\\theta)^2]\\] 這裏用一個數學技巧，將式子中的估計量和總體參數之間的差，分成兩個部分：一是估計量本身的方差 (\\(T-E(T)\\))，一是估計量的偏倚 (\\(E(T)-\\theta\\))。 \\[ \\begin{aligned} MSE(T) &amp;= E[(T-\\theta)^2] \\\\ &amp;= E\\{[T-E(T)+E(T)-\\theta]^2\\} \\\\ &amp;= E\\{[T-E(T)]^2+[E(T)-\\theta]^2 \\\\ &amp; \\;\\;\\;\\;\\; \\;\\;+2[T-E(T)][E(T)-\\theta]\\} \\\\ &amp;= E\\{[T-E(T)]^2\\}+E\\{[E(T)-\\theta]^2\\} + 0\\\\ &amp;= Var(T) + [bias(T)^2] \\end{aligned} \\] 10.3 總體方差的估計，自由度 如果 \\(Y_i \\sim (\\mu, \\sigma^2)\\)，並不需要默認或者假定它服從正態分佈或者任何分佈。那麼它的方差我們會用： \\[V_{\\mu}=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\mu)^2\\] 證明 \\(V_{\\mu}\\) 是 \\(\\sigma^2\\) 的無偏估計： \\[ \\begin{aligned} V_{\\mu} &amp;= \\frac{1}{n}\\sum_{i=1}^n(Y_i-\\mu)^2 \\\\ we\\;need\\;to\\;prove &amp;E(V_{\\mu}) = \\sigma^2 \\\\ \\Rightarrow E(V_{\\mu}) &amp;= \\frac{1}{n}\\sum_{i=1}^nE(Y_i-\\mu)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^nVar(Y_i) \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n\\sigma^2 \\\\ &amp;= \\sigma^2 \\end{aligned} \\] 然而通常情況下，我們並不知道總體的均值 \\(\\mu\\)。因此，只好用樣本的均值 \\(\\bar{Y}\\) 來估計 \\(\\mu\\)。所以上面的方程就變成了： \\[V_{\\mu}=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\bar{Y})^2\\] 你如果仔細觀察認真思考，就會發現，上面這個式子是有問題的。這個大問題就在於，\\(Y_i-\\bar{Y}\\) 中我們忽略掉了樣本均值 \\(\\bar{Y}\\) 和總體均值 \\(\\mu\\) 之間的差 (\\(\\bar{Y}-\\mu\\))。因此上面的計算式來估計總體方差時，很顯然是會低估平均平方差，從而低估了總體方差。 這裏需要引入自由度 (degree of freedom) 在參數估計中的概念。 字面上可以理解爲：自由度是估計過程中使用了多少互相獨立的信息。所以在上面第一個公式中：\\(V_{\\mu}=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\mu)^2\\)。所有的 \\(n\\) 個觀察值互相獨立，不僅如此，他們還對總體均值獨立。然而在第二個我們用 \\(\\bar{Y}\\) 取代了 \\(\\mu\\) 的公式中，樣本均數則與觀察值不互相獨立。因爲樣本均數必然總是落在觀察值的中間。然而總體均數並不一定就會落在觀察值中間。總體均數，和觀察值之間是自由，獨立的。因此，當我們觀察到 \\(n-1\\) 個觀察值時，剩下的最後一個觀察值，決定了樣本均值的大小。所以說，樣本均值的自由度，是 \\(n-1\\)。 所以，加入了自由度的討論，我們可以相信，用樣本估計總體的方差時，使用下面的公式將會是總體方差的無偏估計： \\[V_{n-1}=\\frac{1}{n-1}\\sum_{i=1}^n(Y_i-\\bar{Y})=\\frac{n}{n-1}V_n\\] 證明 利用上面也用到過的證明方法 – 把樣本和總體均值之間的差分成兩部分： \\[ \\begin{aligned} V_{\\mu} &amp;= \\frac{1}{n}\\sum_{i=1}^n(Y_i-\\mu)^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n[(Y_i-\\bar{Y})+(\\bar{Y}-\\mu)]^2 \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n[(Y_i-\\bar{Y})^2+(\\bar{Y}-\\mu)^2\\\\ &amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+2(Y_i-\\bar{Y})(\\bar{Y}-\\mu)]\\\\ &amp;=\\frac{1}{n}\\sum_{i=1}^n(Y_i-\\bar{Y})^2+\\frac{1}{n}\\sum_{i=1}^n(\\bar{Y}-\\mu)^2\\\\ &amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;+\\frac{2}{n}(\\bar{Y}-\\mu)\\sum_{i=1}^n(Y_i-\\bar{Y}) \\\\ &amp;= V_n+(\\bar{Y}-\\mu)^2 \\\\ &amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;(\\text{note that}\\;\\sum_{i=1}^n(Y_i-\\bar{Y})=0) \\\\ \\Rightarrow V_n &amp;= V_{\\mu}-(\\bar{Y}-\\mu)^2 \\\\ \\therefore E(V_n)&amp;= E(V_{\\mu}) - E[(\\bar{Y}-\\mu)^2] \\\\ &amp;= Var(Y)-Var(\\bar{Y}) \\\\ &amp;= \\sigma^2-\\frac{\\sigma^2}{n} \\\\ &amp;= \\sigma^2(\\frac{n-1}{n}) \\end{aligned} \\] 因此，我們看見 \\(V_n\\) 正如上面討論的那樣，是低估了總體方差的。雖然當 \\(n\\rightarrow\\infty\\) 時無限接近 \\(\\sigma^2\\) 但是依然是低估了的。所以，我們可以對之進行修正： \\[ \\begin{aligned} E[\\frac{n}{n-1}V_n] &amp;= \\frac{n}{n-1}E[V_n] =\\sigma^2 \\\\ \\Rightarrow E[V_{n-1}] &amp;= \\sigma^2 \\end{aligned} \\] 10.4 樣本方差的樣本分佈 \\(S^2\\) 常用來標記樣本方差，取代上面我們用到的 \\(V_{n-1}\\)： \\[S^2=\\frac{1}{n-1}\\sum_{i=1}^n(Y_i-\\bar{Y})^2\\] 而且上面也證明了，\\(E(S^2)=\\sigma^2\\) 是總體方差的無偏估計。然而，要注意的是，樣本標準差 \\(\\sqrt{S^2}\\) 卻不是總體標準差 \\(\\sigma\\) 的無偏估計(因爲並不是線性變換，而是開了根號) 。 證明樣本標準差 \\(S\\) 不是總體標準差 \\(\\sigma\\) 的無偏估計 \\[ \\begin{aligned} Var(S) &amp;=E(S^2)-[E(S)]^2 \\\\ \\Rightarrow [E(S)]^2 &amp;=E(S^2)-Var(S) \\\\ \\because E(S^2) &amp;=\\sigma^2 \\\\ \\therefore [E(S)]^2 &amp;=\\sigma^2-Var(S) \\\\ E(S) &amp;=\\sqrt{\\sigma^2-Var(S)} \\\\ \\end{aligned}\\] 可見樣本標準差是低估了總體標準差的。 另外可以被證明的是： \\[\\frac{n-1}{\\sigma^2}S^2\\sim \\mathcal{X}_{n-1}^2\\\\ Var(S^2)=\\frac{2\\sigma^4}{n-1}\\] \\(\\mathcal{X}^2_m\\)： 自由度爲 \\(m\\) 的卡方分佈 (Section 11)。是在圖形上向右歪曲的分佈。當自由度增加時，會越來越接近正態分佈。 "],
["chi-square-distribution.html", "第 11 章 卡方分佈 Chi-square distribution 11.1 卡方分佈的期望和方差的證明 11.2 卡方分佈的期望 11.3 卡方分佈的方差 11.4 把上面的推導擴展", " 第 11 章 卡方分佈 Chi-square distribution 11.1 卡方分佈的期望和方差的證明 當 \\(X\\sim N(0,1)\\) 時， \\(X^2\\sim \\mathcal{X}_1^2\\) 如果 \\(X_1, \\dots, X_n\\stackrel{i.i.d}{\\sim} N(0,1)\\)， 那麼 \\(\\sum_{i=1}^nX_i^2\\sim\\mathcal{X}_n^2\\) 其中： \\(\\mathcal{X}_n^2\\) 表示自由度爲 \\(n\\) 的卡方分佈。 且 \\(X_m^2+X_n^2=\\mathcal{X}_{m+n}^2\\) 11.2 卡方分佈的期望 \\[E(X_1^2)=Var(X)+[E(X)]^2=1+0=1\\] \\[\\Rightarrow E(X_n^2)=n\\] 11.3 卡方分佈的方差 \\[ \\begin{aligned} Var(X_1^2) &amp;= E(X_1^{2^2}) - E(X_1^2)^2 \\\\ &amp;= E(X_1^4)-1 \\end{aligned} \\] 11.3.1 下面來求 \\(E(X_1^4)\\) \\[ \\begin{aligned} \\because E(X_1) &amp;= \\int_{-\\infty}^{+\\infty} xf(x)dx \\\\ \\therefore E(X_1^4) &amp;= \\int_{-\\infty}^{+\\infty} x^4f(x)dx \\end{aligned}\\] 已知： \\(f(x)=\\frac{1}{\\sqrt{2\\pi}}e^{(-\\frac{x^2}{2})}\\) 代入上式： \\[ \\begin{aligned} E(X_1^4) &amp;= \\int_{-\\infty}^{+\\infty} x^4f(x)dx \\\\ &amp;= \\int_{-\\infty}^{+\\infty} x^4\\frac{1}{\\sqrt{2\\pi}}e^{(-\\frac{x^2}{2})}dx\\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}x^4e^{(-\\frac{x^2}{2})}dx\\\\ &amp;=\\frac{-1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}x^3(-x)e^{(-\\frac{x^2}{2})}dx \\end{aligned} \\] 令 \\(u=x^3, v=e^{(-\\frac{x^2}{2})},t=-\\frac{x^2}{2}\\) 可以推導： \\[ \\begin{aligned} \\frac{dv}{dx} &amp;= \\frac{dv}{dt}\\frac{dt}{dx} \\\\ &amp;= e^t(-\\frac{1}{2}\\times2x) \\\\ &amp;= (-x)e^{(-\\frac{x^2}{2})} \\\\ \\Rightarrow dv &amp;= (-x)e^{(-\\frac{x^2}{2})}dx \\end{aligned} \\] 再代入上面的式子： \\[ \\begin{aligned} E(X_1^4) &amp;= \\frac{-1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}u\\:dv \\\\ integrate\\; &amp;by\\; parts:\\\\ E(X_1^4) &amp;= \\frac{-1}{\\sqrt{2\\pi}}\\{[u\\:v] \\rvert_{-\\infty}^{+\\infty}-\\int_{-\\infty}^{+\\infty}v\\:du\\} \\\\ &amp;= \\frac{-1}{\\sqrt{2\\pi}}\\{[x^3e^{(-\\frac{x^2}{2})}]\\rvert_{-\\infty}^{+\\infty} -\\int_{-\\infty}^{+\\infty}v\\:du\\} \\\\ &amp;=\\frac{-1}{\\sqrt{2\\pi}}\\{0-0-\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2}{2})}dx^3\\} \\\\ &amp;=\\frac{-1}{\\sqrt{2\\pi}}[-3\\int_{-\\infty}^{+\\infty}x^2e^{(-\\frac{x^2}{2})}dx] \\\\ &amp;=\\frac{-3}{\\sqrt{2\\pi}}[\\int_{-\\infty}^{+\\infty}x(-x)e^{(-\\frac{x^2}{2})}dx] \\\\ \\end{aligned} \\] 再來一次分部積分： 令 \\(a=x,b=e^{(-\\frac{x^2}{2})},d\\:b = (-x)e^{(-\\frac{x^2}{2})}dx\\) \\[ \\begin{aligned} E(X_1^4) &amp;= \\frac{-3}{\\sqrt{2\\pi}}\\{[a\\:b] \\rvert_{-\\infty}^{+\\infty} - \\int_{-\\infty}^{+\\infty}b\\:da\\} \\\\ &amp;=\\frac{-3}{\\sqrt{2\\pi}}\\{[xe^{(-\\frac{x^2}{2})}]\\rvert_{-\\infty}^{+\\infty} -\\int_{-\\infty}^{+\\infty}b\\:da\\} \\\\ &amp;=\\frac{-3}{\\sqrt{2\\pi}}\\{0-0-\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2}{2})}dx\\} \\\\ &amp;=\\frac{-3}{\\sqrt{2\\pi}}[-\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2}{2})}dx] \\\\ &amp;=\\frac{3}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2}{2})}dx \\end{aligned} \\] 下面令 \\(I=\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2}{2})}dx\\\\ \\Rightarrow I^2=\\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2+y^2}{2})}dxdy\\) 接下來需要用到 座標轉換的知識，將 \\(x,y\\) 表示的笛卡爾座標，轉換爲用角度 \\(\\theta\\) 和半徑 \\(r\\) 表示的形式。之後的證明可以在油管上看到，但是我還是繼續證明下去。 直角座標系 (cartesian coordinators) 和 極座標系 (polar coordinators) 之間轉換的關係如下： \\[ \\begin{aligned} x&amp;=r\\:cos\\theta\\\\ y&amp;=r\\:sin\\theta\\\\ r^2&amp;=x^2+y^2\\\\ \\end{aligned} \\] 座標轉換以後可以繼續求 \\(E(X_1^4)\\)。 在那之前我們先求 \\(I^2\\)。 注意轉換座標系統以後，\\(\\theta\\in[0,2\\pi], r\\in[0,+\\infty]\\) \\[ \\begin{aligned} I^2 &amp;= \\int_{-\\infty}^{+\\infty}\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2+y^2}{2})}dxdy \\\\ &amp;= \\int_{0}^{+\\infty}\\int_{0}^{2\\pi}e^{(-\\frac{r^2}{2})}rd\\theta dr \\\\ \\end{aligned} \\] 由於先從中間的 \\(\\int_{0}^{2\\pi}e^{(-\\frac{r^2}{2})}rd\\theta\\) 開始積分，\\(\\theta\\) 以外都可以視爲常數，那麼這個 \\([0,2\\pi]\\) 上的積分就的等於 \\(2\\pi e^{(-\\frac{r^2}{2})}r\\)。 因此上面的式子又變爲： \\[ \\begin{aligned} I^2 &amp;= 2\\pi\\int_{0}^{+\\infty}e^{(-\\frac{r^2}{2})}r\\:dr \\\\ \\because \\frac{d(e^{\\frac{-r^2}{2}})}{dr} &amp;= -e^{(-\\frac{r^2}{2})}r \\\\ \\therefore I^2 &amp;= 2\\pi(-e^{\\frac{-r^2}{2}})\\rvert_0^{+\\infty} \\\\ &amp;= 0-(2\\pi\\times(-1)) \\\\ &amp;= 2\\pi\\\\ \\Rightarrow I &amp;= \\sqrt{2\\pi} \\end{aligned} \\] 所以， \\[ \\begin{aligned} E(X_1^4) &amp;= \\frac{3}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}e^{(-\\frac{x^2}{2})}dx \\\\ &amp;= \\frac{3}{\\sqrt{2\\pi}}\\times I \\\\ &amp;= 3 \\\\ \\Rightarrow Var(X_1^2) &amp;= E(X_1^4) - 1 \\\\ &amp;= 3-1 =2 \\end{aligned} \\] 11.4 把上面的推導擴展 \\[ \\text{Suppose } \\mathcal{X}^2_1, \\cdots \\mathcal{X}^2_k \\stackrel{i.i.d}{\\sim} \\mathcal{X}^2_1 \\\\ \\Rightarrow \\sum_{i=1}^k \\mathcal{X}^2_i \\sim \\mathcal{X}^2_k \\\\ \\Rightarrow \\text{E}(\\sum_{i=1}^n\\mathcal{X}^2_i)=\\sum_{i-1}^n\\text{E}(\\mathcal{X}^2_i)=n\\times1=n\\\\ \\text{Var}(\\sum_{i=1}^n\\mathcal{X}^2_i)=\\sum_{i=1}^n\\text{Var}(\\mathcal{X}^2_i) = n\\times2=2n \\] 結論：\\(X_1, \\dots, X_n\\stackrel{i.i.d}{\\sim} N(0,1)\\) 時，\\(\\sum_{i=1}^nX_i^2\\sim\\mathcal{X}_n^2\\) 服從卡方分佈，其期望 \\(E(X_n^2)=n\\)，方差 \\(Var(X_n^2)=2n\\)。 根據中心極限定理(Section 8) \\[n\\rightarrow \\infty, X_n^2\\sim N(n, 2n)\\] "],
["likelihood-definition.html", "第 12 章 似然 Likelihood 12.1 概率 vs. 推斷 Probability vs. Inference 12.2 似然和極大似然估計 Likelihood and maximum likelihood estimators 12.3 似然方程的一般化定義 12.4 對數似然方程 log-likelihood 12.5 極大似然估計 (maximum likelihood estimator, MLE) 的性質： 12.6 率的似然估計 Likelihood for a rate 12.7 有 \\(n\\) 個獨立觀察時的似然方程和對數似然方程", " 第 12 章 似然 Likelihood 12.1 概率 vs. 推斷 Probability vs. Inference 在概率論的環境下，我們常常被告知的前提是：某某事件發生的概率是多少。例如： 一枚硬幣正面朝上的概率是 \\(0.5\\; Prob(coin\\;landing\\;heads)=0.5\\)。然後在這個前提下，我們又繼續去計算複雜的事件發生的概率(例如，10次投擲硬幣以後4次正面朝上的概率是多少？) 。 \\[ \\binom{10}{4}\\times(0.5^4)\\times(0.5^{10-4}) = 0.205 \\] dbinom(4, 10, 0.5) ## [1] 0.2051 # or you can calculate by hand: factorial(10)*(0.5^10)/(factorial(4)*(factorial(6))) ## [1] 0.2051 在統計推斷的理論中，我們考慮實際的情況，這樣的實際情況就是，我們通過觀察獲得數據，然而我們並不知道某事件發生的概率到底是多少(神如果存在話，只有神知道) 。故這個 \\(Prob(coin\\;landing\\;heads)\\) 的概率大小對於“人類”來說是未知的。我們可能觀察到投擲了10次硬幣，其中有4次是正面朝上的。那麼我們從這一次觀察實驗中，需要計算的是能夠符合觀察結果的“最佳”概率估計 (best estimate)。在這種情況下，似然法 (likelihood) 就是我們進行參數估計的最佳手段。 12.2 似然和極大似然估計 Likelihood and maximum likelihood estimators 此處用二項分佈的例子來理解似然法的概念：假設我們觀察到10個對象中有4個患中二病，我們假定這個患病的概率爲 \\(\\pi\\)。於是我們就有了下面的模型： 模型： 我們假定患病與否是一個服從二項分佈的隨機變量，\\(X\\sim Bin(10,\\pi)\\)。同時也默認每個人之間是否患病是相互獨立的。 數據： 觀察到的數據是，10人中有4人患病。於是 \\(x=4\\)。 現在按照觀察到的數據，參數 \\(\\pi\\) 變成了未知數： \\[Prob(X=4|\\pi)=\\binom{10}{4}\\pi^4(1-\\pi)^{10-4}\\] 此時我們會很自然的考慮，當 \\(\\pi\\) 是未知數的時候，它取值爲多大的時候才能讓這個事件(即：10人中4人患病) 發生的概率最大？ 所以我們可以將不同的數值代入 \\(\\pi\\) 來計算該事件在不同概率的情況下發生的可能性到底是多少： Table 12.1: The probability of observing \\(X=4\\) \\(\\pi\\) 事件 \\(X=4\\) 發生的概率 0.0 0.000 0.2 0.088 0.4 0.251 0.5 0.205 0.6 0.111 0.8 0.006 1.0 0.000 很顯然，如果 \\(\\pi=0.4\\) 時，我們觀察到的事件發生的概率要比 \\(\\pi\\) 取其它值時更大。於是小總結一下目前爲止的步驟如下： 觀察到實驗數據(10人中4個患病) ； 假定這數據服從二項分佈的概率模型，計算不同(\\(\\pi\\) 的取值不同的) 情況下，該事件按照假定模型發生的概率； 通過比較，我們選擇了能夠讓觀察事件發生概率最高的參數取值 (\\(\\pi=0.4\\))。 至此，我們可以知道，似然方程，是一個關於未知參數 \\(\\pi\\) 的函數，我們目前位置做的就是找到這個函數的最大值 (maximised)，和使之成爲最大值時的 \\(\\pi\\) ： \\[L(\\pi|X=4)=\\binom{10}{4}\\pi^4(1-\\pi)^{10-4}\\] 我們可以畫出這個似然方程的形狀， \\(\\pi\\in[0,1]\\) x &lt;- seq(0,1,by=0.001) y &lt;- (factorial(10)/(factorial(4)*(factorial(6))))*(x^4)*((1-x)^6) plot(x, y, type = &quot;l&quot;, ylim = c(0,0.3), ylab = &quot;L(\\U03C0)&quot;, xlab = &quot;\\U03C0&quot;) #title(&quot;Figure 1. Binomial Likelihood&quot;) abline(h=0.251, lty=2) abline(v=0.4, lty=2) 图 12.1: Binomial Likelihood 從圖形上我們也能確認，\\(\\pi=0.4\\) 時能夠讓這個似然方程取得最大值。 12.3 似然方程的一般化定義 對於一個概率模型，如果其參數爲 \\(\\theta\\)，那麼在給定觀察數據 \\(\\underline{x}\\) 時，該參數的似然方程被定義爲： \\(L(\\theta|\\underline{x})=P(\\underline{x}|\\theta)\\) 注意： \\(P(\\underline{x}|\\theta)\\) 可以是概率(離散分佈) 方程，也可以是概率密度(連續型變量) 方程。對於此方程，\\(\\theta\\) 是給定的，然後再計算某些事件發生的概率。 \\(L(\\theta|\\underline{x})\\) 是一個關於參數 \\(\\theta\\) 的方程，此時，\\(\\underline{x}\\) 是固定不變的(觀察值) 。我們希望通過這個方程求出能夠使觀察到的事件發生概率最大的參數值。 似然方程不是一個概率密度方程。 另一個例子： 有一組觀察數據是離散型隨機變量 \\(X\\)，它符合概率方程 \\(f(x|\\theta)\\)。下表羅列了當 \\(\\theta\\) 分別取值 \\(1,2,3\\) 時的概率方程的值，試求每個觀察值 \\(X = 0,1,2,3,4\\) 的最大似然參數估計： Exercise 12.3 \\(x\\) \\(f(x|1)\\) \\(f(x|2)\\) \\(f(x|3)\\) 0 1/3 1/4 0 1 1/3 1/4 0 2 0 1/4 1/6 3 1/6 1/4 1/2 4 1/6 0 1/3 Exercise 12.3 answer \\(x\\) \\(f(x|1)\\) \\(f(x|2)\\) \\(f(x|3)\\) \\(\\theta\\) 0 1/3 1/4 0 1 1 1/3 1/4 0 1 2 0 1/4 1/6 2 3 1/6 1/4 1/2 3 4 1/6 0 1/3 3 12.4 對數似然方程 log-likelihood 似然方程的最大值，可通過求 \\(L(\\theta|data)\\) 的最大值獲得，也可以通過求該方程的對數方程 \\(\\ell(\\theta|data)\\) 的最大值獲得。傳統上，我們估計最大方程的最大值的時候，會給參數戴一頂“帽子”(因爲這是觀察獲得的數據告訴我們的參數) ： \\(\\hat{\\theta}\\)。並且我們發現對數似然方程比一般的似然方程更加容易微分，因此求似然方程的最大值就變成了求對數似然方程的最大值： \\[\\frac{d\\ell}{d\\theta}=\\ell^\\prime(\\theta)=0\\\\ AND\\\\ \\frac{d^2\\ell}{d\\theta^2}&lt;0\\] 要注意的是，微分不一定總是能幫助我們求得似然方程的最大值。如果說參數本身的定義域是有界限的話，微分就行不通了： x &lt;- seq(0,3,by=0.001) y &lt;- (x-1)^2-5 plot(x, y, type = &quot;l&quot;, ylim = c(-5,0-1), ylab = &quot;L(\\U03B8)&quot;, xlab = &quot;\\U03B8&quot;) #title(&quot;Figure 2. Likelihood function with \\n a limited domain&quot;) abline(v=3, lty=2) 图 12.2: Likelihood function with a limited domain 證明：當 \\(L(\\theta|data)\\) 取最大值時，該方程的對數方程 \\(\\ell(\\theta|data)\\) 也是最大值： 如果似然方程是連續可導，只有一個最大值，且可以二次求導，假設 \\(\\hat{\\theta}\\) 使該方程取最大值，那麼： \\[\\frac{dL}{d\\theta}=0, \\frac{d^2L}{d\\theta^2}&lt;0 \\Rightarrow \\theta=\\hat{\\theta}\\] 令 \\(\\ell=\\text{log}L\\) 那麼 \\(\\frac{d\\ell}{dL}=\\ell^\\prime=\\frac{1}{L}\\)： \\[\\frac{d\\ell}{d\\theta}=\\frac{d\\ell}{dL}\\cdot\\frac{dL}{d\\theta}=\\frac{1}{L}\\cdot\\frac{dL}{d\\theta}\\] 當 \\(\\ell(\\theta|data)\\) 取最大值時： \\[\\frac{d\\ell}{d\\theta}=0\\Leftrightarrow\\frac{1}{L}\\cdot\\frac{dL}{d\\theta}=0\\\\ \\because \\frac{1}{L}\\neq0 \\\\ \\therefore \\frac{dL}{d\\theta}=0\\\\ \\Leftrightarrow \\theta=\\hat{\\theta}\\] \\[ \\begin{aligned} \\frac{d^2\\ell}{d\\theta^2} &amp;= \\frac{d}{d\\theta}(\\frac{d\\ell}{dL}\\cdot\\frac{dL}{d\\theta})\\\\ &amp;= \\frac{d\\ell}{dL}\\cdot\\frac{d^2L}{d\\theta^2} + \\frac{dL}{d\\theta}\\cdot\\frac{d}{d\\theta}(\\frac{d\\ell}{dL}) \\end{aligned} \\] 當 \\(\\theta=\\hat{\\theta}\\) 時，\\(\\frac{dL}{d\\theta}=0\\) 且 \\(\\frac{d^2L}{d\\theta^2}&lt;0 \\Rightarrow \\frac{d^2\\ell}{d\\theta^2}&lt;0\\) 所以，求獲得 \\(\\ell(\\theta|data)\\) 最大值的 \\(\\theta\\) 即可令 \\(L(\\theta|data)\\) 獲得最大值。 12.5 極大似然估計 (maximum likelihood estimator, MLE) 的性質： 漸進無偏 Asymptotically unbiased: \\(n\\rightarrow \\infty \\Rightarrow E(\\hat{\\Theta}) \\rightarrow \\theta\\) 漸進最高效能 Asymptotically efficient: \\(n\\rightarrow \\infty \\Rightarrow Var(\\hat{\\Theta})\\) 是所有參數中方差最小的估計 漸進正態分佈 Asymptotically normal: \\(n\\rightarrow \\infty \\Rightarrow \\hat{\\Theta} \\sim N(\\theta, Var(\\hat{\\Theta}))\\) 變形後依然保持不變 Transformation invariant: \\(\\hat{\\Theta}\\) 是 \\(\\theta\\) 的MLE時 \\(\\Rightarrow g(\\hat{\\Theta})\\) 是 \\(g(\\theta)\\) 的 MLE 信息足夠充分 Sufficient： \\(\\hat{\\Theta}\\) 包含了觀察數據中所有的能夠用於估計參數的信息 始終不變 consistent: \\(n\\rightarrow\\infty\\Rightarrow\\hat{\\Theta}\\rightarrow\\theta\\) 或者可以寫成：\\(\\varepsilon&gt;0, lim_{n\\rightarrow\\infty}P(|\\hat{\\Theta}-\\theta|&gt;\\varepsilon)=0\\) 12.6 率的似然估計 Likelihood for a rate 如果在一項研究中，參與者有各自不同的追蹤隨訪時間(長度) ，那麼我們應該把事件(疾病) 的發病率用率的形式(多少事件每單位人年, e.g. per person year of observation) 。如果這個發病率的參數用 \\(\\lambda\\) 來表示，所有參與對象的隨訪時間之和爲 \\(p\\) 人年。那麼這段時間內的期望事件(疾病發病) 次數爲：\\(\\mu=\\lambda p\\)。假設事件(疾病發病) 發生是相互獨立的，可以使用泊松分佈來模擬期望事件(疾病發病) 次數 \\(D\\)： \\[D\\sim Poi(\\mu)\\] 假設我們觀察到了 \\(D=d\\) 個事件，我們獲得這個觀察值的概率應該用以下的模型： \\[Prob(D=d)=e^{-\\mu}\\frac{\\mu^d}{d!}=e^{-\\lambda p}\\frac{\\lambda^dp^d}{d!}\\] 因此，\\(\\lambda\\) 的似然方程是： \\[L(\\lambda|observed \\;data)=e^{-\\lambda p}\\frac{\\lambda^dp^d}{d!}\\] 所以，\\(\\lambda\\) 的對數似然方程是： \\[ \\begin{aligned} \\ell(\\lambda|observed\\;data) &amp;= \\text{log}(e^{-\\lambda p}\\frac{\\lambda^dp^d}{d!}) \\\\ &amp;= -\\lambda p+d\\:\\text{log}(\\lambda)+d\\:\\text{log}(p)-\\text{log}(d!) \\\\ \\end{aligned} \\] 解 \\(\\ell^\\prime(\\lambda|data)=0\\): \\[ \\begin{aligned} \\ell^\\prime(\\lambda|data) &amp;= -p+\\frac{d}{\\lambda}=0\\\\ \\Rightarrow \\hat{\\lambda} &amp;= \\frac{d}{p} \\\\ \\end{aligned} \\] 注意： 在對數似然方程中，不包含參數的部分，對與似然方程的形狀不產生任何影響，我們在微分對數似然方程的時候，這部分也都自動消失。所以不包含參數的部分，與我們如何獲得極大似然估計是無關的。因此，我們常常在寫對數似然方程的時候就把其中沒有參數的部分直接忽略了。例如上面泊松分佈的似然方程中，\\(d\\:\\text{log}(p)-\\text{log}(d!)\\) 不包含參數 \\(\\lambda\\) 可以直接不寫出來。 12.7 有 \\(n\\) 個獨立觀察時的似然方程和對數似然方程 當有多個獨立觀察時，總體的似然方程等於各個觀察值的似然方程之乘積。如果 \\(X_1,\\dots,X_n\\stackrel{i.i.d}{\\sim}f(\\cdot|\\theta)\\) \\[L(\\theta|x_1,\\cdots,x_n)=f(x_1,\\cdots,x_n|\\theta)=\\prod_{i=1}^nf(x_i|\\theta)\\\\ \\Rightarrow \\ell(\\theta|x_1,\\cdots,x_n)=\\sum_{i=1}^n\\text{log}(f(x_i|\\theta))\\] "],
["llr.html", "第 13 章 對數似然比 Log-likelihood ratio 13.1 正態分佈數據的極大似然和對數似然比 13.2 \\(n\\) 個獨立正態分佈樣本的對數似然比 13.3 \\(n\\) 個獨立正態分佈樣本的對數似然比的分佈 13.4 似然比信賴區間 13.5 練習題", " 第 13 章 對數似然比 Log-likelihood ratio 對數似然比的想法來自於將對數似然方程圖形的 \\(y\\) 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的對數似然比 (log-likelihood ratio) 來獲得： \\[llr(\\theta)=\\ell(\\theta|data)-\\ell(\\hat{\\theta}|data)\\] 由於 \\(\\ell(\\theta)\\) 的最大值在 \\(\\hat{\\theta}\\) 時， 所以，\\(llr(\\theta)\\) 就是個當 \\(\\theta=\\hat{\\theta}\\) 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 \\(LR(\\theta)=\\frac{L(\\theta)}{L(\\hat{\\theta})}\\) 取對數而已。 之前我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子： \\[L(\\pi|X=4)=\\binom{10}{4}\\pi^4(1-\\pi)^{10-4}\\\\ \\Rightarrow \\ell(\\pi)=\\text{log}[\\pi^4(1-\\pi)^{10-4}]\\\\ \\Rightarrow llr(\\pi)=\\ell(\\pi)-\\ell(\\hat{\\pi})=\\text{log}\\frac{\\pi^4(1-\\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\\] 其實由上也可以看出 \\(llr(\\theta)\\) 只是將對應的似然方程的 \\(y\\) 軸重新調節了一下而已。形狀是沒有改變的： par(mfrow=c(1,2)) x &lt;- seq(0,1,by=0.001) y &lt;- (x^4)*((1-x)^6)/(0.4^4*0.6^6) z &lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6) plot(x, y, type = &quot;l&quot;, ylim = c(0,1.1),yaxt=&quot;n&quot;, frame.plot = FALSE, ylab = &quot;LR(\\U03C0)&quot;, xlab = &quot;\\U03C0&quot;) axis(2, at=seq(0,1, 0.2), las=2) title(main = &quot;Binomial likelihood ratio&quot;) abline(h=1.0, lty=2) segments(x0=0.4, y0=0, x1=0.4, y1=1, lty = 2) plot(x, z, type = &quot;l&quot;, ylim = c(-10, 1), yaxt=&quot;n&quot;, frame.plot = FALSE, ylab = &quot;llr(\\U03C0)&quot;, xlab = &quot;\\U03C0&quot; ) axis(2, at=seq(-10, 0, 2), las=2) title(main = &quot;Binomial log-likelihood ratio&quot;) abline(h=0, lty=2) segments(x0=0.4, y0=-10, x1=0.4, y1=0, lty = 2) 图 13.1: Binomial likelihood ratio and log-likelihood ratio 13.1 正態分佈數據的極大似然和對數似然比 假設單個樣本 \\(y\\) 是來自一組服從正態分佈數據的觀察值：\\(Y\\sim N(\\mu, \\tau^2)\\) 那麼有： \\[ \\begin{aligned} f(y|\\mu) &amp;= \\frac{1}{\\sqrt{2\\pi\\tau^2}}e^{(-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2)} \\\\ \\Rightarrow L(\\mu|y) &amp;=\\frac{1}{\\sqrt{2\\pi\\tau^2}}e^{(-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2)} \\\\ \\Rightarrow \\ell(\\mu)&amp;=\\text{log}(\\frac{1}{\\sqrt{2\\pi\\tau^2}})-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2\\\\ omitting&amp;\\;terms\\;not\\;in\\;\\mu \\\\ &amp;= -\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2 \\\\ \\Rightarrow \\ell^\\prime(\\mu) &amp;= 2\\cdot[-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})\\cdot\\frac{-1}{\\tau}] \\\\ &amp;=\\frac{y-\\mu}{\\tau^2} \\\\ let \\; \\ell^\\prime(\\mu) &amp;= 0 \\\\ \\Rightarrow \\frac{y-\\mu}{\\tau^2} &amp;= 0 \\Rightarrow \\hat{\\mu} = y\\\\ \\because \\ell^{\\prime\\prime}(\\mu) &amp;= \\frac{-1}{\\tau^2} &lt; 0 \\\\ \\therefore \\hat{\\mu} &amp;= y \\Rightarrow \\ell(\\hat{\\mu}=y)_{max}=0 \\\\ llr(\\mu)&amp;=\\ell(\\mu)-\\ell(\\hat{\\mu})=\\ell(\\mu)\\\\ &amp;=-\\frac{1}{2}(\\frac{y-\\mu}{\\tau})^2 \\end{aligned} \\] 13.2 \\(n\\) 個獨立正態分佈樣本的對數似然比 假設一組觀察值來自正態分佈 \\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu,\\sigma^2)\\)，先假設 \\(\\sigma^2\\) 已知。將觀察數據 \\(x_1,\\cdots, x_n\\) 標記爲 \\(\\underline{x}\\)。 那麼： \\[ \\begin{aligned} L(\\mu|\\underline{x}) &amp;=\\prod_{i=1}^nf(x_i|\\mu)\\\\ \\Rightarrow \\ell(\\mu|\\underline{x}) &amp;=\\sum_{i=1}^n\\text{log}f(x_i|\\mu)\\\\ &amp;=\\sum_{i=1}^n[-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2]\\\\ &amp;=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\\\ &amp;=-\\frac{1}{2\\sigma^2}[\\sum_{i=1}^n(x_i-\\bar{x})^2+\\sum_{i=1}^n(\\bar{x}-\\mu)^2]\\\\ omitting&amp;\\;terms\\;not\\;in\\;\\mu \\\\ &amp;=-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(\\bar{x}-\\mu)^2\\\\ &amp;=-\\frac{n}{2\\sigma^2}(\\bar{x}-\\mu)^2 \\\\ &amp;=-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2\\\\ \\because \\ell(\\hat{\\mu}) &amp;= 0 \\\\ \\therefore llr(\\mu) &amp;= \\ell(\\mu)-\\ell(\\hat{\\mu}) = \\ell(\\mu) \\end{aligned} \\] 13.3 \\(n\\) 個獨立正態分佈樣本的對數似然比的分佈 假設我們用 \\(\\mu_0\\) 表示總體均數這一參數的值。要注意的是，每當樣本被重新取樣，似然，對數似然方程，對數似然比都隨着觀察值而變 (即有自己的分佈)。 考慮一個服從正態分佈的單樣本 \\(Y\\): \\(Y\\sim N(\\mu_0,\\tau^2)\\)。那麼它的對數似然比： \\[llr(\\mu_0|Y)=\\ell(\\mu_0)-\\ell(\\hat{\\mu})=-\\frac{1}{2}(\\frac{Y-\\mu_0}{\\tau})^2\\] 根據卡方分佈 (Section 11) 的定義： \\[\\because \\frac{Y-\\mu_0}{\\tau}\\sim N(0,1)\\\\ \\Rightarrow (\\frac{Y-\\mu_0}{\\tau})^2 \\sim \\mathcal{X}_1^2\\\\ \\therefore -2llr(\\mu_0|Y) \\sim \\mathcal{X}_1^2\\] 所以，如果有一組服從正態分佈的觀察值：\\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu_0,\\sigma^2)\\)，且 \\(\\sigma^2\\) 已知的話： \\[-2llr(\\mu_0|\\bar{X})\\sim \\mathcal{X}_1^2\\] 根據中心極限定理 (Section 8)，可以將上面的結論一般化： Theorem 13.1 如果 \\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}f(x|\\theta)\\)。 那麼當重複多次從參數爲 \\(\\theta_0\\) 的總體中取樣時，那麼統計量 \\(-2llr(\\theta_0)\\) 會漸進於自由度爲 \\(1\\) 的卡方分佈： \\[-2llr(\\theta_0)=-2\\{\\ell(\\theta_0)-\\ell(\\hat{\\theta})\\}\\xrightarrow[n\\rightarrow\\infty]{}\\;\\sim \\mathcal{X}_1^2\\] 13.4 似然比信賴區間 如果樣本量 \\(n\\) 足夠大 (通常應該大於 \\(30\\))，根據上面的定理： \\[-2llr(\\theta_0)=-2\\{\\ell(\\theta_0)-\\ell(\\hat{\\theta})\\}\\sim \\mathcal{X}_1^2\\] 所以： \\[Prob(-2llr(\\theta_0)\\leqslant \\mathcal{X}_{1,0.95}^2=3.84) = 0.95\\\\ \\Rightarrow Prob(llr(\\theta_0)\\geqslant-3.84/2=-1.92) = 0.95\\] 故似然比的 \\(95\\%\\) 信賴區間就是能夠滿足 \\(llr(\\theta)=-1.92\\) 的兩個 \\(\\theta\\) 值。 13.4.1 以二項分佈數據爲例 繼續用本文開頭的例子： \\[llr(\\pi)=\\ell(\\pi)-\\ell(\\hat{\\pi})=\\text{log}\\frac{\\pi^4(1-\\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}\\] 如果令 \\(llr(\\pi)=-1.92\\) 在代數上可能較難獲得答案。然而從圖形上，如果我們在 \\(y=-1.92\\) 畫一條橫線，和該似然比方程曲線相交的兩個點就是我們想要求的信賴區間的上限和下限： x &lt;- seq(0,1,by=0.001) z &lt;- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6) plot(x, z, type = &quot;l&quot;, ylim = c(-10, 1), yaxt=&quot;n&quot;, frame.plot = FALSE, ylab = &quot;llr(\\U03C0)&quot;, xlab = &quot;\\U03C0&quot; ) axis(2, at=seq(-10, 0, 2), las=2) abline(h=0, lty=2) abline(h=-1.92, lty=2) segments(x0=0.15, y0=-12, x1=0.15, y1=-1.92, lty = 2) segments(x0=0.7, y0=-12, x1=0.7, y1=-1.92, lty = 2) axis(1, at=c(0.15,0.7)) text(0.9, -1, &quot;-1.92&quot;) arrows(0.8, -1.92, 0.8, 0, lty = 1, length = 0.08) arrows( 0.8, 0, 0.8, -1.92, lty = 1, length = 0.08) 图 13.2: Log-likelihood ratio for binomial example, with 95% confidence intervals shown 從上圖中可以讀出，\\(95\\%\\) 對數似然比信賴區間就是 \\((0.15, 0.7)\\) 13.4.2 以正態分佈數據爲例 本文前半部分證明過， \\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu,\\sigma^2)\\)，先假設 \\(\\sigma^2\\) 已知。將觀察數據 \\(x_1,\\cdots, x_n\\) 標記爲 \\(\\underline{x}\\)。 那麼： \\[llr(\\mu|\\underline{x}) = \\ell(\\mu|\\underline{x})-\\ell(\\hat{\\mu}) = \\ell(\\mu|\\underline{x}) \\\\ =-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2\\] 很顯然，這是一個關於 \\(\\mu\\) 的二次方程，且最大值在 MLE \\(\\hat{\\mu}=\\bar{x}\\) 時取值 \\(0\\)。所以可以通過對數似然比法求出均值的 \\(95\\%\\) 信賴區間公式： \\[-2\\times[-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2]=3.84\\\\ \\Rightarrow L=\\bar{x}-\\sqrt{3.84}\\frac{\\sigma}{\\sqrt{n}} \\\\ U=\\bar{x}+\\sqrt{3.84}\\frac{\\sigma}{\\sqrt{n}} \\\\ note: \\;\\sqrt{3.84}=1.96\\] 注意到這和我們之前求的正態分佈均值的信賴區間公式 (Section 10.1) 完全一致。 13.5 練習題 13.5.1 Q1 假設十個對象中有三人死亡，用二項分佈模型來模擬這個例子，求這個例子中參數 \\(\\pi\\) 的似然方程和圖形 (likelihood) ? 解 \\[\\begin{aligned} L(\\pi|3) &amp;= \\binom{10}{3}\\pi^3(1-\\pi)^{10-3} \\\\ omitting\\;&amp;terms\\;not\\;in\\;\\pi \\\\ \\Rightarrow \\ell(\\pi|3) &amp;= \\text{log}[\\pi^3(1-\\pi)^7] \\\\ &amp;= 3\\text{log}\\pi+7\\text{log}(1-\\pi)\\\\ \\Rightarrow \\ell^\\prime(\\pi|3)&amp;= \\frac{3}{\\pi}-\\frac{7}{1-\\pi} \\\\ let \\; \\ell^\\prime&amp; =0\\\\ &amp;\\frac{3}{\\pi}-\\frac{7}{1-\\pi} = 0 \\\\ &amp;\\frac{3-10\\pi}{\\pi(1-\\pi)} = 0 \\\\ \\Rightarrow MLE &amp;= \\hat\\pi = 0.3 \\end{aligned}\\] 图 13.3: Binomial likelihood function 3 out of 10 subjects 計算似然比，並作圖，注意方程圖形未變，\\(y\\) 軸的變化；取對數似然比，並作圖 LR &lt;- L/max(L) ; head(LR) ## [1] 0.0000000 0.0004192 0.0031234 0.0098111 0.0216286 ## [6] 0.0392577 plot(pi, LR, type = &quot;l&quot;, ylim = c(0, 1),yaxt=&quot;n&quot;, col=&quot;darkblue&quot;, frame.plot = FALSE, ylab = &quot;&quot;, xlab = &quot;\\U03C0&quot;) grid(NA, 5, lwd = 1) axis(2, at=seq(0,1,0.2), las=2) title(main = &quot;Binomial likelihood ratio function\\n 3 out of 10 subjects&quot;) 图 13.4: Binomial likelihood ratio function 3 out of 10 subjects logLR &lt;- log(L/max(L)) plot(pi, logLR, type = &quot;l&quot;, ylim = c(-4, 0),yaxt=&quot;n&quot;, col=&quot;darkblue&quot;, frame.plot = FALSE, ylab = &quot;&quot;, xlab = &quot;\\U03C0&quot;) grid(NA, 5, lwd = 1) axis(2, at=seq(-4,0,1), las=2) #title(main = &quot;Binomial log-likelihood ratio function\\n 3 out of 10 subjects&quot;) abline(h=-1.92, lty=1, col=&quot;red&quot;) axis(4, at=-1.92, las=0) 图 13.5: Binomial log-likelihood ratio function 3 out of 10 subjects 13.5.2 Q2 與上面用同樣的模型，但是觀察人數變爲 \\(100\\) 人 患病人數爲 \\(30\\) 人，試作對數似然比方程之圖形，與上圖對比： 图 13.6: Binomial log-likelihood ratio function 3 out of 10 and 30 out of 100 subjects 可以看出，兩組數據的 MLE 都是一致的， \\(\\hat\\pi=0.3\\)，但是對數似然比方程圖形在 樣本量爲 \\(n=100\\) 時比 \\(n=10\\) 時窄很多，由此產生的似然比信賴區間也就窄很多(精確很多) 。所以對數似然比方程的曲率(二階導數) ，反映了觀察獲得數據提供的對總體參數 \\(\\pi\\) 推斷過程中的信息量。而且當樣本量較大時，對數似然比方程也更加接近左右對稱的二次方程曲線。 13.5.3 Q3 在一個實施了160人年的追蹤調查中，觀察到8個死亡案例。使用泊松分佈模型，繪製對數似然比方程圖形，從圖形上目視推測極大似然比的 \\(95\\%\\) 信賴區間。 解 \\[\\begin{aligned} d = 8, \\;p &amp;= 160\\; person\\cdot year \\\\ \\Rightarrow D\\sim Poi(\\mu &amp;=\\lambda p) \\\\ L(\\lambda|data) &amp;= Prob(D=d=8) \\\\ &amp;= e^{-\\mu}\\frac{\\mu^d}{d!} \\\\ &amp;= e^{-\\lambda p}\\frac{\\lambda^d p^d}{d!} \\\\ omitting&amp;\\;terms\\;not\\;in\\;\\lambda \\\\ &amp;= e^{-\\lambda p}\\lambda^d \\\\ \\Rightarrow \\ell(\\lambda|data)&amp;= \\text{log}(e^{-\\lambda p}\\lambda^d) \\\\ &amp;= d\\cdot \\text{log}(\\lambda)-\\lambda p \\\\ &amp; = 8\\times \\text{log}(\\lambda) - 160\\times\\lambda \\end{aligned}\\] 图 13.7: Poisson log-likelihood ratio function 8 events in 160 person-years lambda LogLR 0.010 -6.4755 0.011 -5.8730 0.012 -5.3369 0.013 -4.8566 0.014 -4.4237 0.015 -4.0318 0.016 -3.6755 0.017 -3.3505 0.018 -3.0532 0.019 -2.7807 0.020 -2.5303 0.021 -2.3000 0.022 -2.0878 0.023 -1.8922 0.024 -1.7118 0.025 -1.5452 0.026 -1.3914 0.027 -1.2495 0.028 -1.1185 0.029 -0.9978 0.030 -0.8866 0.031 -0.7843 0.032 -0.6903 0.033 -0.6041 0.034 -0.5253 0.035 -0.4534 0.036 -0.3880 0.037 -0.3288 0.038 -0.2755 0.039 -0.2277 0.040 -0.1851 0.041 -0.1476 0.042 -0.1148 0.043 -0.0866 0.044 -0.0627 0.045 -0.0429 0.046 -0.0271 0.047 -0.0150 0.048 -0.0066 0.049 -0.0016 0.050 0.0000 0.051 -0.0016 0.052 -0.0062 0.053 -0.0138 0.054 -0.0243 0.055 -0.0375 0.056 -0.0534 0.057 -0.0718 0.058 -0.0926 0.059 -0.1159 0.060 -0.1414 0.061 -0.1692 0.062 -0.1991 0.063 -0.2311 0.064 -0.2651 0.065 -0.3011 0.066 -0.3389 0.067 -0.3786 0.068 -0.4201 0.069 -0.4633 0.070 -0.5082 0.071 -0.5547 0.072 -0.6029 0.073 -0.6525 0.074 -0.7037 0.075 -0.7563 0.076 -0.8103 0.077 -0.8657 0.078 -0.9225 0.079 -0.9806 0.080 -1.0400 0.081 -1.1006 0.082 -1.1624 0.083 -1.2255 0.084 -1.2896 0.085 -1.3550 0.086 -1.4214 0.087 -1.4889 0.088 -1.5575 0.089 -1.6271 0.090 -1.6977 0.091 -1.7693 0.092 -1.8419 0.093 -1.9154 0.094 -1.9898 0.095 -2.0652 0.096 -2.1414 0.097 -2.2185 0.098 -2.2964 0.099 -2.3752 0.100 -2.4548 所以從列表數據結合圖形， 可以找到信賴區間的下限在 0.022~0.023 之間， 上限在 0.093～0.094 之間。 "],
["quadratic-llr.html", "第 14 章 二次方程近似法求對數似然比 approximate log-likelihood ratios 14.1 正態近似法求對數似然 Normal approximation to the log-likelihood 14.2 參數转换 parameter transformations 14.3 練習題", " 第 14 章 二次方程近似法求對數似然比 approximate log-likelihood ratios 爲什麼要用二次方程近似對數似然比方程？ 上節也看到，我們會碰上難以用代數學計算獲得對數似然比信賴區間的情況 (Section 13.4.1: binomial example)。 我們同時知道，對數似然比方程會隨着樣本量增加而越來越漸進於二次方程，且左右對稱。 所以，我們考慮當樣本量足夠大時，用二次方程來近似對數似然比方程從而獲得參數估計的信賴區間。 14.1 正態近似法求對數似然 Normal approximation to the log-likelihood 根據前一節 (Section 13.4.2)，如果樣本均數的分佈符合正態分佈：\\(\\bar{X}\\sim N(\\mu, \\sigma^2/n)\\)。那麼樣本均數的對數似然比爲： \\[llr(\\mu|\\bar{X})=\\ell(\\mu|\\bar{X})=-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2\\] 其中， \\(\\bar{x}\\) 是正態分佈總體均數 \\(\\mu\\) 的極大似然估計 (maximum likelihood estimator, MLE)。如果已知總體的方差參數，那麼 \\(\\sigma/\\sqrt{n}\\) 是 \\(\\bar{x}\\) 的標準誤 (standard error)。 因此，假設 \\(\\theta\\) 是我們想尋找的總體參數。有些人提議可以使用下面的關於 \\(\\theta\\) 的二次方程來做近似： \\[f(\\theta|data)=-\\frac{1}{2}(\\frac{\\theta-M}{S})^2\\] 上述方程具有一個正態二次對數似然 (比) 的形式，而且該方程的極大似然估計(MLE)， \\(M\\) 的標準誤爲 \\(S\\)。如果我們正確地選用 \\(M\\) 和 \\(S\\)，那我們就可以用這樣的方程來近似求真實觀察數據的似然 \\(\\ell(\\theta|data)\\)。 通過近似正態對數似然比，\\(M\\) 應當選用使方程取最大值時，參數 \\(\\theta\\) 的極大似然估計 \\(M=\\hat{\\Theta}\\)。 但是在選用標準誤 \\(S\\) 上必須滿足下列條件： \\(S\\) 是極大似然估計 \\(\\hat{\\Theta}\\) 的標準誤。 被選擇的 \\(S\\) 必須儘可能的使該二次方程形成一個十分接近真實的對數似然比方程。特別是在最大值的部分必須與之無限接近或者一致。所以二者在 MLE 的位置應當有相同的曲率(二階導數) 。 由於，一個方程的曲率是該方程的二階導數(斜線斜率變化的速度) 。所以對數似然比方程在 MLE 取最大值時的曲率(二階導數) 爲： \\[\\left.\\frac{d^2}{d\\theta^2}\\ell(\\theta)\\right\\vert_{\\theta=\\hat{\\theta}}=\\ell^{\\prime\\prime}(\\hat{\\theta})=-\\frac{1}{S^2}\\\\ \\Rightarrow S^2=\\left.-\\frac{1}{\\ell^{\\prime\\prime}(\\theta)}\\right\\vert_{\\theta=\\hat{\\theta}} \\] 在正態分佈的例子下，\\(M=\\bar{x}, S=\\sigma/\\sqrt{n}\\)。對數似然比方程最大值時的曲率(二階導數) 恰好就爲標準誤的平方的負倒數： \\(\\ell^{\\prime\\prime}(\\theta)=-\\frac{1}{SE^2}\\) \\(\\Rightarrow\\) 被叫做 Fisher information。 稍微總結一下： 任意的對數似然比方程 \\(llr(\\theta)\\) 都可以考慮用一個二次方程來近似： \\[f(\\theta|data)=-\\frac{1}{2}(\\frac{\\theta-M}{S})^2\\] 其中 \\(\\begin{aligned} &amp;M=\\hat\\theta\\\\ &amp;S^2=\\left.-\\frac{1}{\\ell^{\\prime\\prime}(\\theta)}\\right\\vert_{\\theta=\\hat{\\theta}}\\\\ &amp;when \\\\ &amp; n\\rightarrow\\infty \\Rightarrow \\begin{cases} S^2\\rightarrow Var(\\hat\\theta) \\\\ S\\rightarrow SE(\\hat\\theta) \\end{cases} \\end{aligned}\\) 14.1.1 近似法估算對數似然比的信賴區間 一旦我們決定了使用正態近似法來模擬對數似然比方程，對數似然比的信賴區間算法就回到了前一節中我們算過的方法，也就是： \\[-2f(\\theta)&lt;\\mathcal{X}_{1,(1-\\alpha)}^2\\] 故信賴區間爲： \\(m\\pm\\sqrt{\\mathcal{X}_{1,(1-\\alpha)}^2}S\\)。求\\(95\\%\\) 水平的信賴區間時，\\(\\mathcal{X}_{1,0.95}^2=3.84\\)，所以就又看到了熟悉的 \\(M\\pm1.96S\\)。 14.1.2 以泊松分佈爲例 一個被追蹤的樣本，經過了 \\(p\\) 人年的觀察，記錄到了 \\(d\\) 個我們要研究的事件： \\[D\\sim Poi(\\mu), where \\mu=\\lambda p\\] Step 1. 找極大似然估計 (MLE)，之前介紹似然方程時推導過的泊松分佈的似然方程 (Section 12.6)： \\[\\begin{aligned} P(D=d|\\lambda) &amp;= \\frac{e^{-\\mu}\\cdot\\mu^d}{d!} \\\\ &amp;=\\frac{e^{-\\lambda p}\\cdot\\lambda^d p^d}{d!} \\\\ omitting&amp;\\;terms\\;not\\;in\\;\\mu \\\\ &amp;\\Rightarrow \\ell(\\lambda) = d\\text{log}\\lambda - \\lambda p \\\\ &amp;\\Rightarrow \\ell^\\prime(\\lambda) = \\frac{d}{\\lambda} -p \\\\ &amp;\\Rightarrow \\hat\\lambda=\\frac{d}{p} = \\textbf{M} \\end{aligned}\\] Step 2. 求似然方程的二階導數，確認 MLE 是使方程獲得最大值的點，然後確定 \\(S^2\\)： \\[\\begin{aligned} &amp; \\ell^\\prime(\\lambda) = \\frac{d}{\\lambda} -p \\\\ &amp; \\Rightarrow \\ell^{\\prime\\prime}(\\lambda) = -\\frac{d}{\\lambda^2}&lt;0 \\Rightarrow \\textbf{MLE is maximum} \\\\ &amp; S^2 = \\left.-\\frac{1}{\\ell^{\\prime\\prime}(\\lambda)}\\right\\vert_{\\lambda=\\hat{\\lambda}=d/p} = -\\frac{1}{-d/\\hat\\lambda^2} = -\\frac{1}{-d/(d/p)^2} \\\\ &amp;\\Rightarrow S^2 = \\frac{d}{p^2} \\\\ \\end{aligned}\\] Step 3. 把前兩部求得的 \\(MLE\\) 和 \\(S^2\\) 代入近似的二次方程： \\[\\begin{aligned} &amp; \\hat\\lambda=\\frac{d}{p}=M,\\; S^2 = \\frac{d}{p^2} \\\\ &amp; using\\;approximate\\;quadratic\\;llr \\\\ &amp; q(\\lambda) = -\\frac{1}{2}(\\frac{\\lambda-M}{S})^2\\\\ &amp;\\Rightarrow q(\\lambda) = -\\frac{1}{2}(\\frac{\\lambda-\\frac{d}{p}}{\\frac{\\sqrt{d}}{p}})^2\\\\ &amp; let \\; q(\\lambda)=-1.92\\\\ &amp;\\Rightarrow -\\frac{1}{2}(\\frac{\\lambda-\\frac{d}{p}}{\\frac{\\sqrt{d}}{p}})^2=-1.92\\\\ &amp;(\\frac{\\lambda-\\frac{d}{p}}{\\frac{\\sqrt{d}}{p}})^2=3.84\\\\ &amp;\\frac{\\lambda-\\frac{d}{p}}{\\frac{\\sqrt{d}}{p}} = \\pm1.96\\\\ &amp;\\Rightarrow 95\\%CI \\;for \\;\\lambda = \\frac{d}{p}\\pm1.96\\frac{\\sqrt{d}}{p} \\end{aligned}\\] 結論就是： 發病(死亡) 率 \\(\\lambda\\) 的 \\(95\\%\\) 信賴區間爲： \\(M\\pm1.96S\\)。所以我們不需要每次都代入對數似然比方程，只要算出 \\(MLE = M\\) 和 \\(S\\) 之後代入這個公式就可以用二次方程近似法算出信賴區間。 14.1.3 以二項分佈爲例 \\[K\\sim Bin(n,\\pi)\\] Step 1. 找極大似然估計 (MLE)： \\[ \\begin{aligned} &amp; Prob(K=k) = \\pi^k(1-\\pi)\\binom{n}{k}\\\\ &amp;\\Rightarrow L(\\pi|k) = \\pi^k(1-\\pi)\\binom{n}{k}\\\\ &amp;omitting\\;terms\\;not\\;in\\;\\pi \\\\ &amp;\\Rightarrow \\ell(\\pi) = k\\:\\text{log}\\pi+(n-k)\\text{log}(1-\\pi) \\\\ &amp;\\ell^\\prime(\\pi) = \\frac{k}{\\pi}-\\frac{n-k}{1-\\pi} \\\\ &amp; let\\;\\ell^\\prime(\\hat\\pi) =0 \\\\ &amp;\\Rightarrow \\frac{k}{\\hat\\pi}-\\frac{n-k}{1-\\hat\\pi}=0\\\\ &amp;\\Rightarrow \\frac{\\hat\\pi}{1-\\hat\\pi}=\\frac{k}{n-k}\\\\ &amp;\\Rightarrow \\frac{\\hat\\pi}{1-\\hat\\pi}=\\frac{k/n}{1-k/n}\\\\ &amp;\\Rightarrow \\hat\\pi=\\frac{k}{n} = p = \\textbf{M} \\end{aligned} \\] Step 2. 將對數似然方程的二次微分 (二階導數)，確認在 MLE 爲極大值，並確認 \\(S^2\\)： \\[ \\begin{aligned} &amp;\\ell^\\prime(\\pi) = \\frac{k}{\\pi}-\\frac{n-k}{1-\\pi} \\\\ &amp;\\ell^{\\prime\\prime}(\\pi)=\\frac{-k}{\\pi^2}-\\frac{n-k}{(1-\\pi)^2} &lt;0 \\\\ &amp;\\therefore at\\;\\textbf{MLE}\\;\\ell(\\pi)\\;has\\;maximum \\\\ S^2&amp;=\\left.-\\frac{1}{\\ell^{\\prime\\prime}(\\pi)}\\right\\vert_{\\pi=\\hat\\pi=k/n=p}\\\\ &amp;=\\frac{1}{\\frac{k}{\\hat\\pi^2}+\\frac{n-k}{(1-\\hat\\pi)^2}}\\\\ &amp;=\\frac{\\hat\\pi^2(1-\\hat\\pi)^2}{k(1-\\hat\\pi)^2+(n-k)\\hat\\pi^2}\\\\ &amp;=\\frac{P^2(1-P)^2}{np(1-p)^2+(n-np)p^2}\\\\ &amp;=\\frac{p(1-p)}{n(1-p)+np}\\\\ &amp;=\\frac{p(1-p)}{n}\\\\ &amp;\\Rightarrow S=\\sqrt{\\frac{p(1-p)}{n}} \\end{aligned} \\] Step 3. 將求得的 MLE 和 \\(S^2\\) 代入近似信賴區間： \\[ 95\\% CI \\;for \\; \\pi:\\\\ M\\pm1.96S=p\\pm1.96\\sqrt{\\frac{p(1-p)}{n}}\\\\ \\] 14.2 參數转换 parameter transformations 如果將參數 \\(\\theta\\) 通過某種數學方程轉化成 \\(g(\\theta)\\)，那麼我們可以認爲，轉化後的方程的 MLE 爲 \\(g(\\hat\\theta)\\)，其中 \\(\\hat\\theta\\) 是參數 \\(\\theta\\) 的 MLE。 類似地，如果 \\(\\theta_1 \\sim \\theta_2\\) 是參數 \\(\\theta\\) 的似然比信賴區間，那麼 \\(g(\\theta_1)\\sim g(\\theta_2)\\) 就是 \\(g(\\theta)\\) 的似然比信賴區間。 以下爲轉換參數以後獲取信賴區間的步驟： 將參數通過某些數學方程(通常是取對數) 轉化，使新的對數似然比方程更加接近二次方程的對稱圖形。 Transform parameter so that \\(llr\\) is closer to a quadratic shape. 用本節學到的二次方程近似法，求得轉化後的參數的似然比信賴區間。 Use our quadratic approximation on the transformed parameter to calculate our likelihood ratio confidence intervals. 將第2步計算獲得的似然比信賴區間再通過轉化參數時的逆函數轉換回去，以獲得原參數的似然比信賴區間。 Transform the confidence intervals back, or to any scale we wish – they remain valid. 14.2.1 以泊松分佈爲例 當我們用泊松分佈模擬事件在某段時間內發生率 \\(\\lambda\\) 時，注意到這個事件發生率必須滿足 \\(\\lambda&gt;0\\)。當事件發生次數較低時，會讓似然方程的圖形被擠壓在低值附近。如果嘗試用對數轉換 \\(\\lambda \\rightarrow \\text{log}(\\lambda)\\) 此時 \\(\\text{log}(\\lambda)\\) 就不再被限制與 \\(&gt;0\\)。下面我們嘗試尋找對數轉換過後的 \\(M\\) 和 \\(S\\)。 令 \\(\\beta=\\text{log}(\\lambda), \\Rightarrow e^\\beta=\\lambda\\) 從本文上半部分中我們已知 \\(\\hat\\lambda=\\frac{d}{p}\\)。 對數轉換以後的 \\(M\\) 是什麼? 根據定義，\\(MLE(\\beta)=MLE[\\text{log}(\\lambda)]=\\text{log}(\\hat\\lambda)\\) \\(\\Rightarrow M=\\hat\\beta=\\text{log}(\\frac{d}{p})\\) 對數轉換以後的 \\(S\\) 是什麼? 泊松分佈的對數似然方程是：\\(\\ell(\\lambda|d)=d \\text{log}(\\lambda) - \\lambda p\\) 用 \\(\\beta\\) 替換掉 \\(\\lambda\\) \\(\\begin{aligned} &amp; \\ell(\\beta|d)=d \\beta - pe^\\beta\\\\ &amp; \\Rightarrow \\ell^\\prime(\\beta)=d-pe^\\beta \\Rightarrow \\ell^{\\prime\\prime}(\\beta)=-pe^\\beta \\\\ &amp; S^2 = \\left.-\\frac{1}{\\ell^{\\prime\\prime}(\\beta)}\\right\\vert_{\\beta=\\hat{\\beta}} = \\left.\\frac{1}{pe^\\beta}\\right\\vert_{\\beta=\\hat{\\beta}} = \\frac{1}{pe^{\\text{log}(d/p)}}\\\\ &amp;\\Rightarrow S^2=\\frac{1}{d} \\therefore S=\\frac{1}{\\sqrt{d}} \\end{aligned}\\) 轉換後的近似二次方程： \\(\\begin{aligned} &amp; q(\\beta) = -\\frac{1}{2}(\\frac{\\beta-M}{S})^2 = -\\frac{1}{2}(\\frac{\\beta-\\text{log}(\\frac{d}{p})}{\\frac{1}{\\sqrt{d}}})^2 \\end{aligned}\\) \\(\\beta\\) 的 \\(95\\%\\) 信賴區間 \\(=\\text{log}(\\frac{d}{p})\\pm1.96\\frac{1}{\\sqrt{d}}\\) \\(\\lambda\\) 的 \\(95\\%\\) 信賴區間 \\(=exp(\\text{log}(\\frac{d}{p})\\pm1.96\\frac{1}{\\sqrt{d}})\\) 14.2.2 以二項分佈爲例 在研究對象 \\(n\\) 人中觀察到 \\(k\\) 個人患有某種中二疾病。 令 \\(\\beta=\\text{log}(\\pi) \\Rightarrow \\pi=e^\\beta\\) 從上文的推倒也已知 \\(\\hat\\pi=\\frac{k}{n}=p\\) \\(\\begin{aligned} &amp;\\Rightarrow \\ell(\\beta)=k\\text{log}\\pi+(n-k)\\text{log}(1-\\pi)=k\\beta+(n-k)\\text{log}(1-e^\\beta) \\\\ &amp;\\Rightarrow \\ell^{\\prime}(\\beta)=k-\\frac{(n-k)(e^\\beta)}{1-e^\\beta} \\\\ &amp;\\Rightarrow \\ell^{\\prime\\prime}(\\beta)=-(n-k)\\frac{e^\\beta(1-e^\\beta)+e^{2\\beta}}{(1-e^\\beta)^2} \\\\ &amp; \\ell^{\\prime\\prime}(\\beta)= -(n-k)\\frac{e^\\beta}{(1-e^\\beta)^2}\\\\ &amp;\\Rightarrow S^2 = \\left.-\\frac{1}{\\ell^{\\prime\\prime}(\\beta)}\\right\\vert_{\\beta=\\hat{\\beta}} = \\frac{(1-e^{\\hat\\beta})^2}{(n-k)e^{\\hat\\beta}} \\\\ &amp;\\because \\hat\\beta=\\text{log}(\\hat\\pi) \\\\ &amp;\\therefore e^{\\hat\\beta} = \\frac{k}{n}\\\\ &amp;\\Rightarrow S^2=\\frac{(1-\\frac{k}{n})^2}{(n-k)\\frac{k}{n}}=\\frac{n-k}{nk}=\\frac{1}{k}-\\frac{1}{n}\\\\ &amp; \\Rightarrow S=\\sqrt{\\frac{1}{k}-\\frac{1}{n}}\\\\ \\end{aligned}\\) 14.3 練習題 14.3.1 Q1 在\\(n=100\\)人中觀察到有\\(k=40\\)人患病，假設每個人只有患病，不患病兩個狀態，用二項分佈來模擬這個數據，\\(\\pi\\) 爲患病的概率。下面是 \\(\\pi \\in [0.2,0.6]\\) 區間的對數似然比方程曲線。 pi &lt;- seq(0.2, 0.6, by=0.01) L &lt;- (pi^40)*((1-pi)^60) Lmax &lt;- rep(max(L), 41) LR &lt;- L/Lmax logLR &lt;- log(LR) plot(pi, logLR, type = &quot;l&quot;, ylim = c(-11, 0),yaxt=&quot;n&quot;, frame.plot = FALSE, ylab = &quot;logLR(\\U03C0)&quot;, xlab = &quot;\\U03C0&quot;) grid(NA, 5, lwd = 2) # add some horizontal grid on the background axis(2, at=seq(-12,0,2), las=2) 图 14.1: Binomial log-likelihood ratio between 0.2-0.6 #title(main = &quot;Figure 1. Binomial log-likelihood ratio&quot;) 用一個二次方程來模擬上面的對數似然比曲線：\\(f(\\pi)=-\\frac{(\\pi-M)^2}{2S^2}\\)，其中 \\(M=\\hat\\pi=\\frac{k}{n}=0.4\\)，\\(S^2=\\frac{p(1-p)}{n}=0.0024\\) par(mai = c(1.2, 0.5, 1, 0.7)) quad &lt;- -(pi-0.4)^2/(2*0.0024) plot(pi, quad, type = &quot;l&quot;, ylim = c(-4, 0),yaxt=&quot;n&quot;, col=&quot;red&quot;, frame.plot = FALSE, ylab = &quot;&quot;, xlab = &quot;\\U03C0&quot;) lines(pi, logLR, col=&quot;black&quot;) grid(NA, 4, lwd = 1) # add some horizontal grid on the background axis(2, at=seq(-4,0,1), las=2) #title(main = &quot;Figure 2. Quadratic approximation\\n of binomial log-likelihood ratio \\n 40 out of 100 subjects&quot;) abline(h=-1.92, lty=1, col=&quot;red&quot;) axis(4, at=-1.92, las=2) legend(x=0.27, y= -5.5 ,xpd = TRUE, legend=c(&quot;logLR&quot;,&quot;Quadratic&quot;), bty = &quot;n&quot;, col=c(&quot;black&quot;,&quot;red&quot;), lty=c(1,1), horiz = TRUE) #the legend is below the graph 图 14.2: Quadratic approximation of binomial log-likelihood ratio 40 out of 100 subjects 14.3.2 Q2 依舊使用二項分佈數據來模擬，觀察不同的事件數量和樣本量對近似計算的影響。 類比上面的問題，用同樣的 \\(\\hat\\pi=0.4\\)，但是 \\(n=10, k=4\\) 時的圖形： par(mai = c(1.2, 0.5, 1, 0.7)) pi &lt;- seq(0.0, 0.85, by=0.01) L &lt;- (pi^4)*((1-pi)^6) logLR &lt;- log(L/max(L)) quad &lt;- -(pi-0.4)^2/(2*0.4*0.6/10) plot(pi, quad, type = &quot;l&quot;, ylim = c(-5, 0),yaxt=&quot;n&quot;, col=&quot;red&quot;, frame.plot = FALSE, ylab = &quot;&quot;, xlab = &quot;\\U03C0&quot;) lines(pi, logLR, col=&quot;black&quot;) grid(NA, 4, lwd = 1) axis(2, at=seq(-5,0,1), las=2) #title(main = &quot;Figure 3. Quadratic approximation\\n of binomial log-likelihood ratio\\n 4 out of 10 subjects&quot;) abline(h=-1.92, lty=1, col=&quot;red&quot;) axis(4, at=-1.92, las=2) legend(x=0.17, y= -6.5 ,xpd = TRUE, legend=c(&quot;logLR&quot;,&quot;Quadratic&quot;), bty = &quot;n&quot;, col=c(&quot;black&quot;,&quot;red&quot;), lty=c(1,1), horiz = TRUE) #the legend is below the graph 图 14.3: Quadratic approximation of binomial log-likelihood ratio 4 out of 10 subjects \\(\\hat\\pi=0.4, n=1000, k=400\\) 图 14.4: Quadratic approximation of binomial log-likelihood ratio 400 out of 1000 subjects \\(\\hat\\pi=0.01, n=100, k=1\\) 注意此圖中紅線提示的近似二次曲線，信賴區間的下限已經低於0，是無法接受的近似。 图 14.5: Quadratic approximation of binomial log-likelihood ratio 1 out of 100 subjects \\(\\hat\\pi=0.01, n=1000, k=10\\) 图 14.6: Quadratic approximation of binomial log-likelihood ratio 10 out of 1000 subjects \\(\\hat\\pi=0.01, n=10000, k=100\\) 图 14.7: Quadratic approximation of binomial log-likelihood ratio 100 out of 10000 subjects \\(\\hat\\pi=0.99, n=100, k=99\\) 注意此圖中紅線提示的近似二次曲線，信賴區間的上限已經大於1，和上面的 Figure 5. 一樣也是無法接受的近似。 图 14.8: Quadratic approximation of binomial log-likelihood ratio 99 out of 100 subjects 總結： 二次方程近似時，在二項分佈的情況下，隨着 \\(n, k\\) 增加，近似越理想。 "],
["-construction-of-a-hypothesis-test.html", "第 15 章 假設檢驗的構建 Construction of a hypothesis test 15.1 什麼是假設檢驗 Hypothesis testing 15.2 錯誤概率和效能方程 error probabilities and the power function 15.3 如何選擇要檢驗的統計量 15.4 複合假設 composite hypotheses 15.5 爲反對零假設 \\(H_0\\) 的證據定量 15.6 雙側替代假設情況下，雙側 \\(p\\) 值的定量方法 15.7 假設檢驗構建之總結 15.8 練習題", " 第 15 章 假設檢驗的構建 Construction of a hypothesis test 15.1 什麼是假設檢驗 Hypothesis testing 一般來說，我們的假設(或者叫假說) 是對與我們實驗觀察數據來自的總體(或人羣) 的概率分佈的描述。在參數檢驗的背景下，就是要檢驗描述這個總體(或人羣) 的概率分佈的參數 (parameters)。最典型的情況是，我們提出兩個互補的假設，一個叫作零假設(或者叫原假設) ，null hypothesis (\\(H_0\\))；另一個是與之對應的(互補的) 替代假設，althernative hypothesis (\\(H_1/H_A\\))。 例如，若 \\(X\\) 是一個服從二項分佈的隨機離散變量 \\(X\\sim Bin(5, \\theta)\\)。可以考慮如下的零假設和替代假設：\\(H_0: \\theta=\\frac{1}{2}; H_1: \\theta=\\frac{2}{3}\\)。 當建立了零假設和替代假設以後，假設檢驗就是要建立如下的規則以確定： 從樣本中計算所得的參數估計值爲多少時，拒絕零假設。(接受替代假設爲“真”) 從樣本中計算所得的參數估計值爲多少時，零假設不被拒絕。(接受零假設爲“真”) 注意：(這一段很繞) 上面的例子是零假設和替代假設均爲簡單假設的情況，實際操作中常常會設計更加複雜的(不對稱的) 假設：即簡單的 \\(H_0\\)，複雜的 \\(H_1\\)。如此一來當零假設 \\(H_0\\) 不被拒絕時，我們並不一定就接受之。因爲無證據證明 \\(H_1\\) 不等於有證據證明 \\(H_0\\)。(Absence of evidence is not evidence of absence). 換句話說，無證據讓我們拒絕 \\(H_0\\) 本身並不成爲支持 \\(H_0\\) 爲“真”的證據。因爲在實際操作中，當我們設定的簡單的零假設沒有被拒絕，可能還存在其他符合樣本數據的零假設；相反地，當樣本數據的計算結果拒絕了零假設，我們只能接受替代假設。所以，反對零假設的證據，同時就是支持替代假設的證據。 在樣本空間 sample space 中，決定了零假設 \\(H_0\\) 會被拒絕的子集 subset，被命名爲拒絕域 rejection region 或者 判別區域 critical region，用 \\(\\mathfrak{R}\\) 來標記。 15.2 錯誤概率和效能方程 error probabilities and the power function 這一部分也可以參考本書臨牀試驗樣本量計算 (Section 29) 部分。 表 15.1 : Definition of Type I and Type II error SAMPLE \\(\\underline{x} \\notin \\mathfrak{R}\\) Accept \\(H_0\\) \\(\\underline{x} \\in \\mathfrak{R}\\) Reject \\(H_0\\) TRUTH \\(H_0\\) is true \\(\\checkmark\\) \\(\\alpha\\) Type I error \\(H_1\\) is true \\(\\beta\\) Type II error \\(\\checkmark\\) 假如一個假設檢驗是關於總體參數 \\(\\theta\\) 的： \\[H_0: \\theta=\\theta_0 \\text{ v.s. } H_1: \\theta=\\theta_1 \\] 這個檢驗的效能被定義爲當替代假設爲“真”時，拒絕零假設的概率(該檢驗方法能夠檢驗出有真實差別的能力) ： \\[\\text{Power}=\\text{Prob}(\\underline{x}\\in\\mathfrak{R}|H_1\\text{ is true}) = 1-\\text{Prob}(\\text{Type II error})\\] 觀察數據只有兩種可能：落在拒絕域內，或者落在拒絕域之外。第二類錯誤我們常常使用 \\(\\beta\\) 來表示，所以 \\(\\text{Power}=1-\\beta\\)。 檢驗的顯著性水平用 \\(\\alpha\\) 來表示。\\(\\alpha\\) 的直觀意義就是，檢驗結果錯誤的拒絕了零假設 \\(H_0\\)，接受了替代假設 \\(H_1\\)，即假陽性的概率。 \\[\\text{Prob}(\\underline{x}\\in \\mathfrak{R} |H_0 \\text{ is true})=\\text{Prob(Type I error)}\\] 15.2.1 以二項分佈爲例 用本文開頭的例子： \\(X\\sim Bin(5,\\theta)\\)。和我們建立的零假設和替代假設：\\(H_0: \\theta=\\frac{1}{2}; H_1: \\theta=\\frac{2}{3}\\)： 考慮兩種檢驗方法： A 方法：當且僅當5次觀察都爲“成功”時才拒絕 \\(H_0 (\\text{i.e.}\\; X=5)\\)。所以此時判別區域 \\(\\mathfrak{R}\\) 爲 \\(5\\)。檢驗效能 \\(\\text{Power}=1-\\beta\\) 爲：\\(Prob(X=5|H_1 \\text{ is true})=(\\frac{2}{3})^5=0.1317\\)。顯著性水平 \\(\\alpha\\) 爲 \\(Prob(X=5|H_0 \\text{ is true})=(\\frac{1}{2})^5=0.03125\\)。 B 方法：當觀察到3,4,5次“成功”時，拒絕 \\(H_0 (\\text{i.e.} X=3,4,5)\\)。此時判別區域 \\(\\mathfrak{R}\\) 爲 \\(3,4,5\\)。檢驗效能 \\(Power\\) 爲：\\(Prob(X=3,4,\\text{ or }5|H_1 \\text{ is ture})=\\sum_{i=3}^5(\\frac{2}{3})^i(\\frac{1}{3})^{5-i}\\approx0.7901\\)；顯著性水平 \\(\\alpha\\) 爲：\\(Prob(X=3,4,5|H_0 \\text{ is true})=\\sum_{i=3}^5(\\frac{1}{2})^i(\\frac{1}{2})^{5-i}=0.5\\) # the power in test B dbinom(3,5,2/3)+dbinom(4,5,2/3)+dbinom(5,5,2/3) ## [1] 0.7901 # the size in test B dbinom(3,5,0.5)+dbinom(4,5,0.5)+dbinom(5,5,0.5) ## [1] 0.5 比較上面兩種檢驗方法，可以看到，用B方法時，我們有更高的概率獲得假陽性結果(犯第一類錯誤，錯誤地拒絕 \\(H_0\\)，接受 \\(H_1\\))，但是也有更高的檢驗效能 \\(1-\\beta\\)(真陽性更高) 。這個例子就說明了，試圖提高檢驗效能的同時，會提高犯第一類錯誤的概率。實際操作中我們常常將第一類錯誤的概率固定，例如 \\(\\alpha=0.05\\)，然後儘可能選擇檢驗效能最高的檢驗方法。 15.3 如何選擇要檢驗的統計量 在上面的二項分佈的實驗中，“成功的次數” 是我們感興趣的要檢驗的統計量。但也可能是第一次出現 “成功” 之前的實驗次數，或者，任何與假設相關的統計量。相似的，如果觀察不是離散變量而是連續的，可以拿來檢驗的指標就有很多，如均值，中位數，衆數，幾何平均值等。 幸運地是，當明確了零假設和替代假設後，我們可以利用 Neyman-Pearson lemma 似然比公式1: 來決定使用哪個統計量做檢驗最有效： \\[\\text{Neyman-Pearson lemma}=\\frac{L_{H_0}}{L_{H_1}}\\] 這公式很直觀，因爲當觀察數據更加支持 \\(H_1\\) 時 (\\(L_{H_1}\\) 更大)，\\(H_0\\) 的可能性相對更小，就更應該被拒絕。而且，由於似然比越小，他的對數就越小，實際計算時我們常使用對數似然比：\\(\\ell_{H_0}-\\ell_{H_1}\\)。 問題來了，那到底要多小才算小？這個進入拒絕域的閾值由兩個指標來決定： 被檢驗統計量的樣本分佈 (the sampling distribution of the test statistic) 第一類錯誤概率 \\(\\alpha\\) (the required value of \\(\\alpha\\)) 15.3.1 以已知方差的正態分佈爲例 假如已知 \\(X_1, \\cdots, X_n \\stackrel{i.i.d}{\\sim} N(\\mu, \\sigma^2)\\) 而且方差 \\(\\sigma^2\\) 也是已知的。如果令 \\(H_0: \\mu=5\\; ;H_1: \\mu=10\\) 可以通過如下的方法找到我們需要的最佳檢驗統計量 best statistic 根據之前的推導 (Section 13) 可知正態分佈的似然方程如下： \\[\\ell(\\mu|\\underline{x}) =-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-\\mu)^2\\] 所以已知 \\(\\sigma^2\\) 時，我們的零假設和替代假設之間的對數似然比 \\(\\ell_{H_0}-\\ell_{H_1}\\) 爲: \\[\\ell_{H_0}-\\ell_{H_1}=-\\frac{1}{2\\sigma^2}(\\sum_{i=1}^n(x_i-5)^2-\\sum_{i=1}^n(x_i-10)^2)\\] 然而，我們只需要考慮隨着數據變化的部分，所以忽略掉不變的部分2： \\[ \\begin{aligned} \\ell_{H_0}-\\ell_{H_1} &amp; = -(\\sum_{i=1}^n(x_i-5)^2-\\sum_{i=i}^n(x_i-10)^2)\\\\ &amp; = 75n - 2\\times(10-5)\\sum_{i=1}^nx_i \\\\ \\end{aligned} \\] 所以只要樣本和 (sum of sample) \\(\\sum_{i=1}^nx_i\\) (最佳統計量 best statistic) 足夠大，零假設就會被拒絕。而且注意到最佳統計量可以乘以任何常數用作新的最佳統計量。爲了方便我們就用樣本均數 \\(\\frac{1}{n}\\sum_{i=1}^nx_i\\) 作此處的最佳統計量。所以此時，我們的最佳檢驗就是當樣本均值足夠大，超過某個閾值時，我們拒絕零假設。而且，樣本均值的樣本分佈是可以知道的，這樣就便於我們繼續計算下一步：拒絕域 (判別區域) 。 15.4 複合假設 composite hypotheses 目前爲止我們討論的假設檢驗限制太多，實際操作時，我們多考慮類似如下的假設： \\(H_0: \\theta=\\theta_0 \\;\\text{v.s.}\\; H_1: \\theta&gt;\\theta_0\\) [單側的替代假設] \\(H_0: \\theta=\\theta_0 \\;\\text{v.s.}\\; H_1: \\theta\\neq\\theta_0\\) [雙側的替代假設] 所以我們面臨的問題是簡單假設中用於判定的最佳統計量，是始終如一地適用？我們一一來看： 15.4.1 單側替代假設 本章目前爲止的推導中我們發現，樣本均值越大，零假設和替代假設的對數似然比 \\(\\ell_{H_0}-\\ell_{H_1}\\) 越小。所以我們在樣本均值較大時，拒絕零假設，那麼就可以把原來使用的簡單替代假設 \\(H_1: \\mu=10\\) 擴展爲，任意大於 \\(5\\) 的 \\(\\mu\\) ，即 \\(\\mu&gt;5\\) 。因爲大於 \\(5\\) 的任何均值，都提供了更小的對數似然比，都會讓我們拒絕零假設。所以在正態分佈時，單側替代假設的最佳檢驗統計量還是樣本均值。 15.4.2 雙側替代假設 雙側替代假設的情況下，我們無法繼續使用樣本均值作爲最佳統計量。因爲當我們想檢驗：\\(H_0: \\mu=5 \\;\\text{v.s.}\\; H_1: \\mu&lt;5\\) 時，必須獲得足夠小的樣本均值才能讓我們拒絕零假設。此處暫且先按下不表。 15.5 爲反對零假設 \\(H_0\\) 的證據定量 重新再考慮複合假設：\\(H_0: \\theta=\\theta_0\\;\\text{v.s.}\\;H_1: \\theta&gt;\\theta_0\\) 假如存在一個總是可用的最佳檢驗統計量，用 \\(T\\) 來標記 (或 \\(T(x)\\))， 這個統計量足夠大時，我們拒絕 \\(H_0\\)。 別忘了我們還要給事先固定好的顯著性水平 \\(\\alpha\\) 定義與之相關的判別區域： \\[\\text{Prob}(\\underline{x}\\in\\mathfrak{R}|H_0)=\\alpha\\] 如果我們知道 \\(T\\) 的樣本分佈，我們就可以使用一個閾值 \\(c\\) 來定義這個判別區域： \\[Prob(T\\geqslant c|H_0)=\\alpha\\] 更加正式的，我們定義判別區域 \\(\\mathfrak{R}\\) 爲： \\[\\{\\underline{x}:\\text{Prob}(T(x)\\geqslant c|H_0)=\\alpha\\}\\] 換句話說，當統計量 \\(T&gt;c\\) 時，我們拒絕 \\(H_0\\) 。如果先不考慮拒絕或不拒絕的二元判定，我們可以用一個連續型測量值來量化反對零假設 \\(H_0\\) 的證據。再考慮從觀察數據中獲得的 \\(T\\) ，即數據告訴我們的 \\(t\\) 。所以，當 \\(t\\) 值越大，說明觀察值相對零假設 \\(H_0\\) 越往極端的方向走。因此我們可以用 \\(T\\) 的樣本分佈來計算觀察值大大於等於這個閾值(極端值) 時的概率： \\[p=\\text{Prob}(T\\geqslant t|H_0)\\] 這個概率公式被稱爲是單側 \\(p\\) 值 (one-side p-value)。單側 \\(p\\) 值越小，統計量 \\(T\\) 的樣本空間就有越小比例(越強) 的證據支持零假設 \\(H_0\\)。 我們把這以思想用到假設檢驗中時，就可以認爲： \\[p&lt;\\alpha \\Leftrightarrow t&gt;c\\] 所以用我們一貫的設定 \\(\\alpha=0.05\\)，所以如果計算獲得 \\(p&lt;0.05\\) 我們就認爲獲得了足夠強的拒絕零假設 \\(H_0\\) 的證據。 15.5.1 回到正態分佈的均值比較問題上來(單側替代假設) 繼續考慮 \\(X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim} N(\\mu, \\sigma^2)\\)，假設 \\(\\sigma^2=10\\)，我們要檢驗的是 \\(H_0: \\mu=5 \\;\\text{v.s}.\\; H_1: \\mu&gt;5\\) 確定最佳檢驗統計量：已經證明過，單側替代假設的最佳檢驗統計量是樣本均值 \\(\\bar{x}\\)。 確定該統計量的樣本分佈：已知樣本均數的樣本分佈是 \\(\\bar{X}\\sim N(\\mu,\\sigma^2/n)\\) 。\\(\\Rightarrow Z=\\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1)\\)，所以在 \\(H_0\\) 條件下，\\(\\Rightarrow Z=\\frac{\\bar{X}-5}{\\sqrt{10}/\\sqrt{n}} \\sim N(0,1)\\) 所以當一個檢驗的顯著性水平設定爲 \\(\\alpha=0.05\\) 時，我們用判別區域 \\(\\mathfrak{R}\\)，使統計量據落在該判別區域內的概率爲 \\(0.05\\)： \\(\\text{Prob}(\\bar{X}\\geqslant c|H_0) = 0.05\\) 已知在標準正態分佈時，\\(\\text{Prob}(Z\\geqslant1.64)=0.05=\\text{Prob}(\\frac{\\bar{X}-5}{\\sqrt{10}/\\sqrt{n}}\\geqslant1.64)\\) 假設樣本量是 \\(10\\)，那麼數據的判別區域 \\(\\mathfrak{R}\\) 就是 \\(\\bar{X}\\geqslant6.64\\)。 假設觀察數據告訴我們，\\(\\bar{X}=7.76\\) 。那麼這一組觀察數據計算得到的統計量落在了判別區域內，就提供了足夠的證據拒絕接受 \\(H_0\\)。 我們可以給這個觀察數據計算相應的單側 \\(p\\) 值： \\(p=\\text{Prob}(\\bar{X}\\geqslant7.76|H_0)=\\text{Prob}(Z+5\\geqslant7.76)\\\\=\\text{Prob}(Z\\geqslant2.76)=0.003\\) 所以，觀察數據告訴我們，在 \\(H_0\\) 的前提下，觀察值出現的概率是 \\(0.3\\%\\) 。即，在無數次重複取樣實驗中，僅有 \\(0.3\\%\\) 的結果可以給出支持 \\(H_0\\) 的證據。因此我們拒絕 \\(H_0\\) 接受 \\(H_1\\)。 15.6 雙側替代假設情況下，雙側 \\(p\\) 值的定量方法 图 15.1: Deliberately use an assymmetrical distribution to highlight the issues 此處故意使用一個左右不對稱的概率密度分佈來解釋。 現在的替代假設是雙側的： \\[H_0: \\theta=\\theta_0 \\;\\text{v.s.}\\; H_1: \\theta\\neq\\theta_0\\] 正常來說，雙側的假設檢驗應該分成兩個單側檢驗。即： \\(H_1: \\theta&gt;\\theta_0\\); \\(H_1: \\theta&lt;\\theta_0\\). 每個單側檢驗都有自己的最佳檢驗統計量。令 \\(T\\) 是 1. 的最佳檢驗統計量，該統計量的樣本分佈如上圖 15.1 所示(左右不對稱) 。假如觀察數據給出的統計量爲 \\(t_{\\text{obs}}\\)，那麼在概率上反對零假設的情況可以有兩種： \\(T\\geqslant t_{\\text{obs}}\\) 其中， \\(\\text{Prob}(T\\geqslant t_{\\text{obs}}|H_0)=\\tilde p\\); \\(T\\leqslant t^\\prime\\) 其中，\\(t^\\prime\\) 滿足： \\(\\text{Prob}(T\\leqslant t^\\prime|H_0) =\\tilde p\\)。(圖15.1) 所以概率密度分佈兩側的距離可以不對稱，但是只要左右兩側概率密度分佈的面積(\\(=\\tilde p\\))相同，那麼就可以直接認爲，雙側 \\(p\\) 值是兩側面積之和 (\\(p=2\\times \\tilde p\\))，且觀察數據提供的統計量落在這兩個面積內的話，都足以提供證據拒絕零假設 \\(H_0\\)。 注意： 被選中的 \\(t^\\prime\\) 值大小不大可能滿足：\\(|t^\\prime - E(T|\\theta_0)|=|t_{obs}-E(T|\\theta_0)|\\)。因爲那只有在完全左右對稱的分佈中才會出現。但是，此處我們關心的是面積左右兩邊的尾部要相等即可，所以我們只需要知道右半邊，較大的那個 \\(t_{obs}\\) 就完全足夠了。 回到上面的均值比較問題 (Section 15.5.1)。現在我們要進行雙側假設檢驗，即： \\(H_0: \\mu=5 \\text{ v.s. } H_1: \\mu\\neq5\\)，最佳統計量依然還是樣本均數 \\(\\bar{X}\\)。數據告訴我們說 \\(\\bar{X}=7.76\\)，因此雙側 \\(p\\) 值就是將已求得的單側 \\(\\tilde p\\) 值乘以 \\(2\\)： \\(\\text{two-sided } p=2\\tilde p= 0.006\\) 當然，實際操作中我們很少進行這樣繁瑣的論證，多數情況下就直接報告雙側 \\(p\\) 值。 15.7 假設檢驗構建之總結 按照如下的步驟一一構建我們的假設檢驗過程： 先建立零假設，和替代假設 (Section 15.1)； 定義最佳檢驗統計量 (用 Neyman-Pearson lemma) (Section 15.3)； 取得零假設條件下，最佳統計量的樣本分佈(通常都較爲困難，有時候我們會傾向於使用“不太理想”，但是計算較爲簡便的過程。) ； 定義拒絕域(判別區域) (常用 \\(\\alpha=0.05\\)) ； 計算觀察數據的檢驗統計量； 如果觀察數據的檢驗統計量落在了提前定義好的拒絕域內，那麼我們的檢驗結論就是：觀察數據拒絕了零假設支持替代假設。然而在實際操作時，如果發現數據的檢驗統計量不在拒絕域內，我們僅僅只能下結論說：觀察數據無法拒絕零假設(而不是接受零假設！) ； 報告計算得到的反對零假設的定量 \\(p\\) 值。 作爲統計學家，我們的任務是評價數據提供的證據，而不是簡單的去接受或者拒絕一個假設。 15.8 練習題 15.8.1 Q1 某種藥物有兩種使用方法：可以口服，也可以注射。兩種方法都被認爲可以使血漿中藥物濃度在24小時候達到相似的平均水平，\\(3 \\mu \\text{g/L}\\)。已知口服該藥物後，濃度的方差爲 \\(1\\)，而如果是注射的話方差只有 \\(1/4\\)。因此設計了一個口服臨牀實驗，觀察到24小時後血漿中藥物濃度數據爲：2.54, 0.93, 2.75, 4.51, 3.71, 1.62, 3.01, 4.13, 2.08, 3.33。假設這組觀察數據獨立同分佈 \\(\\stackrel{i.i.d}{\\sim} N(3, \\sigma^2)\\) 證明以下的假設的最佳檢驗統計量是 \\(\\sum_{i=1}^{10}(x_i-3)^2\\)： \\[H_0: \\sigma^2=1/4 \\text{ v.s. } H_1: \\sigma^2=1\\] 解 根據 Neyman-Pearson lemma (Section 15.3) 來判斷最佳檢驗統計量： 下面用 \\(\\sigma^2_0, \\sigma^2_1\\) 分別標記零假設和替代假設時的方差。 \\[ \\begin{aligned} L(\\sigma^2|\\underline{x},\\mu=3) &amp;= \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(-\\frac{1}{2}(\\frac{x_i-3}{\\sigma})^2) \\\\ \\Rightarrow \\ell(\\sigma^2) &amp;=-\\frac{1}{2}\\sum_{i=1}^n\\text{log}\\sigma^2-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-3)^2 \\\\ &amp;= -\\frac{n}{2}\\text{log}\\sigma^2-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(x_i-3)^2 \\\\ \\Rightarrow \\ell(\\sigma_0^2)-\\ell(\\sigma_1^2)&amp;= \\frac{n}{2}\\text{log}\\sigma_1^2+\\frac{1}{2\\sigma_1^2}\\sum_{i=1}^n(x_i-3)^2\\\\ &amp;\\;\\;\\;\\;\\;\\;-\\frac{n}{2}\\text{log}\\sigma_0^2-\\frac{1}{2\\sigma_0^2}\\sum_{i=1}^n(x_i-3)^2\\\\ &amp;=\\frac{n}{2}(\\text{log}\\sigma_1^2-\\text{log}\\sigma_0^2)+\\frac{1}{2}(\\frac{1}{\\sigma_1^2}-\\frac{1}{\\sigma_0^2})\\sum_{i=1}^n(x_i-3)^2\\\\ &amp;=\\frac{n}{2}\\text{log}\\frac{\\sigma_1^2}{\\sigma_0^2}+\\frac{1}{2}(\\frac{1}{\\sigma_1^2}-\\frac{1}{\\sigma_0^2})\\sum_{i=1}^n(x_i-3)^2 \\end{aligned} \\] 觀察上面的式子就會發現，當實驗重複後唯一會發生變化的就是後面的 \\(\\sum_{i=1}^n(x_i-3)^2\\)。 由於，\\(\\sigma_0^2=1/4, \\; \\sigma_1^2=1\\)，所以 \\((\\frac{1}{\\sigma_1^2}-\\frac{1}{\\sigma_0^2})&lt;0\\)。那麼當 \\(\\sum_{i=1}^n(x_i-3)^2\\) 越大，\\(\\ell(\\sigma_0^2)-\\ell(\\sigma_1^2)\\) 就越小。因此，這就是我們尋找的最佳檢驗統計量。 證明上面的檢驗統計量總是可以作爲最佳檢驗統計量，用於檢驗單側替代假設：\\(H_1: \\sigma^2&gt;1/4\\)。 上面的替代假設中 \\(\\sigma_1^2=1\\)，如果將替代假設改成 \\(\\sigma_1^2&gt;1/4\\)，那麼 \\((\\frac{1}{\\sigma_1^2}-\\frac{1}{\\sigma_0^2})&lt;0\\) 依然成立。所以，\\(\\sum_{i=1}^n(x_i-3)^2\\)，或者這部分乘以任何一個不變的常數依然是替代假設爲 \\(H_1: \\sigma^2&gt;1/4\\) 時的最佳檢驗統計量。 在 \\(H_0\\) 條件下，樣本分佈 \\(\\sum_{i=1}^{10}(x_i-3)^2\\) 是怎樣的分佈？利用這個分佈來定義顯著性水平爲 \\(\\alpha=0.05\\) 時的拒絕域。 在\\(H_0\\) 條件下，有： \\[X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}N(3,1/4)\\\\ \\Rightarrow \\frac{X_i-3}{\\sqrt{1/4}}\\sim N(0,1)\\\\ \\Rightarrow (\\frac{X_i-3}{\\sqrt{1/4}})^2 \\sim \\mathcal{X}_1^2\\\\ \\Rightarrow \\sum_{i=1}^{10}(\\frac{X_i-3}{\\sqrt{1/4}})^2 \\sim \\mathcal{X}_{10}^2\\\\ \\Rightarrow 4\\sum_{i=1}^{10}(X_i-3)^2\\sim \\mathcal{X}_{10}^2\\\\ \\text{Let } T=\\sum_{i=1}^{10}(X_i-3)^2\\\\ \\Rightarrow 4T \\sim \\mathcal{X}_{10}^2\\] 拒絕域被定義爲檢驗統計量取大於等於某個臨界值時概率爲 \\(0.05\\)，即 \\(\\text{Prob}(T\\geqslant t)=0.05\\) \\[\\text{Prob}(4T\\geqslant \\mathcal{X}^2_{10,0.95})=0.05\\\\ \\Rightarrow \\text{Prob}(T\\geqslant 1/4\\mathcal{X}^2_{10,0.95})=0.05\\] 所以，此處當顯著性水平定爲 \\(\\alpha=0.05\\) 時，拒絕域就是要大於自由度爲 \\(10\\) 的卡方分佈的 \\(95\\%\\) 分位點。 在 \\(H_0\\) 條件下，該檢驗統計量的正態分佈模擬是怎樣的？ 根據中心極限定理(Section 8) 和 卡方分佈的性質 (Section 11) \\[n\\rightarrow \\infty, X_n^2\\sim N(n, 2n)\\] 所以近似地， \\[\\mathcal{X}_{10}^2\\sim N(\\text{E}(\\mathcal{X}_{10}^2)=10,\\text{Var}(\\mathcal{X}_{10}^2)=20)\\\\ \\Rightarrow 4T\\sim \\text{approx} N(10,20)\\\\ \\Rightarrow \\frac{4T-10}{\\sqrt{20}} \\stackrel{\\cdot}{\\sim} N(0,1)\\] 用上面的正態分佈模擬，和觀察嘗試對單側替代假設作統計檢驗並依據所得結果作出結論：\\[H_0: \\sigma^2=1/4 \\text{ v.s. } H_1: \\sigma^2&gt;1/4\\] 用上面的正態分佈近似法，我們可以計算拒絕域： \\[\\text{Prob}(\\frac{4T-10}{\\sqrt{20}}\\geqslant Z_{0.95})=0.05\\] 已知標準正態分佈的 \\(95\\%\\) 分位點取值 \\(1.64\\)，所以拒絕域： \\[\\frac{4T-10}{\\sqrt{20}}\\geqslant 1.64\\\\ \\Rightarrow T\\geqslant1/4(10+1.64\\sqrt{20})=1/4\\times17.33\\] 由觀察數據可得：\\(T=11.5\\) ，所以觀察數據的檢驗統計量落在了拒絕域內。我們的結論是：觀察數據提供了極強的證據證明在顯著性水平爲 \\(5\\%\\) 時，口服該藥物24小時後的血漿藥物濃度的方差大於 \\(1/4\\)。 區分與之前討論的對數似然比 (Section 13)，之前討論的對數似然比指的是所有的似然和極大似然之間的比，此處的似然比只是純粹在探討兩個假設之間的似然比，與極大似然無關。↩ Rememer that \\(\\ell_{H_0}-\\ell_{H_1}\\) is a random variable: the data varies each time we sample, with consequently varying relative support for the hypotheses, and so we are only interested in that part of \\(\\ell_{H_0}-\\ell_{H_1}\\) which depends on the results, the data, which vary with each sample (i.e. which contains the random part); the constant part provides no information on the relative support the data give to the hypotheses, so we ignore it.↩ "],
["section-16.html", "第 16 章 假設檢驗的近似方法 16.1 近似和精確檢驗 approximate and exact tests 16.2 精確檢驗法之 – 似然比檢驗法 Likelihood ratio test 16.3 練習題 16.4 近似檢驗法之 – Wald 檢驗 16.5 近似檢驗法之 – Score 检验 16.6 LRT, Wald, Score 檢驗三者的比較 16.7 練習題", " 第 16 章 假設檢驗的近似方法 本章教你怎麼徒手搞似然比檢驗 (likelihood ratio test)，Wald 檢驗 (Wald test)，和 Score 檢驗 (Score test)。 16.1 近似和精確檢驗 approximate and exact tests 前一章描述了如何用對數似然比尋找最佳檢驗統計量 (Section 15.3)。一旦找到並確定了最佳檢驗統計量，接下去還需要確定這個最佳檢驗統計量的樣本分佈，用定好的顯著性水平(\\(\\alpha=0.05\\))確定拒絕域，再使用觀察數據計算數據本身的統計量，然後對反對零假設的證據定量(計算 \\(p\\) 值) 。前一章用的例子均來自於正態分佈，所以我們都能夠不太複雜地獲得樣本均值，樣本方差等較容易取得樣本分佈的檢驗統計量。正如我們在前一章最後部分 (Section 15.7) 總結的那樣，大多數情況下我們沒有那麼幸運。最佳檢驗統計量的樣本分佈會很難確定。所以另一個進行假設檢驗的途徑就是近似檢驗法 (approximate tests)。 16.2 精確檢驗法之 – 似然比檢驗法 Likelihood ratio test 記得我們之前說到，簡單假設 \\(H_0: \\theta=\\theta_0\\text{ v.s. } H_1: \\theta=\\theta_1\\) 的檢驗的最佳檢驗統計量可以使用 Neyman-Pearson lemma (尼曼皮爾森輔助定理) (Section 15.3) 來確定： \\[\\ell_{H_0}-\\ell_{H_1} = \\ell(\\theta_0)-\\ell(\\theta_1)\\] 如果假設變成了複合型假設：\\(H_0: \\theta\\in\\omega_0 \\text{ v.s. } H_1: \\theta\\in\\omega_1\\)。此時，\\(\\omega_0, \\omega_1\\) 分別指兩種假設條件下我們關心的總體參數的可能取值範圍。那麼可以把上面的定理擴展成，在 \\(\\omega_0, \\omega_1\\) 兩個取值範圍內，零假設和對立假設在給出的觀察數據條件下的極大似然之比： \\[\\text{log}\\frac{\\text{max}_{H_0}[L(\\theta|data)]}{\\text{max}_{H_1}[L(\\theta|data)]}=\\text{max}_{H_0}[\\ell(\\theta|data)]-\\text{max}_{H_1}[\\ell(\\theta|data)]\\\\ =\\text{max}_{\\theta\\in\\omega_0}[\\ell(\\theta|data)]-\\text{max}_{\\theta\\in\\omega_1}[\\ell(\\theta|data)]\\] 典型的假設檢驗情況下，我們面對的是簡單的零假設和複合型的替代假設： \\[H_0: \\theta=\\theta_0 \\text{ v.s. } H_1: \\theta\\neq\\theta_0\\] 所以在這個情況下，套用擴展以後的 Neyman-Pearson lemma： \\[\\text{max}_{H_0}[\\ell(\\theta)]-\\text{max}_{H_1}[\\ell(\\theta)]=\\ell(\\theta_0) - \\ell(\\hat\\theta)=llr(\\theta_0)\\] 之前討論對數似然比 (Section 13.3) 時我們已知： \\[\\text{Under }H_0: \\theta=\\theta_0\\Rightarrow -2llr(\\theta_0)\\stackrel{\\cdot}{\\sim}\\mathcal{X}_1^2\\] 於是利用自由度爲 \\(1\\) 的卡方檢驗的特徵我們就可以爲反對零假設的證據定量，計算關鍵的拒絕域。如果說顯著性水平爲 \\(\\alpha\\) 那麼，我們拒絕零假設 \\(H_0:\\theta=\\theta_0\\) 的拒絕域是： \\[-2llr(\\theta_0)&gt;\\mathcal{X}^2_{1,1-\\alpha}\\] 當使用 \\(\\alpha=0.05\\) 時，這個關鍵的拒絕域就是：\\(-2llr(\\theta_0)&gt;3.84\\)。 這就是傳說中的 (對數) 似然比檢驗，(log-)Likelihood ratio test (LRT)。 LRT 的優點： 簡單； \\(p\\) 值不會被參數尺度 (parameter scale) 左右，也就是說如果我們對參數進行了數學轉換 (Section 14.2) 也不會影響似然比檢驗計算得到的 \\(p\\) 值大小。 LRT 的缺點： 非正態分佈的數據時，LRT 只能算是漸進有效 (asymptotic valid)，即樣本量要足夠大時結果才能令人滿意； 無法總是保證這是最佳檢驗統計量； 需要計算兩次對數似然 (MLE 和 零假設時)。 16.3 練習題 假設有在觀察對象 \\(n=100\\) 人中發生了 \\(k=40\\) 個事件。假定數據服從二項分佈，已知人羣中每個人發生該事件的概率爲 \\(\\pi_0=0.5\\)。嘗試計算似然比檢驗統計量：\\(-2llr(\\pi_0)\\)，並進行顯著性水平爲 \\(\\alpha=0.05\\) 的假設檢驗：\\(H_0: \\pi=\\pi_0 \\text{ v.s. }H_1: \\pi\\neq\\pi_0\\) 解 \\[ \\begin{aligned} &amp;\\because f(k=40|\\pi) = \\binom{100}{40}\\pi^{40}(1-\\pi)^{100-40} \\\\ &amp;\\text{Ignoring terms} \\text{ not with } \\pi \\\\ &amp;\\therefore \\ell(\\pi|k=40) = 40\\text{log}\\pi+60\\text{log}(1-\\pi) \\\\ &amp;\\Rightarrow \\ell^\\prime(\\pi|k=40) = \\frac{40}{\\pi}-\\frac{60}{1-\\pi} \\\\ &amp;\\text{Let } \\ell^\\prime(\\pi|k=40) = 0 \\\\ &amp;\\Rightarrow \\frac{40}{\\pi}-\\frac{60}{1-\\pi} =0 \\\\ &amp;\\Rightarrow \\text{ MLE } \\hat\\pi=0.4 \\\\ &amp;\\Rightarrow llr(\\pi_0)=\\ell(\\pi_0)-\\ell(\\hat\\pi) \\\\ &amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;=40\\text{log}0.5+60\\text{log}(1-0.5)-40\\text{log}0.4-60\\text{log}(1-0.4)\\\\ &amp;\\;\\;\\;\\;\\;\\;\\;\\;\\;=-2.013\\\\ &amp;\\Rightarrow -2llr=4.026 &gt; \\text{Pr}(\\mathcal{X}^2_{1,0.95})=3.84 \\end{aligned} \\] 所以當顯著性水平爲 \\(\\alpha=0.05\\) 時，數據提供了足夠拒絕零假設的證據。該事件在此人羣中發生的概率要低於人羣的 \\(0.5\\)。 16.4 近似檢驗法之 – Wald 檢驗 和 LRT 一樣， Wald 檢驗也適用於檢驗 \\(H_0: \\theta=\\theta_0 \\text{ v.s. } H_1: \\theta\\neq\\theta_0\\)。但是本方法其實是使用對數似然比方程的近似二次方程 (Section 14)。相比之下，LRT 使用的是精確的對數似然比，只對檢驗統計量 \\(-2llr\\) 進行了自由度爲 \\(1\\) 的卡方分佈 \\(\\mathcal{X}_1^2\\) 近似。本節介紹的 Wald 檢驗過程中使用了兩次近似，一次是計算對數似然比時使用了二次方程，一次則是和 LRT 一樣對檢驗統計量進行 \\(\\mathcal{X}_1^2\\) 近似。 根據之前的對數似然比近似結論 (Section 14.1) ： \\[llr(\\theta)\\approx-\\frac{1}{2}(\\frac{M-\\theta}{S})^2\\text{ asymptotically}\\] 其中，\\(M\\) 是 \\(\\text{MLE }\\hat\\theta\\)，\\(S=\\sqrt{\\left.-\\frac{1}{\\ell^{\\prime\\prime}(\\theta)}\\right\\vert_{\\theta=\\hat{\\theta}}}\\) 而且前一節我們也看到， \\[ \\text{Under }H_0: \\theta=\\theta_0\\Rightarrow -2llr(\\theta_0) \\stackrel{\\cdot}{\\sim}\\mathcal{X}_1^2\\\\ \\Rightarrow -2\\times-\\frac{1}{2}(\\frac{M-\\theta_0}{S})^2 \\stackrel{\\cdot}{\\sim}\\mathcal{X}_1^2 \\\\ \\Rightarrow (\\frac{M-\\theta_0}{S}) \\stackrel{\\cdot}{\\sim} N(0,1)\\\\ \\text{Let } W=(\\frac{M-\\theta_0}{S}) \\] \\(W\\) 就是我們在 Wald 檢驗中用到的檢驗統計量。接下來就可以計算給定顯著水平 \\(\\alpha\\) 時的拒絕域，給 \\(p\\) 值定量： 當 \\(W&gt;N(0,1)_{1-\\alpha/2}\\) 或 \\(W&lt;N(0,1)_{\\alpha/2}\\)時，拒絕 \\(H_0: \\theta=\\theta_0\\)； 或者，當 \\(W^2&gt;\\mathcal{X}^2_{1,1-\\alpha}\\) 時，拒絕 \\(H_0: \\theta=\\theta_0\\)。 這就是我們心心念念的 Wald 檢驗。 图 16.1: Likelihood ratio and Wald tests: solid (green) line is log-likelihood ratio, dashed (red) is quadratic approximation 上圖 16.1 解釋了 LRT 和 Wald 檢驗的不同之處。紅色虛線是二次方程，用於近似似然比方程(綠色實線) 。二者在 \\(\\text{MLE}=\\hat\\theta\\) 時同時取極大值。Wald 檢驗的是，數據提供的 \\(\\hat\\theta\\) 和我們想要比較的零假設 \\(\\theta_0\\) 之間的橫軸差距。在檢驗量 \\(W\\) 中我們還把這個差除以觀察數據均值的標準差(數據的標準誤) 。 如果數據本身波動大，\\(W\\) 的分母(標準誤) 較大，那麼即使 \\(\\hat\\theta - \\theta_0\\) 保持不變，統計量變小，反對零假設的證據也就越小。反觀，LRT 檢驗的檢驗統計量就是上圖 16.1 顯示的縱軸差 \\(\\ell(\\theta_0)-\\ell(\\hat\\theta)\\) 的大小。二者之間的關係被直觀的顯示在圖中。 Wald 檢驗優點： 比 LRT 略簡單； 不必再計算零假設時的對數似然，只需要 \\(MLE\\) 和它的標準誤。 Wald 檢驗缺點： 兩次近似(LRT只用了一次近似) ； 無法總是保證這是最佳檢驗統計量； 參數如果被數學轉換 (Section 14.2)，\\(p\\) 值會跟着變化。 16.4.1 再以二項分佈爲例 在 \\(n\\) 個實驗對象中觀察到 \\(k\\) 個事件，使用參數爲 \\(\\pi\\) 的二項分佈模型來模擬。使用 Wald 檢驗法對下列假設做出統計檢驗： \\(H_0: \\pi=\\pi_0 \\text{ v.s. } H1: \\pi\\neq\\pi_0\\)。將參數 logit 轉換 (log-odds) 之後，對轉換後的新參數再做一次 Wald 檢驗。 解 根據之前的二次方程近似法推導 (Section 14.1.3)： \\[ \\begin{aligned} &amp; M=\\text{MLE}=\\hat\\pi=\\frac{k}{n}=p\\\\ &amp; S=se(\\hat\\pi)=\\sqrt{\\frac{p(1-p)}{n}}\\\\ &amp; \\Rightarrow \\text{Under } H_0: \\pi=\\pi_0\\\\ &amp; W=(\\frac{p-\\pi_0}{\\sqrt{\\frac{p(1-p)}{n}}})\\stackrel{\\cdot}{\\sim} N(0,1) \\end{aligned} \\] 根據參數數學轉換的性質 (Section 14.2) \\[ \\begin{aligned} &amp;\\text{New parameter } \\beta=g(\\pi)=\\text{logit}(\\pi)=\\text{log}\\frac{\\pi}{1-\\pi}\\\\ &amp; \\text{MLE}=\\text{logit}(\\hat\\pi)=\\text{log}\\frac{\\hat\\pi}{1-\\hat\\pi} \\\\ &amp; \\text{Here we need to use delta-method to approximate standard error of } g(\\pi)\\\\ &amp; S=se[g(\\hat\\pi)]\\approx g^\\prime(\\pi)\\times se(\\hat\\pi) \\\\ &amp; = \\frac{1}{\\hat\\pi(1-\\hat\\pi)}\\sqrt{\\frac{p(1-p)}{n}}\\\\ &amp; =\\sqrt{\\frac{1}{k}+\\frac{1}{n-k}} \\\\ &amp; \\text{So the Wald test becomes}\\\\ &amp; H_0: \\beta=\\beta_0\\\\ &amp; \\Rightarrow W=\\frac{\\text{log}(\\frac{\\hat\\pi}{1-\\hat\\pi})-\\text{log}(\\frac{\\pi_0}{1-\\pi_0})}{\\sqrt{\\frac{1}{k}+\\frac{1}{n-k}}}\\stackrel{\\cdot}{\\sim} N(0,1) \\end{aligned} \\] 可見對參數進行了數學轉換之後，檢驗統計量的計算式發生了變化。因此 \\(p\\) 值也會不同。 16.5 近似檢驗法之 – Score 检验 注意到 Wald 檢驗使用的近似二次方程是在 MLE， 也就是極大似然比時的點 \\(\\hat\\theta\\) 和對數似然比方程取相同的值和相同曲率 (二次導數)。 可以類比的是，Score 检验是基于另一種二次方程模擬，Score 檢驗的近似二次方程和對數似然比方程在零假設 (\\(\\theta_0\\)) 時取相同的曲率。所以，Score 檢驗使用的近似方程在 \\(\\theta_0\\) 時和對數似然比方程在相同位置時的傾斜度 (一階導數)，和曲率 (坡度的變化程度，二階導數) 相同。所以令 \\(U\\) 爲對數似然比方程在 \\(\\theta_0\\) 時的坡度，定義 \\(V\\) 是對數似然比方程在 \\(\\theta_0\\) 時的曲率的負數： \\[ \\begin{aligned} &amp; U=\\ell^\\prime(\\theta)|_{\\theta=\\theta_0}=\\ell^\\prime(\\theta_0)\\\\ &amp; V=-E[\\ell^{\\prime\\prime}(\\theta)]|_{\\theta=\\theta_0}=-E[\\ell^{\\prime\\prime}(\\theta_0)] \\end{aligned} \\] 注：此處的 \\(V=-E[l^{\\prime\\prime}(\\theta_0)]\\) 又常常被叫做 Expected Fisher information。 記得在 Wald 檢驗中使用的近似方程： \\[llr(\\theta)\\approx-\\frac{1}{2}(\\frac{M-\\theta}{S})^2\\text{ asymptotically}\\] 令 \\(q(\\theta)=-\\frac{1}{2}(\\frac{M-\\theta}{S})^2\\) 就有： \\[ \\begin{aligned} &amp; q^\\prime(\\theta) =\\frac{M-\\theta}{S^2}\\\\ &amp; \\Rightarrow q^\\prime(\\theta_0) =\\frac{M-\\theta_0}{S^2}\\\\ &amp; q^{\\prime\\prime}(\\theta) =-\\frac{1}{S^2}\\\\ &amp; \\Rightarrow q^{\\prime\\prime}(\\theta_0)=E[l^{\\prime\\prime}(\\theta_0)]\\\\ &amp; \\Rightarrow \\frac{1}{S^2} =-E[l^{\\prime\\prime}(\\theta_0)]\\\\ &amp; q^\\prime(\\theta_0) = \\frac{M-\\theta_0}{S^2} = -E[l^{\\prime\\prime}(\\theta_0)](M-\\theta_0)\\\\ &amp; = \\ell^\\prime(\\theta_0)\\\\ &amp; \\Rightarrow M-\\theta_0 = -\\frac{\\ell^\\prime(\\theta_0)}{E[l^{\\prime\\prime}(\\theta_0)]}\\\\ &amp; \\Rightarrow M = -\\frac{\\ell^\\prime(\\theta_0)}{E[l^{\\prime\\prime}(\\theta_0)]}+\\theta_0\\\\ &amp; q(\\theta)=-\\frac{1}{2}(\\frac{M-\\theta}{S})^2=\\frac{E[l^{\\prime\\prime}(\\theta_0)]}{2}(-\\frac{\\ell^\\prime(\\theta_0)}{E[l^{\\prime\\prime}(\\theta_0)]}+\\theta_0-\\theta)^2\\\\ &amp; q(\\theta)=-\\frac{V}{2}(\\frac{U}{V}+\\theta_0-\\theta)^2\\\\ &amp; \\Rightarrow \\text{ Under } H_0: \\theta=\\theta_0\\\\ &amp; \\Rightarrow q(\\theta_0)=-\\frac{V}{2}(\\frac{U}{V})^2=-\\frac{U^2}{2V}\\\\ &amp; \\Rightarrow -2q(\\theta_0)=\\frac{U^2}{V} \\stackrel{\\cdot}{\\sim}\\mathcal{X}_1^2\\\\ &amp; \\text{Or equivalently} \\frac{U}{\\sqrt{V}} \\stackrel{\\cdot}{\\sim} N(0,1) \\end{aligned} \\] 這就是 Score 檢驗時使用的檢驗統計量。相應的拒絕域就可以被定義爲： 當 \\(\\frac{U^2}{V}&gt;\\mathcal{X}_{1,1-\\alpha}^2\\) 時，拒絕 \\(H_0\\) 如下面的示意圖 16.2 所示，Score 檢驗，比較的是 \\(\\theta_0\\) 時的校正後似然方程的坡度 (一階導數/二階導數)，和極大似然時的坡度 (一階導數=0) 的差別。如果這個值越大，說明零假設時的似然和極大似然 (觀察數據的信息) 的距離越遠，拒絕零假設的證據就越有力。 图 16.2: Score test: solid (green) line is log-likelihood ratio, dashed (red) is quadratic approximation Score 檢驗優點： 比 LRT 簡單； 不需要計算 MLE，只需要計算零假設時的對數似然比方程之坡度和曲率； 在流行病學用到的檢驗方法中最常用，也最容易擴展 (Mantel-Haenszel test, log rank test, generalised linear models such as logistic, Poisson, Cox regressions)。 Score 檢驗缺點： 和 Wald 檢驗一樣用到了兩次近似； 無法總是保證這是最佳檢驗統計量； 參數如果被數學轉換 (Section 14.2)，\\(p\\) 值會跟着變化。 16.5.1 再再以二項分佈爲例 \\(K\\sim Bin(n, \\pi)\\) 假如已知人羣中事件發生的概率是 \\(\\pi_0\\)。試推導此時的 Score 檢驗的檢驗統計量。 解 對二項分佈數據進行 Score 檢驗的時候我們需要計算 \\(U, V\\)，然後計算統計量 \\(\\frac{U^2}{V}\\) 和 \\(\\mathcal{X}_1^2\\) 比較即可。 \\[ \\begin{aligned} &amp; \\text{Let } p=\\frac{k}{n} \\\\ &amp; \\ell(\\pi|k) = k\\text{log}(\\pi)+(n-k)\\text{log}(1-\\pi)\\\\ &amp; \\ell^\\prime(\\pi)=\\frac{k}{\\pi}-\\frac{n-k}{1-\\pi}=\\frac{k-n\\pi}{\\pi(1-\\pi)}\\\\ &amp; = \\frac{p-\\pi}{\\pi(1-\\pi)/n}\\\\ &amp; \\Rightarrow U = \\ell^\\prime(\\pi_0)=\\frac{p-\\pi_0}{\\pi_0(1-\\pi_0)/n}\\\\ &amp; \\ell^{\\prime\\prime}(\\pi|K)=-\\frac{K}{\\pi^2}-\\frac{n-K}{(1-\\pi)^2}\\\\ &amp; \\Rightarrow -\\ell^{\\prime\\prime}(\\pi|K)=\\frac{K}{\\pi^2}+\\frac{n-K}{(1-\\pi)^2}\\\\ &amp; \\because E(K)=n\\pi\\\\ &amp; \\therefore -E[\\ell^{\\prime\\prime}(\\pi|K)]=\\frac{n\\pi}{\\pi^2}+\\frac{n-n\\pi}{(1-\\pi)^2}\\\\ &amp; =\\frac{n}{\\pi}+\\frac{n}{1-\\pi}=\\frac{n}{\\pi(1-\\pi)}\\\\ &amp; \\text{ Under } H_0: \\pi=\\pi_0 \\Rightarrow V=-E[\\ell^{\\prime\\prime}(\\pi_0)]=\\frac{n}{\\pi_0(1-\\pi_0)}\\\\ &amp; \\Rightarrow \\frac{U^2}{V}=\\frac{(p-\\pi_0)^2}{\\pi_0(1-\\pi_0)/n} \\stackrel{\\cdot}{\\sim}\\mathcal{X}_1^2\\\\ &amp; \\text{OR } \\frac{U}{\\sqrt{V}} = \\frac{p-\\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}} \\stackrel{\\cdot}{\\sim} N(0,1) \\end{aligned} \\] 16.6 LRT, Wald, Score 檢驗三者的比較 LRT 比較的是對數似然方程在零假設 \\(H_0\\) 和極大似然估計 (MLE) 時之間的縱軸差 (圖 16.1)；Wald 檢驗試圖直接比較 MLE 和 \\(H_0\\) 的橫軸差 (二次方程近似法，並用標準誤校正) (圖 16.1)；Score 檢驗比較的是對數似然方程在 \\(H_0\\) 時的切線斜率 (二次方程近似法，用曲率也就是二階導數校正) (圖 16.2)。三種檢驗比較的東西各不相同，但是這種差距大到進入拒絕域時，數據就會拒絕零假設。其中 Score 檢驗的計算過程最爲簡便，只需要計算 \\(H_0\\) 時對數似然方程的一階和二階導數，而不用去計算 MLE，因此更多的被應用在流行病學數據計算中。 如果對數似然方程本身就是左右對稱的 (正態分佈的情況下)，這三個檢驗方法計算的所有結果都是完全一致的。如果對數似然方程只是近似左右對稱，那麼三者的計算結果會十分接近。可以說，三種檢驗方法是漸進等價的。 如果對參數進行了數學轉換，三者中只有 LRT 的計算結果保持不變。如果對參數的數學轉換使得對數似然方程更加接近左右對稱的二次方程，那麼 Wald 和 Score 檢驗的計算結果可以得到改善。 如果說，MLE 和 零假設之間的差距很大，那麼 Wald 或者 Score 檢驗所使用的二次方程近似法的誤差會增加，此時傾向於使用 LRT 來進行精確檢驗。當然如果當樣本量較大，要檢驗的差距也很大，三種檢驗方案都能夠提供證據拒絕零假設 (\\(p\\) 值都會很小)。 如果三種檢驗方案給出的計算結果迥異，即使使用了數學轉換結果也沒有明顯改善的話，那麼最大的問題是樣本量太小。這時候還是老老實實用 LRT 吧。 幾乎所有的參數檢驗都歸類與這章節介紹的三種檢驗方法。比如說 \\(Z\\) 檢驗， \\(t\\) 檢驗， \\(F\\) 檢驗都是 LRT。在流行病學研究中最常用的還是 Score 檢驗。 我們的結論是，當條件允許的情況下，統計檢驗都推薦儘量使用精確檢驗 LRT。 16.7 練習題 16.7.1 Q1 在對數似然比章節 (Section 13.2)，我們曾經證明過，已知方差時： \\[ \\begin{aligned} &amp; llr(\\mu|\\underline{x})=\\ell(\\mu|\\underline{x})=-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2\\\\ &amp; \\Rightarrow -2llr(\\mu|\\underline{x})=-2\\ell(\\mu|\\underline{x})=(\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}})^2 \\end{aligned} \\] 當觀察數據 \\(X_1,\\cdots,X_n\\sim N(\\mu,1^2)\\) ，求 LRT, Wald, Score 三種檢驗方法對下列假設進行檢驗時的檢驗統計量： \\(H_0: \\mu=\\mu_0 \\text{ v.s. } H_1: \\mu\\neq\\mu_0\\) 解 \\[ \\begin{aligned} &amp; \\text{Model: } X_1, \\cdots, X_n \\stackrel{i.i.d}{\\sim} N(\\mu, 1)\\\\ &amp; H_0: \\mu=\\mu_0 \\text{ v.s. } H_1: \\mu\\neq\\mu_0\\\\ &amp; \\text{Model } \\Rightarrow \\bar{X} \\sim N(\\mu, \\frac{1}{n}) \\\\ &amp; \\text{If we observe } \\bar{X} = \\bar{x}\\\\ &amp; \\ell(\\mu|\\bar{x})=-\\frac{1}{2}(\\frac{\\bar{x}-\\mu}{1/\\sqrt{n}})^2\\\\ &amp; \\textbf{For LRT, under } H_0: \\mu=\\mu_0 \\Rightarrow -2llr(\\mu_0) \\stackrel{\\cdot}{\\sim}\\mathcal{X}_1^2\\\\ &amp; \\Rightarrow \\frac{\\bar{x}-\\mu}{1/\\sqrt{n}} \\sim N(0,1)\\\\ &amp; \\textbf{For Wald test, under } H_0: \\mu=\\mu_0 \\Rightarrow \\frac{M-\\mu_0}{S}\\sim N(0,1) \\\\ &amp; \\Rightarrow \\frac{\\bar{x}-\\mu}{1/\\sqrt{n}} \\sim N(0,1)\\\\ &amp; \\textbf{For Score test, under } H_0: \\mu=\\mu_0 \\Rightarrow U=\\ell^\\prime(\\mu_0), V=-E[\\ell^{\\prime\\prime}(\\mu_0)]\\\\ &amp; U=\\ell^\\prime(\\mu_0)=(\\frac{\\bar{x}-\\mu_0}{1/\\sqrt{n}})\\sqrt{n}=\\frac{\\bar{x}-\\mu_0}{1/n}\\\\ &amp; \\ell^{\\prime\\prime}(\\mu_0)=-\\frac{1}{1/n}=-n \\Rightarrow V=-E[n]=n\\\\ &amp; \\frac{U^2}{V}=(\\frac{\\bar{x}-\\mu_0}{1/n})^2/n=(\\frac{\\bar{x}-\\mu_0}{1/\\sqrt{n}})^2\\\\ &amp; \\Rightarrow \\frac{U^2}{V} \\sim \\mathcal{X}_1^2 \\Rightarrow \\frac{U}{\\sqrt{V}}=\\frac{\\bar{x}-\\mu_0}{1/\\sqrt{n}} \\sim N(0,1) \\end{aligned} \\] 本題證明了，當數據服從正態分佈時，三種檢驗方法使用的檢驗統計量，是完全一致的。 16.7.2 Q2 根據醫生的觀察，某種癌症患者的生存時間服從平均值爲 \\(1/\\beta_0\\) 的指數分佈 (exponentially distributed)。有一種新藥物可以改善平均生存時間 (仍然服從指數分佈)。已知指數分佈的密度方程是：\\(f(x|\\beta)=\\beta \\text{exp} (-\\beta x), \\text{ where } \\beta, x&gt;0\\)。 證明指數分佈的均值是 \\(1/\\beta\\) 解 \\[ \\begin{aligned} &amp; X\\sim f(x|\\beta), x&gt;0 \\Rightarrow E(X)=\\int_0^\\infty x\\cdot f(x)\\text{d} x = \\int_0^\\infty x\\cdot \\beta \\cdot e^{-\\beta x} \\text{d}x\\\\ &amp; E(x)= - \\int_0^\\infty x\\cdot \\frac{\\text{d}e^{-\\beta x}}{\\text{d}x} \\cdot \\text{d}x\\\\ &amp; \\text{We can now integrate by parts, using } \\int_a^b u \\frac{\\text{d}v}{\\text{d}x} \\text{d}x = [uv]_a^b-\\int_a^b v \\frac{\\text{d}u}{\\text{d}x} \\text{d}x \\\\ &amp; E(X) = -[x\\cdot e^{-\\beta x}]_0^\\infty + \\int_0^\\infty e^{-\\beta x} \\text{d} x \\\\ &amp; \\;\\;\\;\\; = -0+\\int_0^\\infty e^{-\\beta x} \\text{d} x\\\\ &amp; \\;\\;\\;\\; = \\int_0^\\infty\\frac{\\text{d}}{\\text{d}x} \\frac{e^{-\\beta x}}{-\\beta} \\text{d} x\\\\ &amp; \\;\\;\\;\\; = [\\frac{e^{-\\beta x}}{-\\beta}]_0^\\infty = \\frac{1}{-\\beta}[0-1]=\\frac{1}{\\beta} \\end{aligned} \\] 請寫下本題設定條件下的數學模型，零假設和替代假設 解： 假設患者人數爲 \\(n\\)，他們的生存時間爲相互獨立的隨機變量： \\(X_1,\\cdots,X_n\\)。那麼本例中的數學模型爲：\\(\\text{Model: } X_1,\\cdots,X_n\\stackrel{i.i.d}{\\sim}f(x|\\beta)=\\beta e^{-\\beta x}\\)。我們可以提出如下的零假設和替代假設：\\(H_0: \\beta=\\beta_0 \\text{ v.s. } H_1: \\beta\\neq\\beta_0\\)。 推導此模型參數 \\(\\beta\\) 的極大似然估計 (MLE)，試使用似然比檢驗法來推導進行假設檢驗時使用的檢驗統計量。 解 \\[ \\begin{aligned} &amp; L(\\beta|\\underline{x}) = \\prod_{i=1}^n f(x_i|\\beta)=\\prod_{i=1}^n\\beta e^{-\\beta x_i} \\\\ &amp; \\ell(\\beta)=\\sum_{i=1}^n\\text{log}(\\beta e^{-\\beta x_i})=\\sum\\text{log}\\beta-\\sum\\beta x_i=n\\text{log}\\beta-\\beta\\sum x_i \\\\ &amp; \\;\\;\\;\\; = n\\text{log}\\beta-\\beta n \\bar{x} \\\\ &amp; \\Rightarrow \\ell^\\prime(\\beta)=\\frac{n}{\\beta}-n\\bar{x}\\text{ MLE solves } \\ell^\\prime(\\beta)=0 \\text{ when }\\ell^{\\prime\\prime}(\\beta) &lt; 0 \\\\ &amp; \\ell^\\prime(\\beta)=0 \\Rightarrow \\hat\\beta=\\frac{1}{\\bar{x}}, \\text{ and } \\ell^{\\prime\\prime}(\\beta)=-n\\frac{1}{\\beta^2} &lt; 0\\\\ &amp; \\Rightarrow \\text{ LRT test statistic: Under } H_0: \\beta=\\beta_0 \\Rightarrow -2llr(\\beta_0) \\sim \\mathcal{X}_1^2\\\\ &amp; llr(\\beta_0)=\\ell(\\beta_0)-\\ell(\\hat\\beta)=n\\text{log}\\beta_0-\\beta_0n\\bar{x}-n\\text{log}\\hat\\beta+\\hat\\beta n \\bar{x}\\\\ &amp; \\text{ Substituting with MLE } \\hat\\beta=\\frac{1}{\\bar{x}}\\\\ &amp; \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = n\\text{log}\\beta_0-\\beta_0n\\bar{x}+n\\text{log}\\bar{x}+ n\\\\ &amp; \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; = n(\\text{log}\\beta_0\\bar{x}-\\beta_0\\bar{x}+1) \\textbf{ this is the statistic for LRT} \\end{aligned} \\] 推導 Score 和 Wald 檢驗法時的檢驗統計量 解 \\[ \\begin{aligned} &amp; \\textbf{Score test: under } H_0 \\Rightarrow \\frac{U^2}{V}\\sim \\mathcal{X}_1^2 \\text{ where } U=\\ell^\\prime(\\beta_0), V=-E[\\ell^{\\prime\\prime}(\\beta_0)]\\\\ &amp; \\Rightarrow U=\\frac{n}{\\beta_0}-n\\bar{x}; V = -E[-n\\frac{1}{\\beta_0^2}] = n\\frac{1}{\\beta_0^2} \\\\ &amp; \\Rightarrow \\frac{U^2}{V}=(\\frac{n}{\\beta_0}-n\\bar{x})^2\\cdot\\frac{\\beta_0^2}{n} = (\\frac{(\\frac{n}{\\beta_0}-n\\bar{x})\\beta_0}{\\sqrt{n}})^2\\\\ &amp; \\;\\;\\;\\;\\;\\;\\;\\;\\; = n(1-\\bar{x}\\beta_0)^2\\\\ &amp; \\textbf{This is the statistic for Score test}\\\\ &amp; \\textbf{Wald test: under } H_0: \\beta=\\beta_0 \\Rightarrow W=(\\frac{M-\\beta_0}{S})^2 \\sim \\mathcal{X}_1^2, \\\\ &amp; \\text{ where } M=\\hat\\beta=\\frac{1}{\\bar{x}}, \\text{ and } S^2=-\\frac{1}{\\ell^{\\prime\\prime}(\\hat\\beta)}\\\\ &amp; \\ell^{\\prime\\prime}(\\beta)=-n\\frac{1}{\\beta^2}\\Rightarrow \\ell^{\\prime\\prime}(\\hat\\beta)=-n\\bar{x}^2\\Rightarrow S^2=\\frac{1}{n\\bar{x}^2}\\\\ &amp; \\Rightarrow W=(\\frac{M-\\beta_0}{S})^2=\\frac{(\\frac{1}{\\bar{x}}-\\beta_0)^2}{\\frac{1}{n\\bar{x}^2}}=n(1-\\beta_0\\bar{x})^2\\\\ &amp; \\textbf{This is the statistic for Wald test} \\end{aligned} \\] 注意到在這個特例中， Score 和 Wald 檢驗的統計量竟然不謀而合。 觀察5名患者，獲得診斷後的生存數據 (年)： \\(0.5,1,1.25,1.5,0.75\\)。用上面推導的統計量對這個數據進行假設檢驗：\\(H_0: \\beta=0.5 \\text{ v.s. } \\beta\\neq0.5\\)，你如何下結論？ 解 \\[ \\begin{aligned} &amp;\\text{Data: } x_1,\\cdots,x_n=0.5,1,1.25,1.5,0.75. \\Rightarrow \\bar{x}=1\\\\ &amp;H_0: \\beta=0.5 \\text{ v.s. } \\beta\\neq0.5\\\\ &amp;\\textbf{LRT test: } \\\\ &amp; llr(\\beta_0) = n(\\text{log}\\beta_0\\bar{x}-\\beta_0\\bar{x}+1) = 5\\times(\\text{log}0.5-0.5\\times1+1) = -0.966\\\\ &amp;\\Rightarrow -2llr=1.93 &lt; \\text{Prob}(\\mathcal{X}^2_{1,0.95}) = 3.84 \\\\ &amp; \\text{There is no evidence that } \\beta\\neq0.5.\\\\ &amp;\\textbf{Score test: } \\\\ &amp; \\frac{U^2}{V} = n(1-\\bar{x}\\beta_0)^2 = 5\\times(1-1\\times0.5)^2=1.25 &lt; \\text{Prob}(\\mathcal{X}^2_{1,0.95}) = 3.84 \\\\ &amp; \\text{There is no evidence that } \\beta\\neq0.5.\\\\ &amp;\\textbf{Wald test: } \\\\ &amp; W=n(1-\\beta_0\\bar{x})^2=5\\times(1-0.5\\times1)^2=1.25&lt; \\text{Prob}(\\mathcal{X}^2_{1,0.95}) = 3.84 \\\\ &amp; \\text{There is no evidence that } \\beta\\neq0.5.\\\\ \\end{aligned} \\] 16.7.3 Q3 隨機變量 \\(X_1,\\cdots,X_n\\) 互相獨立且在區間 \\([0,\\alpha]\\) 內服從相同的恆定概率分佈 (identical uniform distribution)。 試着畫出參數 \\(\\alpha\\) 的似然方程示意圖。不進行任何數學計算，試着想象一下如果對 \\(\\alpha\\) 進行某種假設檢驗會出現什麼問題嗎？ "],
["-normal-error.html", "第 17 章 誤差模型 Normal error", " 第 17 章 誤差模型 Normal error "],
["section-18.html", "第 18 章 探索數據和簡單描述", " 第 18 章 探索數據和簡單描述 "],
["section-19.html", "第 19 章 信賴區間", " 第 19 章 信賴區間 "],
["section-20.html", "第 20 章 假設檢驗", " 第 20 章 假設檢驗 "],
["section-21.html", "第 21 章 相關", " 第 21 章 相關 "],
["section-22.html", "第 22 章 比較", " 第 22 章 比較 "],
["section-23.html", "第 23 章 假定前提和數據轉換", " 第 23 章 假定前提和數據轉換 "],
["-simple-linear-regression.html", "第 24 章 簡單線性迴歸 Simple Linear Regression 24.1 一些背景和術語 24.2 簡單線性迴歸模型 simple linear regression model 24.3 區分因變量和預測變量 24.4 參數的估計 estimation of parameters 24.5 殘差方差的估計 Estimation of the residual variance \\((\\sigma^2)\\) 24.6 R 演示 例 1： 圖 24.1 數據 24.7 R 演示 例 2： 表24.1 數據 24.8 練習", " 第 24 章 簡單線性迴歸 Simple Linear Regression 24.1 一些背景和術語 思考下面這些問題： 脂肪攝入量增加，會導致體重增加嗎？ 兒童成年時的身高，可以用父母親的身高來預測嗎？ 如果其他條件都沒有變化，飲食習慣的改變，是否能影響血清膽固醇的水平？ 上面的問題中，自變量 (預測變量)，和因變量 (反應量) 分別是什麼？ 你可能還會碰到像下面這些稱呼，他們都是一個意思： 因變量 Dependent variable = 反應量 response variable = 結果變量 outcome variable; 自變量 independent variable = 預測變量 predictor variable = 解釋變量 explanatory variable = 共變量 covariate. 所有的非簡單統計模型 (non-trivial statistical models) 都包括以下三個部分： 隨機變量 random variables： 因變量永遠都是隨機變量； 預測變量不一定是隨機變量； 在相對簡單的模型中，我們討論的因變量和預測變量幾乎都來自於從人羣中抽取觀察樣本收集來的數據。 人羣參數 population parameters： 人羣參數，是我們希望通過收集樣本獲得的數據來估計 (estimate) 的參數。 對不確定性的描述 representation of uncertainty： 不確定性，意爲因變量的變動中，沒有被預測變量解釋的部分。 其他的術語問題： 單一因變量的統計模型：univariate model; 多個因變量的統計模型： multivariate model; 單一因變量，含有多個預測變量的統計模型：multivariable model； 在線性迴歸中，單一因變量，單一預測變量的統計模型：simple linear regression (簡單線性迴歸)； 在線性迴歸中，單一因變量，多個預測變量的統計模型：multiple linear regression (多重線性迴歸)； 儘量避免將預測變量 (predictor variable) 寫作自變量 (independent variable)，因爲 “independent” 有自己的統計學含義 (獨立)。然而我們在線性迴歸中使用的預測變量，不一定都互相獨立，所以容易讓人混淆其意義。 24.2 簡單線性迴歸模型 simple linear regression model 即：單一因變量，單一預測變量的統計模型。 24.2.1 例1 下面的散點圖 24.1 展示的是一項橫斷面調查的結果，調查的是一些兒童的年齡 (月)，和他們的體重 (千克) 之間的關係。 图 24.1: Age and weight of children in a cross-sectional survey 24.2.2 例2 表 24.1 羅列的是11名兒童能夠自己獨立行走時的年齡。這些兒童在剛出生時被隨機分配到兩個組中 (積極鍛鍊走路，和對照組)。如果你熟悉均數比較，這樣的數據可以通過簡單 \\(t\\) 檢驗來分析其均值的不同。但是實際上後面你會看到簡單 \\(t\\) 檢驗和簡單線性迴歸是同一回事。 表 24.1: Childen’s ages at time of first walking aline by randomisation group Age in months for walking alone Active Exercise (n=6) Eight Week Control (n=5) 9.00 13.25 9.50 11.5 9.75 12 10.00 13.5 13.00 11.5 9.50 – 24.3 區分因變量和預測變量 在簡單兩樣本 \\(t\\) 檢驗中，我們不區分那兩個要比較的數據 \\((X, Y)\\)。所以 \\(X\\) 和 \\(Y\\) 的關係，同分析 \\(Y\\) 和 \\(X\\) 的關係是一樣的。表 24.1 的例子中，視“直立行走的年齡”這一變量爲因變量十分直觀且自然。圖 24.1 的例子中我們顯然可以關心是否可以用兒童的年齡來推測他/她的體重。所以年齡被視爲預測變量 \\((X)\\)，體重被視爲因變量或者叫結果變量 \\((Y)\\)。 24.3.1 均值 (期待值) 公式 圖 24.1 的例子中，當我們決定考察體重變化 \\((Y)\\) 和年齡的關係 \\((X)\\) 後，我們需要提出一個模型，來描述二者之間的關係。這個模型中，最重要的信息，是均值，或者叫期待值： \\[ E(Y|X=x), \\text{ the expected value of } Y \\text{ when } X \\text{ takes the value } x \\] 在簡單線性迴歸模型中，我們認爲這個均值方程是線性關係： \\[ E(Y|X=x) = \\alpha +\\beta x \\] 所以這個線性關係中，有兩個參數 (parameters) 是我們關心的 \\(\\alpha, \\beta\\)。 \\(\\alpha\\) 是截距 intecept。意爲當 \\(X\\) 取 \\(0\\)時， \\(Y\\) 的期待值大小； \\(\\beta\\) 是方程的斜率 slope。意爲當 \\(X\\) 上升一個單位時，\\(Y\\) 上升的期待值大小。 需要強調的是，這樣的線性模型，是我們提出，用來模擬真實數據時使用的。你如果作死當然還可以提出更加複雜的模型。如下面圖 24.2 顯示的是線性迴歸直線， 而圖 24.3 顯示的是較爲複雜的迴歸曲線。曲線方程可能更加擬合我們收集到的數據，然而這樣的連續的斜率變化很可能僅僅只解釋了這個樣本量數據，而不能解釋在人羣中年齡和體重的關係。 图 24.2: Linear mean function for age and weight of children in a cross-sectional survey 图 24.3: Non-linear mean function for age and weight of children in a cross-sectional survey 24.3.2 條件分佈和方差 the conditional distribution and the variance function 如果要完全明確一個統計模型，另一個重要的事情是，該模型能否準確描述因變量在預測變量的條件分佈 it is necessary to describe the ddistribution of the dependent variable conditional on the predictor variable。使用簡單線性迴歸模型有幾個前提假設，一是因變量對預測變量的條件分佈的方差是保持不變的 the variance of the dependent variable (conditional on the predictor variable) is constant。二是，這樣的條件分佈是一個正態分佈。有時候，這些假設條件並不能得到滿足。上面的散點圖 24.1看上去還算符合這兩個假設前提：在每一個年齡階段，體重的分佈沒有發生歪斜 (skew)，分散分佈 (方差) 也相對穩定。但是圖 24.4 中的價格-克拉數據很明顯無法滿足上面的前提假設。在線性迴歸模型中，我們使用 \\(\\sigma^2\\) 表示殘差的方差 (residual variance)。 图 24.4: Relationship between diamond carat and price 24.3.3 定義簡單線性迴歸模型 用來描述一個隨機變量 \\((Y)\\) 和另一個變量 \\((X)\\) 之間關係的簡單線性迴歸模型，被定義爲： \\[ (Y|X=x) \\sim N(\\alpha+\\beta x, \\sigma^2) \\] 上面這個模型，十分清楚地描述了我們提出的模型，還有其中我們對數據的分佈的假設。同樣的模型，你可能更多得看到被寫成如下的方式： \\[ y=\\alpha+\\beta x+ \\varepsilon \\text{, where } \\varepsilon\\sim N(0,\\sigma^2) \\] 假如，我們有一組樣本數據 \\(\\underline{x}\\)， 樣本量爲 \\(n\\)。我們就可以把通過上面的迴歸模型實現的 \\(Y_i\\) 和它對應的 \\(X_i (i=1,\\cdots, n)\\)。描述爲如下的形式： \\[ \\begin{equation} (Y_i|X_i=x_i) \\sim NID(\\alpha+\\beta x, \\sigma^2) \\text{ where } i=1,\\cdots,n \\end{equation} \\tag{24.1} \\] 此處的 \\(NID\\) 意爲獨立且服從正態分佈 (normally and independently distributed)。這裏默認的一個重要前提是所有的觀察值 \\(X_i\\) 是相互獨立互不影響的。例如上面圖 24.1 所示兒童的年齡和體重數據，就必須假設這些兒童都來自沒有血緣關係的獨立家庭。如果這以數據中的兒童，有些是兄弟姐妹的話，觀察數據互相獨立的前提就無法得到滿足。具體這類數據的分析方法會在 “Analysis of hierarchical and other dependent data” 中詳盡介紹。 公式 (24.1) 常被記爲： \\[ \\begin{equation} (Y_i|X_i=x_i) = \\alpha + \\beta x_i + \\varepsilon_i, \\text{ where } \\varepsilon_i\\sim NID(0,\\sigma^2) \\end{equation} \\tag{24.2} \\] 或者爲了簡潔表述寫成： \\[ \\begin{equation} y_i = \\alpha + \\beta x_i + \\varepsilon_i, \\text{ where } \\varepsilon_i\\sim NID(0,\\sigma^2) \\end{equation} \\tag{24.3} \\] 24.3.4 殘差 residuals 公式 (24.2) 和 (24.3) 其實已經包含了殘差的表達式： \\[ \\varepsilon_i = y_i - (\\alpha + \\beta x_i) \\] 所以 \\(\\varepsilon_i\\) 的意義是第 \\(i\\) 個觀察對象的隨機(偶然)誤差 (random error)，或者叫真實殘差 (true residual)。其實就是從線性迴歸計算獲得的估計值 \\(\\alpha+\\beta x_i\\)，和觀察值 \\(y_i\\) 之間的差距大小。而且從其公式可見，殘差本身也是由人羣的參數 \\((\\alpha, \\beta)\\) 決定的。殘差也被定義爲迴歸模型的偏差值。當我們用樣本數據獲得的參數估計 \\((\\hat\\alpha, \\hat\\beta)\\) 來取代掉參數 \\((\\alpha, \\beta)\\) 時，這時的模型變成了估計模型，殘差也成了估計殘差或者叫觀察模型和觀察殘差。須和真實殘差加以區分。 24.4 參數的估計 estimation of parameters 簡單線性迴歸模型中有三個人羣參數 \\((\\alpha, \\beta, \\sigma^2)\\)。統計分析的目標，就是使用樣本數據 \\(Y_i, X_i, (i=1, \\cdots, n)\\) 來對總體參數做出推斷 (inference)。在線性迴歸中主要使用普通最小二乘法 (ordinary least squares, OLS) 作爲推斷的工具。在統計學中，我們習慣給希臘字母戴上“帽子”，作爲該參數的估計值，例如 \\(\\hat\\alpha, \\hat\\beta\\) 是參數 \\(\\alpha, \\beta\\) 的估計值。通過線性迴歸模型，給第 \\(i\\) 個觀察值擬合的預測值，被叫做因變量的估計期望值 (estimated expectation)。用下面的式子來表示: \\[ \\hat{y}_i=\\hat\\alpha+\\hat\\beta x_i \\] 此時，第 \\(i\\) 名對象的觀察殘差 (observed or fitted or estimated residuals) 用下面的式子來表示： \\[ \\hat{\\varepsilon}_i = y_i-\\hat{y}_i=y_i-(\\hat\\alpha+\\hat\\beta x_i) \\] 24.4.1 普通最小二乘法估計 \\(\\alpha, \\beta\\) 普通最小二乘法估計的 \\(\\alpha, \\beta\\) 會最小化擬合迴歸直線的偏差 minimize the sum of squared deviations from the fitted regression line。其正式的定義爲：OLS估計值，指的是能夠使殘差平方和 (residual sum of squares, \\(SS_{RES}\\))取最小值的 \\(\\hat\\alpha, \\hat\\beta\\)。 \\[ \\begin{equation} SS_{RES} = \\sum_{i=1}^n \\hat{\\varepsilon}^2_i = \\sum_{i=1}^n (y_i-\\hat\\alpha-\\hat\\beta x_i)^2 \\end{equation} \\tag{24.4} \\] 可以證明的是，OLS的 \\(\\alpha, \\beta\\) 估計值的計算公式爲： \\[ \\begin{equation} \\hat\\alpha=\\bar{y}-\\hat\\beta\\bar{x} \\tag{24.5} \\end{equation} \\] \\[ \\begin{equation} \\hat\\beta=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\tag{24.6} \\end{equation} \\] 其中 \\(\\bar{y}=\\frac{\\sum_{i=1}^ny_i}{n}, \\bar{x}=\\frac{\\sum_{i=1}^nx_i}{n}\\) 證明 求能最小化 \\(SS_{RES}\\) 的 \\(\\alpha\\)， 我們需要把公式 (24.4) 對 \\(\\hat\\alpha\\) 求導，然後將求導之後的式子等於 \\(0\\) 之後求根即可： \\[ \\begin{aligned} &amp; \\frac{\\text{d}SS_{RES}}{\\text{d}\\hat\\alpha} =\\sum_{i=1}^n -2(y_i-\\hat\\alpha-\\hat\\beta x_i) = 0\\\\ &amp; \\text{Since } \\sum_{i=1}^n(y_i) = n\\bar{y}; \\sum_{i=1}^n (x_i) =n\\bar{x} \\\\ &amp; \\Rightarrow -n\\bar{y}+n\\hat\\alpha+n\\hat\\beta\\bar{x} = 0 \\\\ &amp; \\Rightarrow \\hat\\alpha = \\bar{y}-\\hat\\beta\\bar{x} \\end{aligned} \\] 求能最小化 \\(SS_{RES}\\) 的 \\(\\beta\\)，求導之前我們先把公式 (24.4) 中含有 \\(\\hat\\alpha\\) 的部分替換掉： \\[ \\begin{equation} \\begin{split} SS_{RES} &amp;= \\sum_{i=1}^n\\hat\\varepsilon_i^2=\\sum_{i=1}^n(y_i-(\\bar{y}-\\hat\\beta\\bar{x})-\\hat\\beta x_i)^2\\\\ &amp;= \\sum_{i=1}^n((y_i-\\bar{y})-\\hat\\beta(x_i-\\bar{x}))^2 \\\\ \\end{split} \\tag{24.7} \\end{equation} \\] 接下來對上式 (24.7) 求導之後，用相同辦法求根： \\[ \\begin{aligned} &amp;\\frac{\\mathrm{d} SS_{RES}}{\\mathrm{d} \\hat\\beta} = \\sum_{i=1}^n -2(x_i-\\bar{x})(y_i-\\bar{y}) + 2\\hat\\beta(x_i-\\bar{x})^2 = 0\\\\ &amp; \\Rightarrow \\hat\\beta\\sum_{i=1}^n(x_i-\\bar{x})^2 = \\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y}) \\\\ &amp; \\hat\\beta=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2} \\end{aligned} \\] 這兩個式子 (24.5) (24.6) 同時也是參數 \\(\\alpha \\beta\\) 的極大似然估計 (MLE)。 24.5 殘差方差的估計 Estimation of the residual variance \\((\\sigma^2)\\) 殘差方差等於殘差平方和除以樣本量。所以我們會把殘差方差的估計用下面的式子表示： \\[ \\begin{equation} \\hat\\sigma^2=\\sum_{i=1}^n \\frac{\\hat\\varepsilon^2}{n} = \\sum_{i=1}^n \\frac{(y_i-\\hat\\alpha-\\hat\\beta x_i)^2}{n} \\end{equation} \\tag{24.8} \\] 這的確是 \\(\\sigma^2\\) 的極大似然估計 (MLE)。然而我們知道，公式 (24.8) 並不是殘差方差的無偏估計。類似與樣本方差低估了總體方差 (Section 10.3)，那樣，這裏殘差方差的觀察值也是低估了總體殘差方差的。所以，殘差方差的無偏估計需要用下面的式子來校正： \\[ \\begin{equation} \\hat\\sigma^2=\\sum_{i=1}^n \\frac{\\hat\\varepsilon^2}{n-2} = \\sum_{i=1}^n \\frac{(y_i-\\hat\\alpha-\\hat\\beta x_i)^2}{n-2} \\end{equation} \\tag{24.9} \\] 公式 (24.9) 被叫做殘差均方 (Residual Mean Squares, RMS)，常常被標記爲 \\(\\text{MS_{RES}}\\)。分母的 \\(n-2\\)，表示進行殘差方差估計時用掉了兩個信息量 \\(\\alpha, \\beta\\) (自由度減少了 2)， 24.6 R 演示 例 1： 圖 24.1 數據 library(haven) growgam1 &lt;- read_dta(&quot;backupfiles/growgam1.dta&quot;) slm &lt;- lm(wt~age, data=growgam1) summary(slm) # basic default output of the summary ## ## Call: ## lm(formula = wt ~ age, data = growgam1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.924 -0.785 0.007 0.797 4.068 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.8376 0.2101 32.5 &lt;2e-16 *** ## age 0.1653 0.0111 14.9 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.27 on 188 degrees of freedom ## Multiple R-squared: 0.541, Adjusted R-squared: 0.538 ## F-statistic: 221 on 1 and 188 DF, p-value: &lt;2e-16 print(anova(slm), digits = 8) # show the sum of squares for the fitted model and residuals ## Analysis of Variance Table ## ## Response: wt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 359.06320 359.06320 221.39203 &lt; 2.22e-16 ## Residuals 188 304.90655 1.62184 ## ## age *** ## Residuals ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 也可以用 stargazer 包輸出很酷的表格報告： library(stargazer) stargazer(slm, type = &quot;html&quot;) Dependent variable: wt age 0.165*** (0.011) Constant 6.838*** (0.210) Observations 190 R2 0.541 Adjusted R2 0.538 Residual Std. Error 1.274 (df = 188) F Statistic 221.400*** (df = 1; 188) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 其實結果都一樣。我們這裏詳細來看 \\(\\alpha, \\beta, \\sigma^2\\)： \\(\\hat\\alpha = 6.84\\)：當年齡爲 \\(0\\) 時，體重爲 \\(6.84 kg\\)。本數據 24.1 中並沒有 \\(0\\) 歲的兒童，所以這裏的截距的解釋需要非常小心是否合理。 \\(\\hat\\beta = 0.165\\)：這數據中兒童的體重估計隨着年齡升高 \\(1\\) 個月增長 \\(0.165 kg\\)。所以使用這兩個估計值我們就可以來估計任意年齡時兒童的體重。圖 24.2 就是擬合數據以後的簡單線性迴歸曲線。 \\(\\hat\\sigma^2 = 1.62, \\hat\\sigma=1.27\\) 就是默認輸出中最下面的 Residual standard error: 1.274 和 ANOVA 表格中 Residuals 的 Mean Sq=1.62184 部分。含義是，沿着擬合的直線，在每一個給定的年齡上兒童體重的分佈的標準差是 \\(1.27 kg\\)。 24.7 R 演示 例 2： 表24.1 數據 如果在 Stata 聽說你還需要自己生成啞變量 (dummy variables) (應該是計算時，在想要變成啞變量的變量名前面加上 i.)。在 R 裏面，分類變量被設置成因子 “factor” 時，你就完全可以忽略生成啞變量的過程。下圖 24.5 顯示了兩組兒童直立行走時的年齡。 图 24.5: Age at walking by group 擬合簡單線性迴歸也是小菜一碟： wk_age &lt;- lm(Age ~ Group, data=Walk) summary(wk_age) ## ## Call: ## lm(formula = Age ~ Group, data = Walk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.125 -0.738 -0.375 0.388 2.875 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.125 0.512 19.77 1e-08 *** ## Groupcontrol 2.225 0.760 2.93 0.017 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.25 on 9 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.488, Adjusted R-squared: 0.431 ## F-statistic: 8.58 on 1 and 9 DF, p-value: 0.0168 anova(wk_age) ## Analysis of Variance Table ## ## Response: Age ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Group 1 13.5 13.50 8.58 0.017 * ## Residuals 9 14.2 1.57 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 這裏的 \\(\\hat\\alpha=10.125\\)，意爲參照組 (此處，“exercise” 被默認設定爲參照組，而 “control” 被默認拿來和參照組相比較) 的兒童也就是，積極練習走路的小朋友這組能夠獨立行走的平均年齡是 \\(10.125\\) 個月。 \\(\\hat\\beta=2.225\\)，意爲和參照組 (積極練習組) 相比，對照組兒童能夠自己行走的年齡平均要晚 \\(2.225\\) 個月。所以對照組兒童能夠直立行走的平均年齡就是 \\(10.125+2.225=12.35\\) 個月。 上述結果，你如果拿來和下面的兩樣本 \\(t\\) 檢驗的結果相比就知道，是完全一致的。其中統計量 \\(t^2=2.9285^2=F_{1,9}=8.58\\)。 t.test(Age~Group, data=Walk, var.equal=TRUE) ## ## Two Sample t-test ## ## data: Age by Group ## t = -2.9, df = 9, p-value = 0.02 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -3.9437 -0.5063 ## sample estimates: ## mean in group exercise mean in group control ## 10.12 12.35 24.8 練習 使用的數據內容爲：兩次調查同一樣本，99 名健康男性的血清膽固醇水平，間隔一年。 # 數據讀入 library(haven) Chol &lt;- read_dta(&quot;backupfiles/chol.dta&quot;) summary(Chol) ## id chol1 chol2 ## Min. : 1.0 Min. :152 Min. :170 ## 1st Qu.:25.5 1st Qu.:235 1st Qu.:240 ## Median :50.0 Median :265 Median :260 ## Mean :50.0 Mean :265 Mean :264 ## 3rd Qu.:74.5 3rd Qu.:290 3rd Qu.:290 ## Max. :99.0 Max. :360 Max. :355 # Alternative Descriptive Statistics using psych package library(psych) describe(Chol) ## vars n mean sd median trimmed mad min max ## id 1 99 50.0 28.72 50 50.0 37.06 1 99 ## chol1 2 99 264.6 40.76 265 264.5 40.03 152 360 ## chol2 3 99 263.5 38.17 260 262.8 37.06 170 355 ## range skew kurtosis se ## id 98 0.00 -1.24 2.89 ## chol1 208 -0.09 -0.13 4.10 ## chol2 185 0.17 -0.27 3.84 # 兩次膽固醇水平的直方圖 Distribution of the two measures par(mfrow=c(1,2)) hist(Chol$chol1) hist(Chol$chol2) # 對兩次膽固醇水平作散點圖 ggplot(Chol, aes(x=chol1, y=chol2)) + geom_point(shape=20) + scale_x_continuous(breaks=seq(150, 400, 50),limits = c(150, 355))+ scale_y_continuous(breaks=seq(150, 400, 50),limits = c(150, 355)) + theme_stata() +labs(x = &quot;Cholesterol at visit 1 (mg/100ml)&quot;, y = &quot;Cholesterol at visit 2 (mg/100ml)&quot;) 24.8.1 兩次測量的膽固醇水平分別用 \\(C_1, C_2\\) 來標記的話，考慮這樣的簡單線性迴歸模型：\\(C_2=\\alpha+\\beta C_2 + \\varepsilon\\)。我們進行這樣迴歸的前提假設有哪些？ 每個觀察對象互相獨立。 前後兩次測量的膽固醇水平呈線性相關。 殘差值，在每一個給定的 \\(C_1\\) 值處呈現正態分佈，且方差不變。 從散點圖來看這些假設應該都能得到滿足。 # 計算兩次膽固醇水平的 均值，方差，以及二者的協方差 mean(Chol$chol1); mean(Chol$chol2) ## [1] 264.6 ## [1] 263.5 var(Chol$chol1); var(Chol$chol2) ## [1] 1661 ## [1] 1457 cov(Chol$chol1, Chol$chol2) ## [1] 961.2 24.8.2 計算普通最小二乘法 (OLS) 下，截距和斜率的估計值 \\(\\hat\\alpha, \\hat\\beta\\) \\[ \\begin{aligned} \\hat\\beta &amp;= \\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}\\\\ &amp;=\\frac{\\text{Cov}(C_1,C_2)}{\\text{Var}(C_1)}\\\\ &amp;=\\frac{1661.061}{961.224}=0.578 \\end{aligned} \\] cov(Chol$chol1, Chol$chol2)/var(Chol$chol1) ## [1] 0.5787 \\[\\hat\\alpha=\\bar{y}-\\hat\\beta\\bar{x}=263.54-0.578\\times264.59=110.425\\] mean(Chol$chol2)-mean(Chol$chol1)*cov(Chol$chol1, Chol$chol2)/var(Chol$chol1) ## [1] 110.4 24.8.3 和迴歸模型計算的結果作比較，解釋這些估計值的含義 summary(lm(chol2~chol1, data=Chol)) ## ## Call: ## lm(formula = chol2 ~ chol1, data = Chol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.88 -22.06 1.85 16.63 84.12 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 110.4247 20.0113 5.52 2.8e-07 *** ## chol1 0.5787 0.0748 7.74 9.5e-12 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30.2 on 97 degrees of freedom ## Multiple R-squared: 0.382, Adjusted R-squared: 0.375 ## F-statistic: 59.9 on 1 and 97 DF, p-value: 9.51e-12 截距的估計值是 110.4 mg/100ml: 意爲這組樣本，第一次採集數據時，膽固醇水平的平均值是 110.4。 斜率的估計值是 0.58：意爲第一次採集的膽固醇水平每高 1 mg/100ml，那麼第二次採集的膽固醇相應提高的值的期待量爲 0.58. 24.8.4 加上計算的估計值直線 (即迴歸直線) ggplot(Chol, aes(x=chol1, y=chol2)) + geom_point(shape=20, colour=&quot;grey40&quot;) + stat_smooth(method = lm, se=FALSE, size=0.5) + scale_x_continuous(breaks=seq(150, 400, 50),limits = c(150, 355))+ scale_y_continuous(breaks=seq(150, 400, 50),limits = c(150, 355)) + theme_stata() +labs(x = &quot;Cholesterol at visit 1 (mg/100ml)&quot;, y = &quot;Cholesterol at visit 2 (mg/100ml)&quot;) 可以注意到，第一次訪問時膽固醇水平高的人，第二次被測量時膽固醇值高於平均值，但是卻沒有第一次高出平均值的部分多。 相似的，第一次膽固醇水平低的人，第二次膽固醇水平低於平均值，但是卻沒有第一次低於平均值的部分多。這一現象被叫做 “向均數迴歸-regression to the mean” 24.8.5 下面的代碼用於模型的假設診斷 M &lt;- lm(chol2~chol1, data=Chol) par(mfrow = c(2, 2)) # Split the plotting panel into a 2 x 2 grid plot(M) 好心人在 github 上共享了 Check_assumption.R 的代碼，可以使用 ggplot2 來獲取高逼格的模型診斷圖： source(&quot;checkassumptions.R&quot;) check_assumptions(M) "],
["-ordinary-least-squares-estimators-and-inference.html", "第 25 章 最小二乘估計的性質和推斷 Ordinary Least Squares Estimators and Inference 25.1 OLS 估計量的性質 25.2 \\(\\hat\\beta\\) 的性質 25.3 截距和迴歸係數的方差，協方差 25.4 \\(\\alpha, \\beta\\) 的推斷 25.5 線性迴歸模型和 Pearson 相關係數 25.6 Pearson 相關係數和模型迴歸係數的檢驗統計量 \\(t\\) 之間的關係 25.7 練習", " 第 25 章 最小二乘估計的性質和推斷 Ordinary Least Squares Estimators and Inference 前一章介紹了簡單線性迴歸模型中對總體參數 \\(\\alpha, \\beta, \\sigma^2\\) 的估計公式，分別是 (24.5) (24.6) (24.9)。本章繼續介紹他們的統計學性質。下面的標記和統計量也會被用到： \\(\\bar{y}=\\frac{\\sum_{i=1}^n y_i}{n}\\)，因變量 \\(y\\) 的樣本均值； \\(\\bar{x}=\\frac{\\sum_{i=1}^n x_i}{n}\\)，預測變量 \\(x\\) 的樣本均值； \\(SS_{yy}=\\sum_{i=1}^n(y_i-\\bar{y})^2\\)，因變量 \\(y\\) 的校正平方和； \\(SS_{xx}=\\sum_{i=1}^n(x_i-\\bar{x})^2\\)，預測變量 \\(x\\) 的校正平方和； \\(SD_y^2=\\frac{\\sum_{i=1}(y_i-\\bar{y})^2}{n-1}=\\frac{SS_{yy}}{n-1}\\)，因變量 \\(y\\) 的樣本方差； \\(SD_x^2=\\frac{\\sum_{i=1}(x_i-\\bar{x})^2}{n-1}=\\frac{SS_{xx}}{n-1}\\)，因變量 \\(y\\) 的樣本方差； \\(S_{xy}=\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\\)，\\(x,y\\) 的交叉乘積； \\(CV_{xy}=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{n-1}=\\frac{S_{xy}}{n-1}\\)，樣本協方差； \\(r_{xy}=\\frac{CV_{xy}}{SD_xSD_y}\\)，\\(x,y\\) 的樣本相關係數； \\(SS_{RES}=\\sum_{i=1}^n\\hat\\varepsilon^2=\\sum_{i=1}^n(y_i-\\hat\\alpha-\\hat\\beta x_i)^2\\)，殘差的估計平方和。 25.1 OLS 估計量的性質 樣本估計的迴歸直線必定穿過數據的中心 \\((\\bar{x},\\bar{y})\\)。 證明 由於樣本估計的截距和斜率公式 (24.5) (24.6) 可知： \\[ \\begin{aligned} \\hat\\alpha &amp;= \\bar{y} - \\hat\\beta\\bar{x} \\\\ \\hat y_i &amp;= \\hat\\alpha + \\hat\\beta x_i \\\\ \\Rightarrow \\hat y_i &amp;= \\bar{y}+\\hat\\beta(x_i-\\bar{x}) \\end{aligned} \\tag{25.1} \\] 所以，當 \\(\\hat x_i=\\bar{x}\\) 時 \\(\\hat y_i=\\bar{y}\\)。即迴歸直線必然穿過中心點。 如果擬合模型是正確無誤的， \\(\\hat\\alpha,\\hat\\beta,\\hat\\sigma^2\\) 分別是各自的無偏估計。 \\(\\hat\\alpha, \\hat\\beta\\) 是極大似然估計， \\(\\hat\\sigma^2\\) 不是MLE。 \\(\\hat\\alpha, \\hat\\beta\\) 是 \\(\\alpha, \\beta\\) 最有效的估計量。 25.2 \\(\\hat\\beta\\) 的性質 \\[ \\begin{equation} \\hat\\beta=\\frac{S_{xy}}{SS_{xx}}=\\frac{CV_{xy}}{SD_x^2} \\end{equation} \\tag{25.2} \\] 25.2.1 \\(Y\\) 對 \\(X\\) 迴歸， 和 \\(X\\) 對 \\(Y\\) 迴歸 如果我們使用 \\(\\hat\\beta_{y|x}\\) 表示預測變量 \\(x\\)，因變量 \\(y\\) 的簡單線性迴歸係數，那麼我們就有： \\[ \\begin{equation} \\hat\\beta_{y|x} = \\frac{CV_{xy}}{SD_x^2} \\text{ and } \\hat\\beta_{x|y} = \\frac{CV_{xy}}{SD_y^2} \\\\ \\text{Hence, } \\hat\\beta_{y|x}\\hat\\beta_{x|y} = r^2_{xy} \\end{equation} \\tag{25.3} \\] 公式 (25.3) 也證明了：如果兩個變量相關係數爲 \\(1\\) (100% 相關)， \\(Y\\) 對 \\(X\\) 迴歸的迴歸係數，是 \\(X\\) 對 \\(Y\\) 迴歸的迴歸係數的倒數。 25.2.2 例 1： 還是圖 24.1 數據 library(haven) growgam1 &lt;- read_dta(&quot;backupfiles/growgam1.dta&quot;) # regress wt on age summary(lm(wt~age, data=growgam1)) ## ## Call: ## lm(formula = wt ~ age, data = growgam1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.924 -0.785 0.007 0.797 4.068 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.8376 0.2101 32.5 &lt;2e-16 *** ## age 0.1653 0.0111 14.9 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.27 on 188 degrees of freedom ## Multiple R-squared: 0.541, Adjusted R-squared: 0.538 ## F-statistic: 221 on 1 and 188 DF, p-value: &lt;2e-16 print(anova(lm(wt~age, data=growgam1)), digits = 8) ## Analysis of Variance Table ## ## Response: wt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 359.06320 359.06320 221.39203 &lt; 2.22e-16 ## Residuals 188 304.90655 1.62184 ## ## age *** ## Residuals ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # regress age on wt summary(lm(age~wt, data=growgam1)) ## ## Call: ## lm(formula = age ~ wt, data = growgam1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -16.010 -4.239 0.083 3.130 21.111 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.57 2.16 -6.75 1.8e-10 *** ## wt 3.27 0.22 14.88 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.66 on 188 degrees of freedom ## Multiple R-squared: 0.541, Adjusted R-squared: 0.538 ## F-statistic: 221 on 1 and 188 DF, p-value: &lt;2e-16 print(anova(lm(age~wt, data=growgam1)), digits = 8) ## Analysis of Variance Table ## ## Response: age ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wt 1 7103.6730 7103.6730 221.39203 &lt; 2.22e-16 ## Residuals 188 6032.2428 32.0864 ## ## wt *** ## Residuals ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 可以看到二者的輸出結果中統計檢驗量一樣，但是一個是將體重針對年齡迴歸，另一個則是反過來，所以迴歸係數和截距都不同。迴歸方程的含義也就發生了變化。如果把兩條迴歸曲線同時作圖可以更加直觀： 图 25.1: Simple linear regression model line relating weight to age 图 25.2: Simple linear regression model line relating age to weight 25.3 截距和迴歸係數的方差，協方差 假如簡單線性迴歸模型是正確的，那麼截距 \\(\\hat\\alpha\\) 和迴歸係數 \\(\\hat\\beta\\) 的方差分別是： \\[ \\begin{equation} V(\\hat\\alpha) = \\sigma^2(\\frac{1}{n}+\\frac{\\bar{x}^2}{SS_{xx}}) = \\frac{\\sigma^2}{(n-1)} (1-\\frac{1}{n}+\\frac{\\bar{x}^2}{SD_x^2}) \\end{equation} \\tag{25.4} \\] \\[ \\begin{equation} V(\\hat\\beta) = \\frac{\\sigma^2}{SS_{xx}}=\\frac{\\sigma^2}{(n-1)SD_x^2} \\end{equation} \\tag{25.5} \\] 從公式 (25.4) 和 (25.5) 也可以看出，兩個估計量的方差隨着殘差方差的增加而增加 (估計不精確)，隨着樣本兩的增加而減少 (估計更精確)。截距 \\(\\hat\\alpha\\) 的方差會隨着樣本均值的增加而增加。 通常來說，截距和迴歸係數二者之間並非相互獨立。他們的協方差爲： \\[ \\begin{equation} Cov(\\hat\\alpha,\\hat\\beta) = -\\frac{\\sigma^2\\bar{x}}{SS_{xx}} \\end{equation} \\tag{25.6} \\] 上面的公式 (25.4) (25.5) (25.6) 都包含了真實的殘差方差 \\(\\sigma^2\\)。這個量對於我們“人類”來說是未知的。 25.3.1 中心化 centring 簡單線性迴歸模型常用的一個技巧是將預測變量中心化。即，求預測變量的均值，然後將每個觀測值減去均值之後再用這個新的預測變量擬合簡單線性迴歸模型。這樣做其實完全不影響回顧係數，卻會影響截距的大小。此時新的迴歸直線的截距，就等於因變量 (體重) 的均值。 用圖 24.1 數據來解釋： # mean value of age mean(growgam1$age) ## [1] 16.98 growgam1$age_cen &lt;- growgam1$age-mean(growgam1$age) # regress wt on age print(summary(lm(wt~age, data=growgam1)), digit=5) ## ## Call: ## lm(formula = wt ~ age, data = growgam1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.92418 -0.78489 0.00710 0.79747 4.06781 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.837584 0.210070 32.549 &lt; 2.2e-16 *** ## age 0.165331 0.011112 14.879 &lt; 2.2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.274 on 188 degrees of freedom ## Multiple R-squared: 0.54078, Adjusted R-squared: 0.53834 ## F-statistic: 221.39 on 1 and 188 DF, p-value: &lt; 2.22e-16 print(summary(lm(wt~age_cen, data=growgam1)), digit=5) ## ## Call: ## lm(formula = wt ~ age_cen, data = growgam1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.92418 -0.78489 0.00710 0.79747 4.06781 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.644737 0.092391 104.391 &lt; 2.2e-16 *** ## age_cen 0.165331 0.011112 14.879 &lt; 2.2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.274 on 188 degrees of freedom ## Multiple R-squared: 0.54078, Adjusted R-squared: 0.53834 ## F-statistic: 221.39 on 1 and 188 DF, p-value: &lt; 2.22e-16 很明顯，結果顯示中心化不會改變迴歸係數，也不會改變它的方差。但是“新”的截距，其實就等於因變量 (體重) 的均值。而且很多數據都集中在這個均值附近，因而，截距的方差比沒有中心化的迴歸方程要小。 25.4 \\(\\alpha, \\beta\\) 的推斷 \\(\\hat\\alpha, \\hat\\beta\\) 都可以被改寫成關於因變量 \\(Y\\) 的方程，因此同時也是隨機誤差的方程式： \\[ \\begin{aligned} \\hat\\beta &amp;= \\sum_{i=1}^n[\\frac{(x_i-\\bar{x})}{SS_{xx}}(y_i-\\bar{y})] \\\\ \\text{Substituting } &amp;(y_i-\\bar{y}) = \\beta(x_i-\\bar{x})+(\\varepsilon_i-\\bar{\\varepsilon}) \\\\ &amp;= \\beta + \\sum_{i=1}^n[\\frac{x_i-\\bar{x}}{SS_{xx}}(\\varepsilon_i-\\bar{\\varepsilon})] \\end{aligned} \\] 又因爲，\\(\\varepsilon_i \\sim NID(0,\\sigma^2)\\)，估計量 \\(\\hat\\alpha, \\hat\\beta\\) 均爲 \\(\\varepsilon_i\\) 的線性轉換，所以他們也都是服從正態分佈的。 25.4.1 對迴歸係數進行假設檢驗 對於迴歸係數 \\(\\beta\\)，我們可以使用 Wald statistic (Section 16.4) 進行零假設爲 \\(\\text{H}_0: \\beta=0\\) 的假設檢驗。此時，替代假設爲 \\(\\text{H}_1: \\beta\\neq0\\)。最佳檢驗統計量爲： \\[ \\begin{equation} t = \\frac{\\hat\\beta-0}{SE(\\hat\\beta)} \\\\ \\end{equation} \\tag{25.7} \\] 根據公式 (25.5) \\(SE(\\hat\\beta) = \\sqrt{V(\\hat\\beta)} = \\frac{\\hat\\sigma}{\\sqrt{SS_{xx}}}\\)。用 \\(\\hat\\sigma^2\\) 替換掉公式 (25.5) 中的 \\(\\sigma^2\\)，意味着迴歸係數的檢驗統計量 \\(t\\) 服從自由度爲 \\(n-2\\) 的 \\(t\\) 分佈。之後就可以根據 \\(t\\) 分佈的性質求相應的 \\(p\\) 值了，對相關係數是否爲 \\(0\\) 進行檢驗。之所以我們可以在這裏使用 Wald 檢驗，是因爲前提條件：隨機誤差服從正態分佈，於是 \\(\\beta\\) 的對數似然比也是左右對稱的，當對數似然比的圖形左右對稱時，就可以使用二次方程來近似 (Wald 檢驗的實質)。 25.4.2 迴歸係數，截距的信賴區間 估計量 \\(\\beta\\) 的 \\(95\\%\\) 信賴區間的計算公式如下： \\[ \\begin{equation} \\hat\\beta \\pm t_{n-2,0.975}SE(\\hat\\beta) \\end{equation} \\tag{25.8} \\] 其中，\\(t_{n-2, 0.975}\\) 表示自由度爲 \\(n-2\\) 的 \\(t\\) 分佈的 \\(97.5\\%\\) 位點的值。繼續使用之前的實例，圖 24.1 中的數據。體重對年齡進行簡單線性迴歸之後，年齡的估計回顧係數 \\(\\hat\\beta=0.165, SE(\\hat\\beta)=0.0111\\), 此例中 \\(n=190\\)，所以 \\(t_{188, 0.975}=1.973\\)。所以迴歸係數的 \\(95\\%\\) 信賴區間可以如此計算：\\(0.165\\pm1.973\\times0.0111=(0.143, 0.187)\\)。 類似的，估計截距 \\(\\hat\\alpha\\) 的 \\(95\\%\\) 信賴區間的計算式便是： \\(\\hat\\alpha \\pm t_{n-2, 0.975}SE(\\hat\\alpha)\\)。同樣的例子裏，\\(\\hat\\alpha=6.838, SE(\\hat\\beta) = 0.210, t_{188, 0.975}=1.973\\)。所以截距的 \\(95\\%\\) 信賴區間的計算方法就是： \\(6.838\\pm1.973\\times0.210=(6.42, 7.25)\\) 跟下面 R 計算的完全一樣： confint(lm(wt~age, data=growgam1)) ## 2.5 % 97.5 % ## (Intercept) 6.4232 7.2520 ## age 0.1434 0.1873 25.4.3 預測值的信賴區間 (置信带) - 测量回归曲线本身的不确定性 這裏所謂的“預測值”其實並沒有拿來預測什麼新的數值，而是說我們希望通過線性迴歸找到因變量真實值的存在區間 (信賴區間)。所以這個預測值的真實含義其實應該是在預測變量取 \\(X=x\\) 時，因變量的期待值，\\(E(Y|X=x)\\)。 這個預測值的方差公式如下： \\[ \\begin{equation} V(\\hat y_{x}) = \\sigma^2[\\frac{1}{n}+\\frac{(x_i-\\bar{x})^2}{SS_{xx}}] \\end{equation} \\tag{25.9} \\] 於是可以計算它的 \\(95\\%\\) 信賴區間公式是： \\[ \\begin{equation} \\hat y_x \\pm t_{n-2, 0.975} \\hat\\sigma \\sqrt{[\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{SS_{xx}}]} \\end{equation} \\tag{25.10} \\] 其實在之前的圖 (圖 24.2) 我們也已經展示過這個信賴區間的範圍。 25.4.4 预测带 Reference range - 包含了 95% 观察值的区间 此處的 \\(95\\%\\) 預測帶，其實是包含了 \\(95\\%\\) 觀察數據的區間。所以預測帶要比置信帶更寬。它的方差計算公式爲： \\[ \\begin{equation} V(\\hat y_x)+\\sigma^2 = \\sigma^2[1+\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{SS_{xx}}] \\end{equation} \\tag{25.11} \\] 區間計算公式爲： \\[ \\begin{equation} \\hat{y}_x \\pm t_{n-2, 0.975} \\sqrt{1+\\frac{1}{n}+\\frac{(x-\\bar{x})^2}{SS_{xx}}} \\end{equation} \\tag{25.12} \\] 將置信帶和預測帶同時展現則如下圖： library(haven) library(ggplot2) library(ggthemes) growgam1 &lt;- read_dta(&quot;backupfiles/growgam1.dta&quot;) Model &lt;- lm(wt~age, data=growgam1) temp_var &lt;- predict(Model, interval=&quot;prediction&quot;) new_df &lt;- cbind(growgam1, temp_var) ggplot(new_df, aes(x=age, y=wt)) + geom_point(shape=20, colour=&quot;grey40&quot;) + stat_smooth(method = lm, se=FALSE, size = 0.3) + geom_line(aes(y=lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;)+ geom_line(aes(y=upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;)+ scale_x_continuous(breaks=seq(0, 38, 4),limits = c(0,36.5))+ scale_y_continuous(breaks = seq(0, 20, 5),limits = c(0,20.5)) + theme_stata() +labs(x = &quot;Age (Months)&quot;, y = &quot;Weight (kg)&quot;) 图 25.3: Simple linear regression for age and weight of children in a cross-sectional survey with 95% CI of predicted values and 95% reference range 25.5 線性迴歸模型和 Pearson 相關係數 前面也推導過線性迴歸係數和 Pearson 相關係數之間的關係 (Section 25.2.1)，這裏詳細再展開討論它們之間關係的另外兩個重要結論。 25.5.1 \\(r^2\\) 可以理解爲因變量平方和被模型解釋的比例 Pearson 相關係數，因變量的平方和，模型的殘差平方和之間有如下的關係： \\[ \\begin{equation} r^2 = \\frac{SS_{yy}-SS_{RES}}{SS_{yy}} = 1-\\frac{SS_{RES}}{SS_{yy}} \\end{equation} \\tag{25.13} \\] 證明 \\[ \\frac{SS_{RES}}{SS_{yy}} = \\frac{\\sum_{i=1}^n(y_i-\\hat\\alpha-\\hat\\beta x_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2} \\] 因爲 (24.5) : \\(\\hat\\alpha=\\bar{y}-\\hat{\\beta}\\bar{x}\\) \\[ \\begin{aligned} \\frac{SS_{RES}}{SS_{yy}} &amp;= \\frac{\\sum_{i=1}^n[(y_i-\\bar{y})-\\hat\\beta(x_i-\\bar{x})]^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2} \\\\ &amp;=\\frac{\\sum_{i=1}^n(y_i-\\bar{y})^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}-\\frac{2\\hat\\beta\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(y_i-\\bar{y})^2}+\\frac{\\hat\\beta^2\\sum_{i=1}^n(x_i-\\bar{x})^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\\\\ &amp;=1-\\frac{2\\hat\\beta SS_{xy}}{SS_{yy}} + \\frac{\\hat\\beta^2SS_{xx}}{SS_{yy}} \\end{aligned} \\] 又因爲 \\(\\hat\\beta=\\frac{\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n(x_i-\\bar{x})^2}=\\frac{S_{xy}}{SS_{xx}}, r^2=\\frac{S_{xy}^2}{SS_{xx}SS_{yy}}\\)。 \\[ \\begin{aligned} \\frac{SS_{RES}}{SS_{yy}} &amp;= 1-\\frac{2S_{xy}^2}{SS_{yy}SS_{xx}}+\\frac{S_{xy}^2}{SS_{xx}SS_{yy}}\\\\ &amp;=1-2r^2+r^2\\\\ &amp;=1-r^2\\\\ \\Rightarrow r^2&amp;=1-\\frac{SS_{RES}}{SS_{yy}} \\end{aligned} \\] 因此，這裏就引出了非常重要的一個結論，Pearson 相關係數的平方 \\(r^2\\) 的統計學含義是，因變量的平方和 \\(SS_{yy}\\) 中，模型的預測變量能夠解釋的部分 \\(1-SS_{RES}\\) 的百分比。 統計學結果的報告中，爲了和一般相關係數的意義區分，會用大寫的 \\(R^2\\) 來表示這個模型解釋了因變量的百分比。(Section 26.2.3) 25.6 Pearson 相關係數和模型迴歸係數的檢驗統計量 \\(t\\) 之間的關係 \\[ \\begin{equation} t=r\\sqrt{\\frac{n-2}{1-r^2}} \\end{equation} \\tag{25.14} \\] 證明 由於前面推導的 \\(r^2\\) 公式 (25.13)，而且 \\(r^2=\\frac{S_{xy}^2}{SS_{xx}SS_{yy}}\\)： \\[ \\begin{aligned} \\frac{r^2}{1-r^2} &amp; = \\frac{\\frac{S_{xy}^2}{SS_{xx}SS_{yy}}}{\\frac{SS_{RES}}{SS_{yy}}} \\\\ &amp; = \\frac{S_{xy}^2}{SS_{xx}SS_{RES}} \\\\ &amp; = \\frac{S_{xy}^2}{SS_{xx}(n-2)\\hat\\sigma^2} \\end{aligned} \\] 由於公式 (25.5)，所以 \\(\\hat\\sigma^2=V(\\hat\\beta)SS_{xx}\\) \\[ \\begin{aligned} \\frac{r^2}{1-r^2} &amp; = \\frac{S_{xy}^2}{SS^2_{xx}(n-2)V(\\hat\\beta)} \\\\ &amp; = \\frac{\\hat\\beta^2}{(n-2)V(\\hat\\beta)} \\\\ \\Rightarrow t=r\\sqrt{\\frac{n-2}{1-r^2}} \\end{aligned} \\] 這個結論也被用於相關係數的假設檢驗。而且也正如 Section 25.2.1 證明過的那樣，在簡單線性迴歸裏因變量和預測變量的位置對調以後，對於回顧係數是否爲零的檢驗統計量不受影響。 25.7 練習 數據同前一章練習部分數據相同 24.8： # 數據讀入 library(haven) library(ggplot2) library(ggthemes) Chol &lt;- read_dta(&quot;backupfiles/chol.dta&quot;) Model &lt;- lm(chol2~chol1, data=Chol) print(summary(Model), digit=6) ## ## Call: ## lm(formula = chol2 ~ chol1, data = Chol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -56.87654 -22.06181 1.84937 16.63107 84.11839 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 110.4246582 20.0113279 5.51811 2.8499e-07 ## chol1 0.5786806 0.0747598 7.74053 9.5114e-12 ## ## (Intercept) *** ## chol1 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 30.16 on 97 degrees of freedom ## Multiple R-squared: 0.381834, Adjusted R-squared: 0.375462 ## F-statistic: 59.9159 on 1 and 97 DF, p-value: 9.51139e-12 print(anova(Model), digit=6) ## Analysis of Variance Table ## ## Response: chol2 ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## chol1 1 54511.7 54511.7 59.9159 9.5114e-12 *** ## Residuals 97 88250.9 909.8 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 計算截距和迴歸係數的 P 值 HAND CALCULATIONS twosided p-value in R can be obtained by pt(t, df) function ## p value for intercept: 110.42466/20.01133 #=5.518107 ## [1] 5.518 2*pt(5.518107, 97, lower.tail = FALSE) ## [1] 2.85e-07 ## p value for beta: 0.57868/0.07476 #= 7.740503 ## [1] 7.741 2*pt(7.740503, 97, lower.tail = FALSE) ## [1] 9.513e-12 # add fitted regression lines 95% CIs and reference range temp_var &lt;- predict(Model, interval=&quot;prediction&quot;) new_df &lt;- cbind(Chol, temp_var) ggplot(new_df, aes(x=chol1, y=chol2)) + geom_point(shape=20, colour=&quot;grey40&quot;) + stat_smooth(method = lm, se=TRUE, size=0.5) + geom_line(aes(y=lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;)+ geom_line(aes(y=upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;)+ scale_x_continuous(breaks=seq(150, 400, 50),limits = c(150, 355))+ scale_y_continuous(breaks=seq(150, 400, 50),limits = c(150, 355)) + theme_stata() +labs(x = &quot;Cholesterol at visit 1 (mg/100ml)&quot;, y = &quot;Cholesterol at visit 2 (mg/100ml)&quot;) 圖中可見，95% 置信帶變化顯著，距離均值越遠的地方，置信帶越寬。然而預測帶基本是平行的沒有變化。因爲預測帶的涵義是，95%的觀察數據都在這個區間範圍內。 "],
["-introduction-to-analysis-of-variance.html", "第 26 章 方差分析 Introduction to Analysis of Variance 26.1 背景 26.2 簡單線性迴歸模型的方差分析 26.3 分類變量用作預測變量時的 ANOVA", " 第 26 章 方差分析 Introduction to Analysis of Variance 26.1 背景 當我們用統計模型模擬真實數據的時候，我們常常會被問到這樣的問題：“兩個模型哪個能更好的擬合這個數據？” 本章我們先考慮簡單的情況，兩個模型互相比較時，其中一個稍微簡單些的模型使用的預測變量，同時也是另一個較複雜的模型的預測變量 (nested models)。所以，複雜模型的預測變量較多，而其中一個或者幾個預測變量又構成了新的較爲簡單的模型。這兩個模型之間的比較，就需要用到方差分析 Analysis of Variance (ANOVA)。 此處方差分析的原則是：如果複雜模型能夠更好的擬合真實實驗數據，那我們會認爲簡單模型無法解釋的大量殘差平方和，有效地被複雜模型解釋了。所以，這一原則下，可以推理，複雜模型計算獲得的殘差平方和，會顯著地小於簡單模型計算獲得的殘差平方和。ANOVA 就提供了這個殘差平方和變化的定量比較方法。 26.2 簡單線性迴歸模型的方差分析 其實從線性迴歸的第一章節開始，我們都在使用方差分析的思想。圖 24.1 數據的迴歸模型中，我們其實比較了以下兩個模型： 零假設模型：null model, 即認爲年齡和體重之間沒有任何關係 (水平直線)； 替代模型： alternative model, 認爲年齡和體重之間有一定的線性關係 (擬合後的直線)。 图 26.1: NULL (red) and Alternative models (blue) for the data 26.2.1 兩個模型的參數估計 無論是零假設模型，還是替代假設模型，都需要通過最小化殘差來獲得其參數估計： \\[ SS_{RES} = \\sum_{i=1}^n \\hat\\varepsilon^2= \\sum_{i=1}^n(y_i-\\hat y_i)^2 \\] 替代假設模型，在線性迴歸第一部分 (Section 24.3.1) 已經提到過，均值方程是 \\(E(Y|X=x) = \\alpha+\\beta x\\)，且這個方程的參數 \\(\\alpha, \\beta\\) 以及殘差方差 \\(\\sigma^2\\) 的估計值計算公式也已經推導完成 (24.5) (24.6) (24.9)。 零假設模型，它的均值方程是 \\(E(Y|X=x)=\\alpha\\)。所以需要將它的殘差最小化： \\[ SS_{RES} = \\sum_{i=1}^n(y_i-\\hat\\alpha)^2 \\] 由於 (24.5) ：\\(\\hat\\alpha=\\bar{y}-\\hat\\beta\\)，所以 \\(\\hat\\alpha = \\bar{y}\\)。 所以對於零假設模型來說： \\[ SS_{RES} = \\sum_{i=1}^n(y_i-\\bar{y})^2 =SS_{yy} \\] 因此，沒有預測變量的零假設模型，它的殘差平方和，就等於因變量的平方和。 26.2.2 分割零假設模型的殘差平方和 ANOVA，方差分析的原則，其實就是將較簡單模型 (零假設模型) 的殘差平方和 \\((SS_{RES_{NULL}})\\)，分割成下面兩個部分： 替代假設的複雜模型能夠說明的模型平方和 \\((SS_{REG})\\)； 替代假設的複雜模型的殘差平方和 \\((SS_{RES_{ALT}})\\)。 用數學表達式表示爲： \\[ \\begin{equation} \\sum_{i=1}^n(y_i-\\bar{y})^2 = \\sum_{i=1}^n(\\hat{y}-\\bar{y})^2 + \\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\ SS_{RES_{NULL}}(SS_{yy}) = SS_{REG} + SS_{RES_{ALT}} \\end{equation} \\tag{26.1} \\] 證明 \\[ \\begin{aligned} \\sum_{i=1}^n(y_i-\\bar{y})^2 &amp;= \\sum_{i=1}^n[(\\hat{y}-\\bar{y})+(y_i-\\hat{y})]^2\\\\ &amp;= \\sum_{i=1}^n(\\hat{y}-\\bar{y})^2+\\sum_{i=1}^n(y_i-\\hat{y})^2+2\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})(y_i-\\hat{y}) \\\\ &amp;= SS_{REG} + SS_{RES_{ALT}} + 2\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})(y_i-\\hat{y}) \\end{aligned} \\] 接下來就是要證明 \\(\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})(y_i-\\hat{y})=0\\) 因爲公式 (25.1) \\(\\hat{y}_i=\\bar{y}+\\hat{\\beta}(x_i-\\bar{x})\\) 所以公式變形如下： \\[ \\begin{aligned} \\sum_{i=1}^n(\\hat{y}_i-\\bar{y})(y_i-\\hat{y}) &amp;= \\sum_{i=1}^n(\\bar{y}+\\hat\\beta(x_i-\\bar{x})-\\bar{y})(y_i-\\bar{y}-\\hat\\beta(x_i-\\bar{x})) \\\\ &amp;= \\sum_{i=1}^n\\hat\\beta(x_i-\\bar{x})[y_i-\\bar{y}-\\hat\\beta(x_i-\\bar{x})] \\\\ &amp;= \\hat\\beta\\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}) - \\hat\\beta^2\\sum_{i=1}^n(x_i-\\bar{x}) \\\\ &amp;= \\frac{S_{xy}}{S_{xx}}S_{xy} - (\\frac{S_{xy}}{S_{xx}})^2SS_{xx}\\\\ &amp;= 0 \\\\ \\Rightarrow SS_{RES_{NULL}}(SS_{yy}) &amp;= SS_{REG} + SS_{RES_{ALT}} \\end{aligned} \\] 26.2.3 \\(R^2\\) – 我的名字叫決定係數 coefficient of determination 在公式 (26.1) 中，因變量的平方和被分割成了兩個部分：\\(SS_{REG}\\) 迴歸模型能說明的部分，和 \\(SS_{RES_ALT}\\) 迴歸模型的殘差平方和。所以，我們定義迴歸模型能說明的部分，佔因變量平方和的百分比 \\(\\frac{SS_{REG}}{SS_{yy}}\\)，爲決定係數 \\(R^2\\)。 這個決定係數之前 (Section 25.5) 也出現過： \\[ \\begin{equation} R^2 = \\frac{SS_{REG}}{SS_{yy}} = \\frac{\\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2} = 1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2} \\end{equation} \\tag{26.2} \\] 再一次回到數據 (24.1) 的線性迴歸來看： library(haven) growgam1 &lt;- read_dta(&quot;backupfiles/growgam1.dta&quot;) Model &lt;- lm(wt~age, data=growgam1) print(summary(Model), digit=6) ## ## Call: ## lm(formula = wt ~ age, data = growgam1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.924182 -0.784889 0.007099 0.797468 4.067806 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.8375842 0.2100701 32.5491 &lt; 2.22e-16 ## age 0.1653314 0.0111115 14.8793 &lt; 2.22e-16 ## ## (Intercept) *** ## age *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.274 on 188 degrees of freedom ## Multiple R-squared: 0.540782, Adjusted R-squared: 0.53834 ## F-statistic: 221.392 on 1 and 188 DF, p-value: &lt; 2.22e-16 R 輸出的結果中最下面的部分 Multiple R-squared: 0.5408。我們就可以用“人話”來解釋其意義：假定年齡和體重成直線關係，那麼年齡解釋了這組數據中兒童體重變化 (平方和) 的 54%。 26.2.4 方差分析表格 the ANOVA table 一般情況下一個簡單線性迴歸，通過 ANOVA 對因變量平方和的分割，會被彙總成下面這樣的表格： 表 26.1: Analysis of Variance table for a simple liear regression model Source of Variation Sum of Squares Degrees of Freedom Mean Sum of Squares Regression (model) \\(SS_{reg}\\) \\(1\\) \\(MS_{reg} = \\frac{SS_{reg}}{1}\\) Residual \\(SS_{res}\\) \\(n-2\\) \\(MS_{res} = \\frac{SS_{res}}{(n-2)}\\) Total \\(SS_{yy}\\) \\(n-1\\) \\(\\frac{SS_{yy}}{(n-1)}\\) 表格中最右邊一列是平均平方和 (mean sum of squares)。它的定義是將平方和除以各自的自由度。其中殘差的平均平方和 \\(MS_{RES}=\\frac{SS_{RES}}{(n-2)}\\) 是替代模型下殘差方差的無偏估計。總體平均平方和 (total mean sum of squares)，則是零假設模型時的殘差方差估計。在 R 裏面也已經演示過多次 anova(model) 是調取方差分析表格的代碼： Model &lt;- lm(wt~age, data=growgam1) print(anova(Model), digit=8) ## Analysis of Variance Table ## ## Response: wt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 359.06320 359.06320 221.39203 &lt; 2.22e-16 ## Residuals 188 304.90655 1.62184 ## ## age *** ## Residuals ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 注意到 R 省略掉第三行總體平方和的部分，不過其實也不太需要。檢驗統計量 F 的計算也很簡單，就是359.06320/1.62184=221.39。 26.2.5 用 ANOVA 進行假設檢驗 在 ANOVA 中使用的檢驗手段是 \\(F\\) 檢驗。這裏用 \\(F\\) 檢驗來比較模型解釋的因變量平方和部分 \\((SS_{REG})\\) 和這個模型不能解釋的殘差平方和部分 \\(SS_{RES}\\) 經過自由度校正以後比值的大小。 此時我們需要知道零假設和替代假設 \\(\\text{H}_0: \\beta=0 \\text{ v.s. H}_1: \\beta\\neq0\\) 時，\\(SS_{REG}, SS_{RES}\\) 的分佈。 零假設和替代假設時，\\(SS_{RES}\\) 均服從自由度爲 \\(n-2\\) 的卡方分佈： \\[ \\begin{equation} \\text{Because } SS_{RES} = \\sum_{i=1}^n \\varepsilon \\sim N(0, \\sigma^2)\\\\ \\frac{SS_{RES}}{\\sigma^2} \\sim \\chi^2_{n-2} \\end{equation} \\tag{26.3} \\] 零假設時， \\(SS_{REG}\\) 服從自由度爲 \\(1\\) 的卡方分佈，且與 \\(SS_{RES}\\) 相互獨立： \\[ \\begin{equation} \\frac{SS_{REG}}{\\sigma^2} \\sim \\chi^2_1 \\end{equation} \\tag{26.4} \\] 替代假設時，\\(SS_{REG}\\) 服從一個非中心化的卡方檢驗，且與 \\(SS_{RES}\\) 相互獨立： \\[ \\begin{equation} SS_{REG} = \\beta^2 SS_{xx} + U \\text{ where }\\frac{U}{\\sigma^2} \\sim \\chi_1^2 \\end{equation} \\tag{26.5} \\] 26.2.6 簡單線性迴歸時的 \\(F\\) 檢驗 如果兩個隨機變量各自服從相應自由度的卡方分佈，他們的每個元素的比值服從 \\(F\\) 分佈： \\[ A\\sim \\chi_a^2 \\text{ and } B\\sim \\chi_b^2\\\\ \\Rightarrow \\frac{A/a}{B/b} \\sim F_{a,b} \\] 因此，目前爲止的推導過程我們也可以看到，在零假設條件下，\\(MS_{REG}\\) 和 \\(MS_{RES}\\) 的比值會服從 \\(F\\) 分佈，自由度爲 \\((1, n-2)\\)： \\[ \\begin{equation} F=\\frac{SS_{REG}/1}{SS_{RES}/(n-2)} = \\frac{MS_{REG}}{MS_{RES}} \\sim F_{1,n-2} \\end{equation} \\tag{26.6} \\] 在替代假設條件下 \\((\\text{H}_1: \\beta\\neq0)\\)，\\(SS_{REG}\\) 的期望值是 \\(\\sigma^2+\\beta^2SS_{xx}\\)，所以替代假設條件下的 \\(F\\) 檢驗量總是會大於零假設時的 \\(F\\)。因此你可以看到，這是一個雙側檢驗 (\\(\\text{H}_0: \\beta=0 \\text{ v.s. H}_1: \\beta\\neq0\\))，但是由於替代假設的 \\(F\\) 總是較大，所以只需要 \\(F\\) 的右半部分的概率密度積分 (單側 \\(p\\) 值)。 26.2.7 簡單線性迴歸時 \\(F\\) 檢驗和 \\(t\\) 檢驗的一致性 證明 \\[ \\begin{aligned} &amp;F=\\frac{SS_{REG}/1}{SS_{RES}/(n-2)} = \\frac{SS_{REG}}{(SS_{yy}-SS_{REG})/(n-2)} \\\\ &amp;\\text{Since } r^2 = \\frac{SS_{REG}}{SS_{yy}} \\\\ &amp;F=(n-2)\\frac{SS_{yy}r^2}{SS_{yy}-SS_{yy}r^2}=(n-2)(\\frac{r^2}{1-r^2})=t^2 \\end{aligned} \\] 最後一步用到 (Section 25.6) 證明過的，迴歸係數檢驗統計量 \\(t\\)，和 Pearson 相關係數 \\(r\\) 之間的關係。 26.3 分類變量用作預測變量時的 ANOVA 方差分析的應用是如此的廣泛，你可以在多重迴歸中使用，也可以在模型中有分類變量時使用，甚至是同時有連續性變量和分類變量的迴歸模型中得到應用。 之前也遇到過二分類變量的簡單線性迴歸模型，當時我們的做法是使用一個啞變量來表示一個二分類變量。同樣的方法也可以用到多組分類變量上來，然後繼續使用線性迴歸。 26.3.1 一個二分類預測變量 在前面的例子 (Section 24.7) 中也已經展示過，可以通過線性迴歸來分析一個二分類變量 (實驗組對照組)，和一個連續型變量 (能直立行走時的兒童年齡)兩個變量之間的關係。而且其結果同兩樣本 \\(t\\) 檢驗的結果完全一致。 繼續回到之前用過的這個兒童行走數據 (表 24.1)： ## ## Call: ## lm(formula = Age ~ Group, data = Walk) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.1250 -0.7375 -0.3750 0.3875 2.8750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.12500 0.51223 19.7663 1.007e-08 *** ## Groupcontrol 2.22500 0.75977 2.9285 0.0168 * ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.255 on 9 degrees of freedom ## (1 observation deleted due to missingness) ## Multiple R-squared: 0.48795, Adjusted R-squared: 0.43105 ## F-statistic: 8.5763 on 1 and 9 DF, p-value: 0.016797 ## Analysis of Variance Table ## ## Response: Age ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Group 1 13.502 13.5017 8.5763 0.0168 * ## Residuals 9 14.169 1.5743 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 之前分析這個數據的時候也說明過了，這裏的迴歸係數 \\(2.225\\) 的含義是兩組之間均值的差異。而且注意看，這個迴歸係數是否爲零的檢驗統計量\\((t-test)\\)獲得的 \\(p\\) 值和 ANOVA 的檢驗結果 \\((F-test)\\) 也是一致的。正驗證了我們前面證明的結果。(Section 26.2.7) 26.3.2 一個模型，兩種表述 上面這個例子中，一個二分類的預測變量和一個因變量之間的關係，實際上可以用兩種數學模型來表達： 令 \\(y_i, x_i\\) 分別是第 \\(i\\) 名觀察對象的因變量 (“直立行走的年齡”)，和預測變量 (“實驗組或者對照組”) \\((i=1,\\cdots,n)\\)。那麼迴歸模型可以寫作： \\[ \\begin{equation} y_i = \\alpha+\\beta x_i + \\varepsilon_i, \\text{ where } \\varepsilon_i \\sim NID(0, \\sigma^2) \\end{equation} \\tag{26.7} \\] 其中， - \\(x_i=0\\) 時，表示第 \\(i\\) 名觀察對象在實驗組； - \\(x_i=1\\) 時，表示第 \\(i\\) 名觀察對象在對照組。 在這樣的迴歸模型標記下，零假設和替代假設分別是 \\(\\text{H}_0: \\beta=0 \\text{ v.s. H}_1: \\beta\\neq0\\) 另一種模型的表達方式，被叫做 ANOVA 表達方式。是如此描述上面的關係的：令 \\(y_{ki}\\) 表示第 \\(i\\) 名觀察對象，他在第 \\(k\\) 組 \\((i=1,\\cdots, n_k; k=1,2)\\)，此時的模型被寫作： \\[ \\begin{equation} y_{ki} = \\mu_k + \\varepsilon_{ki}, \\text{ where } \\varepsilon_{ki} \\sim NID(0, \\sigma^2) \\end{equation} \\tag{26.8} \\] 此時，\\(\\mu_k\\) 表示第 \\(k\\) 組因變量的均值。零假設和替代假設分別是 \\(\\text{H}_0: \\mu_k=\\mu \\text{ v.s. H}_1: \\mu_k\\neq\\mu\\)。這裏的 \\(\\mu\\) 表示，每個組的平均值等於一個共同的均值 \\(\\mu\\)。 26.3.3 分組變量的平方和 對於預測變量只有一個分組變量的模型，擬合後的數值就是兩組的因變量均值 \\((\\bar{y}_k)\\)。在零假設條件下，兩組均值相等，均等於總體均值 \\(\\bar{y}\\)。這就導致了，殘差平方和，模型平方和在分組變量的 ANOVA 分析時要使用與連續型變量不同的術語。 殘差平方和表示爲： \\[ \\begin{equation} SS_{RES} = \\sum_{k=1}^k\\sum_{i=1}^{n_k} (y_{ki}-\\bar{y}_k)^2 \\end{equation} \\tag{26.9} \\] 其實這就是組內平方和 (within group sum of squares)。 模型平方和表示爲： \\[ \\begin{equation} SS_{REG} = \\sum_{k=1}^k\\sum_{i=1}^{n_k}(\\bar{y}_k-\\bar{y})^2=\\sum_{k=1}^kn_k(\\bar{y}_k-\\bar{y})^2 \\end{equation} \\tag{26.10} \\] 其實這就是組間平方和 (between group sum of squares) Mdl0 &lt;- aov(Age ~ Group, data = Walk) # fit a one-way ANOVA print(summary(Mdl0), digits = 6) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Group 1 13.5017 13.50170 8.57629 0.016797 * ## Residuals 9 14.1687 1.57431 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 1 observation deleted due to missingness 其實這跟之前的 anova(Model) 給出的結果完全一致。 bartlett.test(Age ~ Group, data=Walk) ## ## Bartlett test of homogeneity of variances ## ## data: Age by Group ## Bartlett&#39;s K-squared = 0.63, df = 1, p-value = ## 0.4 FYI. 上面的代碼 bartlett.test() 利用的是另外一個叫做 Bartlett 檢驗法的方差比較公式。(在 STATA 的 oneway 命令中也會默認給出 Bartlett 檢驗的方差是否一致的檢驗結果) 26.3.4 簡單模型的分組變量大於兩組的情況 公式 (26.8), (26.9), 和 (26.10) 在兩組以上分組變量作預測變量時也是適用的。但是當組數爲 \\(K\\) 時，組內平方和 (殘差平方和 \\(SS_{RES}\\)) 的自由度需要修改成 \\(n-K\\) (這是因爲模型中使用了 \\(K\\) 個參數)。此時方差分析 ANOVA 的彙總表格就變爲了下面這樣： 表 26.2: One-way ANOVA table Source of variation Sum of Squares Degrees of Freedom Mean Sum of Squares Between groups \\(SS_{between}\\) \\(K-1\\) \\(\\frac{SS_{between}}{(K-1)}\\) Within groups \\(SS_{within}\\) \\(n-K\\) \\(\\frac{SS_{within}}{(n-K)}\\) Total \\(SS_{yy}\\) \\(n-1\\) \\(\\frac{SS_{yy}}{(n-1)}\\) 此時，檢驗統計量 \\(F\\) 的計算公式爲： \\[ \\begin{equation} F=\\frac{SS_{between}/(K-1)}{SS_{within}/(n-K)} \\sim F_{(K-1),(n-K)} \\end{equation} \\tag{26.11} \\] 在解釋兩組以上分組變量的分析結果時，要注意的是如果 \\(p\\) 值很小，檢驗結果告訴我們的是，各組中因變量的均值不全相等，而不是全部都不相等。其實就是，即使做了這個檢驗，我們也不知道到底那兩組之間是有差異的。如果此時我們發現結果提示均值不全相等，通常我們還會再作進一步的分析，使用類似成對比較法等等 (以後再繼續詳述)。不過提前要記住，如果使用成對比較法時 (pair-wise comparisons)，多重比較的問題 (multiple comparisons)會凸顯出來，主要的結果是增加統計檢驗的假陽性 (false-positive) 概率，此時再繼續使用 \\(p&lt;0.05\\) 作爲統計學意義的標準則是不妥當的。 "],
["-multivariable-models.html", "第 27 章 多元模型分析 Multivariable Models 27.1 背景 27.2 兩個預測變量的線性迴歸模型 27.3 線性回歸模型中使用分組變量 27.4 協方差分析模型 the Analysis of Covariance (ANCOVA) Model 27.5 偏回歸係數的變化", " 第 27 章 多元模型分析 Multivariable Models 27.1 背景 簡單線性迴歸描述的是一個連續型的因變量 \\((y)\\)，和一個單一的預測變量 \\((x)\\) 之間的關係。我們考慮把這個模型擴展成包含多個預測變量，單一因變量的模型。例如，我們可以考慮建立一個模型使用生活習慣 (包括“年齡，性別，運動，飲食習慣等”) 來預測收縮期血壓。此時多重迴歸的思想就可以幫我們理解一些我們更加關心的因子，與因變量之間的關係，同時控制或者叫調整了其他的混雜因子 (control or adjust confounders)。有時候這樣的模型也可以直接應用到生活中去，比如上面的例子，我們可以通過瞭解一個人的生活習慣，用建立好的模型來估計這個人的收縮期血壓。 建立模型之前，必須明確研究的目的是什麼。例如我們關心一個新發現的因子可能與高血壓有關係，那麼模型中我們放進去調整的其他因子 (如年齡，性別，運動) 等和因變量 (血壓) 之間的關係就變得不那麼重要。 多重線性迴歸，或者叫多元模型分析 (multiple linear regression or multivariable linear regression) 是研究一個連續型因變量和多個預測變量之間關係的重要模型。本章還會着重討論混雜 (confounding)的概念。 27.2 兩個預測變量的線性迴歸模型 27.2.1 數學標記法和解釋 這裏假設我們研究一個因變量 \\(Y\\)，和兩個預測變量 \\((X_1,X_2)\\) 的模型。那麼此時兩個預測變量的線性迴歸模型可以記爲： \\[ \\begin{equation} y_i = \\alpha + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\varepsilon_i, \\text{ where } \\varepsilon_i \\sim NID(0, \\sigma^2) \\end{equation} \\tag{27.1} \\] 其中， \\(y_i\\) 是第 \\(i\\) 名研究對象的因變量數據 (例如體重)； \\(x_{1i}\\) 是第 \\(i\\) 名研究對象的第一個預測變量數據 (例如年齡)， \\(X_1\\)； \\(x_{2i}\\) 是第 \\(i\\) 名研究對象的第二個預測變量數據 (例如身高)， \\(X_1\\)； \\(\\alpha\\) 的涵義是，當兩個預測變量均爲 \\(0\\) 時，因變量的期望值； \\(\\beta_1\\) 的涵義是，當 \\(X_2\\) 不變時，\\(X_1\\) 每升高一個單位，因變量的期望值； \\(\\beta_2\\) 的涵義是，當 \\(X_1\\) 不變時，\\(X_2\\) 每升高一個單位，因變量的期望值。 \\(\\beta_1, \\beta_2\\) 叫做偏迴歸係數 (partial regression coefficient)。它們測量的是兩個預測變量中，當一個被控制 (保持不變) 時，另一個對因變量的影響。 這個模型也可以用矩陣的形式來表示： \\[ \\begin{equation} \\textbf{Y} = \\textbf{X}\\beta+\\varepsilon, \\text{ where } \\varepsilon \\sim N(0, \\textbf{I}\\sigma^2) \\\\ \\left( \\begin{array}{c} y_1\\\\ y_2\\\\ \\vdots\\\\ y_n \\end{array} \\right) = \\left( \\begin{array}{c} 1&amp; x_{11} &amp; x_{21} \\\\ 1&amp; x_{12} &amp; x_{22} \\\\ \\vdots &amp; \\vdots&amp; \\vdots \\\\ 1&amp; x_{1n}&amp; x_{2n} \\\\ \\end{array} \\right)\\left( \\begin{array}{c} \\alpha \\\\ \\beta_1\\\\ \\beta_2 \\end{array} \\right)+\\left( \\begin{array}{c} \\varepsilon_1\\\\ \\varepsilon_2\\\\ \\vdots\\\\ \\varepsilon_n\\\\ \\end{array} \\right) \\end{equation} \\tag{27.2} \\] 此時上面的表達式中，\\(\\textbf{X}​\\) 是一個矩陣，\\(\\textbf{Y}, \\beta, \\varepsilon​\\) 均為向量。殘差被認為服從多變量正態分佈 (Multivariate normal distribution) ，這個多變量正態分佈的協方差矩陣為 \\(\\sigma^2​\\) 和單位矩陣 \\(\\textbf{I}​\\) 的乘積來描述。這等價於假設殘差是獨立同分佈且方差 \\(\\sigma^2​\\) 不變。 27.2.2 最小平方和估計 Least Squares Estimation 跟簡單線性回歸相似地，我們需要通過對殘差平方和最小化，來獲得此時多重線性回歸的各項參數估計： \\[ \\begin{equation} SS_{RES} = \\sum_{i=1}^n \\hat\\varepsilon_{i}^2 = \\sum_{i=1}^n(y_i-\\hat{y})^2=\\sum_{i=1}^n(y_i-\\hat\\alpha-\\hat\\beta_1x_{1i}-\\hat\\beta_2x_{2i})^2 \\end{equation} (\\#eq:se) \\] 求能讓這個殘差平方和取最小值的參數估計 \\(\\hat\\alpha,\\hat\\beta_1,\\hat\\beta_2\\) 我們會在下一章用矩陣標記法來解釋。此處要強調的是，這些估計量都是無偏估計量，且可以被證明的是殘差方差可以用下面的式子來定義： \\[ \\begin{equation} \\hat\\sigma^2=\\sum_{i=1}^n\\frac{\\hat\\varepsilon_i^2}{(n-3)}=\\frac{\\sum_{i=1}^n(y_i-\\hat\\alpha-\\hat\\beta_1x_{1i}-\\hat\\beta_2x_{2i})^2}{(n-3)} \\end{equation} (\\#eq:multivar) \\] 27.3 線性回歸模型中使用分組變量 之前我們已展示過，分組變量可以使用啞變量來表示分組變量。分組變量多於兩組時，即可用多個啞變量同時來表示。現在假設變量 \\(X\\) 有三個分組分別用 \\(1,2,3\\) 來表示。那麼用啞變量來描述含有這個分組變量的數學方法可以標記為： \\[ \\begin{equation} y_i = \\alpha+\\beta_1u_{1i}+\\beta_2u_{2i}+\\varepsilon_i, \\text{ where } \\varepsilon_i \\sim NID (0,\\sigma^2) \\end{equation} \\tag{27.3} \\] 其中 \\[ \\begin{aligned} u_{1i}=\\left\\{ \\begin{array}{ll} 1 \\text{ if } x_i=2 \\\\ 0 \\text{ if } x_i\\neq2 \\\\ \\end{array} \\right. ; u_{2i}=\\left\\{ \\begin{array}{ll} 1 \\text{ if } x_i=3 \\\\ 0 \\text{ if } x_i\\neq3 \\\\ \\end{array} \\right. \\end{aligned} \\] 其實如果你願意，你也可以把公式 ((27.3)) 寫成下面這樣： \\[ \\begin{aligned} \\begin{array}{ll} y_i = \\alpha + \\varepsilon_i &amp; \\text{if } x_i=1 \\\\ y_i = \\alpha +\\beta_1+ \\varepsilon_i &amp; \\text{if } x_i=2 \\\\ y_i = \\alpha +\\beta_2+ \\varepsilon_i &amp; \\text{if } x_i=3 \\\\ \\end{array} \\end{aligned} \\] 所以， \\(\\alpha\\) 是 \\(X=1\\) 時因變量的期待值； \\(\\alpha+\\beta_1\\) 是 \\(X=2\\) 時因變量的期待值，所以 \\(\\beta_1\\) 是分組變量 \\(X\\) 前兩組之間因變量的期待值的差； \\(\\alpha+\\beta_2\\) 是 \\(X=3\\) 時因變量的期待值，所以 \\(\\beta_2\\) 是分組變量 \\(X\\) 前兩組之間因變量的期待值的差。 此時的 \\(X=1\\) 這個組通常被當作是分組變量中的基準組，也就是參照組 (reference group)。實際情況下你可能可以改變這個參照組為其他組的任意一個。 27.4 協方差分析模型 the Analysis of Covariance (ANCOVA) Model 協方差分析模型用來分析一個連續型的因變量 \\(Y\\) ，與一個連續型的預測變量 \\((X_1)\\)和一個二分類的預測變量 \\((X_2= 1,2)\\)，模型被標記為： \\[ \\begin{equation} y_i=\\alpha+\\beta_1x_{1i}+\\beta_2u_{2i}+\\varepsilon_i, \\text{ where } \\varepsilon_i \\sim NID(0,\\sigma^2) \\end{equation} \\tag{27.4} \\] 其中， \\(y_{i}\\) 為第 \\(i\\) 名研究對象的因變量數據 (連續型)； \\(x_{1i}\\) 為第 \\(i\\) 名研究對象的第一個預測變量 (也是連續型)； \\(u_i =\\left\\{ \\begin{array}{ll} 1 \\text{ if } x_{2i}=2 \\\\ 0 \\text{ if } x_{2i}=1 \\\\ \\end{array}\\right.\\) 此模型中用到的參數有： \\(\\alpha\\) 是截距，意為當 \\(X_1=0\\) 且 \\(X_2=1 \\; (u=0)\\) 時的因變量期待值； \\(\\beta_1\\) 是當 \\(X_2\\) 保持不變時，\\(X_1\\) 每升高一個單位時，因變量 \\(Y\\) 的期待值； \\(\\beta_2\\) 是當 \\(X_1\\) 保持不變時，分組變量 \\(X_2\\) 的兩組之間因變量 \\(Y\\) 的期待值差異大小。 所以理解了上面的解釋之後，就可以將表達式 (27.4) 描述為： \\[ \\begin{array}{ll} y_i=\\alpha+\\beta_1x_{1i}+\\varepsilon_i &amp; \\text{ if } x_{2i}=1 \\\\ y_i=\\alpha+\\beta_2+\\beta_1x_{1i}+\\varepsilon_i &amp; \\text{ if } x_{2i} = 2 \\end{array} \\] 所以，在一個二維圖形中繪製這兩條回歸直線，你會發現他們之間是平行的。因為他們之間相差的只有截距，決定直線斜率的回歸係數，都是 \\(\\beta_1\\)。再用之前用過的數據，兒童的體重和年齡，如果此時考慮了性別因素的話，多重線性回歸的輸出結果和圖形分別應該是： library(haven) growgam1 &lt;- read_dta(&quot;backupfiles/growgam1.dta&quot;) growgam1$sex &lt;- as.factor(growgam1$sex) Model1 &lt;- lm(wt ~ age + sex, data=growgam1) print(summary(Model1), digits = 5) ## ## Call: ## lm(formula = wt ~ age + sex, data = growgam1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.19236 -0.76268 -0.00696 0.75675 3.79163 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.152414 0.234254 30.5327 &lt; 2.2e-16 *** ## age 0.163998 0.010919 15.0189 &lt; 2.2e-16 *** ## sex2 -0.518854 0.183053 -2.8344 0.005095 ** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.25 on 187 degrees of freedom ## Multiple R-squared: 0.5597, Adjusted R-squared: 0.55499 ## F-statistic: 118.85 on 2 and 187 DF, p-value: &lt; 2.22e-16 print(anova(Model1), digits = 5) ## Analysis of Variance Table ## ## Response: wt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age 1 359.06 359.06 229.6755 &lt; 2.2e-16 *** ## sex 1 12.56 12.56 8.0341 0.005095 ** ## Residuals 187 292.35 1.56 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 图 27.1: Data and fitted values from a regression model relating age and gender to data from a cross-sectional survey. For male children data points shown as circles and fitted values linked by a solid line. For female children data points shown as triangles and fitted values linked by a dashed line. 27.5 偏回歸係數的變化 在增加不同的預測變量進入線性回歸模型中時，原先在方程中的預測變量的偏回歸係數發生了怎樣的變化？ 我們先從最簡單的開始入手。先只考慮一個簡單先行回歸模型的情況。當我們新加入一個預測變量，模型發生了什麼變化？ \\[ \\begin{aligned} &amp; \\text{Model 1: } y_i = \\alpha^*+\\beta_1^*x_{1i}+\\varepsilon^*_i \\\\ &amp; \\text{Model 2: } y_i = \\alpha + \\beta_1x_{1i} + \\beta_2 x_{2i}+\\varepsilon_i \\end{aligned} \\] \\(\\beta_1, \\beta_1^*\\) 表示的其實是完全不同的含義。\\(\\beta_1^*\\) 被稱為粗回歸係數 (crude)，或者叫做調整前回歸係數，\\(\\beta_1\\) 被稱為調整後回歸係數 (adjusted coefficient)。二者之間的差異，其實是可以通過對這兩個變量進行簡單線性回歸來度量的： \\[ \\text{Model 3: } x_{2i} = \\gamma+\\delta_1x_{1i}+\\omega_i \\] 將 Model 2 中的 \\(x_{2i}\\) 用 Model 3 來替換掉： \\[ \\begin{aligned} \\text{Model 2: }y_i &amp;= \\alpha + \\beta_1 x_{1i} + \\beta_2(\\gamma + \\delta_1x_{1i}+\\omega_i) +\\varepsilon_i \\\\ &amp;= \\alpha + \\beta_2\\gamma+(\\beta_1+\\beta_2\\delta_1)x_{1i}+\\beta_2\\omega_i + \\varepsilon_i \\end{aligned} \\] 比較 Model 1 和變形過後的 Model 2 中 \\(x_{1i}\\) 的係數就不難發現： \\[ \\beta_1^* = \\beta_1 + \\beta_2\\delta_1 \\] 由此可見，調整前後 \\(x_{1i}\\) 的回歸係數的變化 \\(\\beta_1^*, \\beta_1\\) 之間的差異，取決於兩個部分的大小： \\(\\beta_2\\) 的大小和它的符號； \\(X_1, X_2\\) 這兩個預測變量之間有多大關聯，用 Model 3 的 \\(\\delta_1\\) 來度量。 所以，當調整後的 \\(\\beta_1 &gt; 0\\) 時，要分三種情況來討論 27.5.1 情況1: \\(\\beta_1 &gt; \\beta_1^*\\) 此時，\\(\\beta_2\\delta_1&lt;0\\) 所以，二者之間一正一負。如下圖所示： 按圖所示，當 \\(X_2\\) 保持不變，\\(X_1\\) 與因變量 \\(Y\\) 正相關 (\\(\\beta_1&gt;0\\))。但是，兩個預測變量之間 \\(X_1, X_2\\) 也呈正相關關係 \\(\\delta_1 &gt;0\\)。而同時，\\(X_2\\) 的升高會導致因變量 \\(Y\\) 的下降 ($_2 &lt;0 $)。這種情況就意味著，如果，我們不調整 \\(X_2\\) (使之保持不變)，那麼 \\(X_1\\) 每升高一個單位，\\(Y\\) 的變化會低於調整 \\(X_2\\) 時，\\(X_1\\) 的變化所引起的 \\(Y\\) 的變化。如果這時候 \\(\\beta_2,\\delta_1\\) 較大，那麼對於 \\(X_1\\) 來說，調整 \\(X_2\\) 前後，回歸係數的變化較大，如果大到一定程度，甚至調整前後的回歸係數的方向 (正負) 都會發生變化。 27.5.2 情況2：\\(\\beta_1&lt;\\beta_1^*\\) "],
["section-28.html", "第 28 章 多元模型分析的解釋", " 第 28 章 多元模型分析的解釋 "],
["sample-size.html", "第 29 章 樣本量計算問題 29.1 背景 29.2 決定所需樣本量大小的統計學因素 29.3 第一類和第二類錯誤 Type I and type II errors 29.4 比較兩組之間的百分比 (percentages or proportions) 29.5 比較兩組之間的均值 29.6 樣本量計算的調整", " 第 29 章 樣本量計算問題 29.1 背景 計劃臨牀實驗的時候，爲了避免偏倚和帶有偏見的結論，應當將注意力放在 如何將實驗對象隨機分配 (randomisation) 設計對照組 (control group) 合適（且必須）的貫徹盲法 (blinding) 另外一個同樣重要的問題是–“我到底需要多少樣本?” 一項臨牀實驗，應該提供足夠的證據來證明新藥物（新治療方法）是否有效，是否安全。影響一個實驗設計的樣本量的因素可能有如下幾種： 統計學方案。 從統計學上可以推算出，需要多少樣本來獲得一個堅實可信的證據來證明藥物的實際有效性。 經濟上的因素。 然而實際上可能還有經濟上，時間上，人力物力資源上的現實因素，會制約到底一個實驗能夠收集到多少樣本量。 倫理道德上的因素。 許多臨牀實驗還必須受制於醫學倫理因素。在倫理上一個實驗到底可以維持多久。或者說，要考慮當實驗中一些受試者的結果不理想，或者是有副作用的時候，我們何時該及時停止該實驗？ 實驗本身的可信度。 如果一個臨牀實驗的規模在設計上就很小，可能它本身的可信度就很低。 這裏我們只考慮沒有其他任何因素的影響下，1. 統計學方案上該如何計算準確的所需樣本量的大小。 比較下列兩個同樣比較了溶栓酶和安慰劑在預防心肌梗塞患者死亡的臨牀實驗： 表 29.1: Results from the 1st Australian and ISIS-2 trials for reducing mortality from post-MI 治療組 溶栓酶 安慰劑 p.values 1st Australian n=264 n=253 死亡人數 26 (9.8%) 32 (12.6%) p = 0.32 評價指標 Risk ratio 0.78 (95% CI: 0.48 to 1.27) ISIS-2 n=8592 n=8595 死亡人數 791 (9.2%) 1029 (12.0%) p &lt; 0.001 評價指標 Risk ratio 0.77 (95% CI: 0.70 to 0.84) 這兩個臨牀實驗獲得的治療效果 (treatment effect)，在數字的百分比上幾乎十分接近。然而由於樣本兩巨大的差距，可以看到第一個實驗的信賴區間十分的大，使得實驗結果是無意義的。而第二個大樣本的實驗結果就告訴我們，溶栓酶的治療效果是有效降低了心肌梗死患者死亡概率（降低了23%）。第一個實驗收集了近500個病例，卻仍然不能提供確實有效的證據證明溶栓酶的治療效果（提供了強的關聯結果，卻是極弱的證據。strong correlation, but weak evidence) 。 29.2 決定所需樣本量大小的統計學因素 實驗主要結果的測量/比較方法是什麼？ What is the principal outcome measure of the trial? 一項臨牀實驗的主要結果，應該是切合該實驗的主要目的的。並且應當能夠客觀評價。(如死亡率的改善，治癒率的提高等等) 實驗數據準備分析的方案是什麼？ How will the data be analysed to detect a treatment difference? 實驗結果獲得的數據是連續型的 (血壓，血糖值，BMI)？還是分類的離散變量 (死亡的發生與否，疾病的治癒與否)？統計學上認爲的，治療結果提示有意義的差別時的概率。通常定爲 5%。(p &lt; 0.05) 對照組的試驗期望結果是怎樣的？ What results are expected in the control group? 當然我們不可能事先預知實驗對照組可能出現的結果。此處只討論我們的預期結果。大多數情況下，我們可以從已經進行過的類似臨牀試驗報告中獲得，或者是從非臨牀干預型研究（觀察型研究）報告中獲得對照組的期望結果。 如果實驗藥物在治療上確實有差異，當這個差異最小爲多少時希望能從設計的實驗中被檢測到？ How small a treatment difference, if it exists, is important to detect? 這一條恐怕是每個臨牀實驗在設計階段最重要，最敏感也是最難做出決定的。如果我們已知這個藥物療效和對照相比差別很大，那麼樣本量不用很大，就足以提供值得信賴的證據。不過臨牀上常常會認爲療效差距不必非常的顯著，但是在臨牀意義上也是十分重要的。 常常在這個問題上會引起衆多討論，因爲醫生和患者可能認爲任何一點差異都是有臨牀意義的。但是如果我們想檢測出較小的差距，會需要非常巨大的樣本量，這將會是十分不切合實際的。What needs to be decided upon is the smallest clinically relevant difference that would be important to detect if it were true. 在上面第 4 條被決定了以後，還要確定的是我們需要多大的把握來相信這個被檢測出來的療效差別？ With what degree of certainty is needed to be able to detect the treatment difference in 4? 在實際臨牀實驗中，結論是從觀察數據中得來的，而不是從我們預想的那個“未知的實驗效果”。觀察獲得的療效差別，可能比預想的大（有效），也很可能比預想的小（無效）。設計較好的臨牀實驗應該有足夠機率觀察到有意義的療效差別，即使觀察得到的結果不如預期的大。當然要增加我們觀察到有意義的療效差別，最簡單的辦法是增加樣本量。這個條件的含義是，當療效真差別真實存在，我們要有足夠大的把握把它通過實驗觀察到。 29.3 第一類和第二類錯誤 Type I and type II errors 下面羅列一下我們在進行實驗設計時要用到的概念和相應的標記，注意雖然我們無法知道真正的人羣裏真實參數 (parameter) 的大小，但是我們需要用一些估計 (estimator) 來代替： \\(p_1=\\) the observed percentage in those on standard treatment 意爲施行標準治療法時觀察到的（治癒/有效）百分比 \\(p_2=\\) the observed percentage in those on “new” treatment 意爲施行“新療法”時觀察到的（治癒/有效）的百分比 \\(\\Rightarrow p_1-p_2=\\) observed treatment effect 意爲可以觀察到的治療效果。 \\(\\pi_1=\\) the anticipated percentage in those on standard treatment 意爲施行標準治療法時，我們預期的（治癒/有效）百分比 \\(\\pi_2=\\) the anticipated percentage in those on “new” treatment 意爲施行“新療法”時，我們預期的（治療/有效）百分比 \\(\\Rightarrow \\pi_1-\\pi_2=\\) is the true difference which has been decided it is important to detect 意爲上面第 4 條中我們設定好的希望通過實驗證實的真實的療效差別。 其餘的數學標記包括： \\(\\alpha=\\) 有意義的療效差異，在統計學上的水平 (概率水平，通常設定爲 0.05 or 5%) \\(1-\\beta=\\) Degree of certainty that a true difference of \\(\\pi_1 - \\pi_2\\) would be detected. 效能, power。意爲有多大的把握能通過實驗檢測出療效差別。（通常將目標值設定爲 \\(1-\\beta=90\\%\\)） Table 2: Observed trial results compared to the truth of 1) no difference; 2) a true \\(\\pi_1-\\pi_2\\) diffrence 真實情況 Truth 無差別 真實差別存在 \\(\\pi_1-\\pi_2\\) 觀察到不存在有意義差別 \\(1−\\alpha\\) \\(\\beta\\) Type II error 觀察到存在有意義差別 \\(\\alpha\\) Type I error \\(1-\\beta\\) Power 考慮上面這個表格，可以很容易想到，一個理想的實驗設計，我們希望這個臨牀實驗獲得的結果儘可能地落在上表中的 左上角：即如果真實情況是無差別的，實驗結果也應該觀察到不存在有意義的差別。 右下角：即如果真實情況是是存在真實差別 \\(\\pi_1-\\pi_2\\) 的，試驗結果也應該觀察到有意義的差別。 然而，我們在獲得臨牀實驗結果之後常常犯的兩類錯誤，同樣在上面的表格中顯示： Type I error: A type I error is when a treatment difference is claimed based on a statistically significant observed result when in truth no such difference exists, i.e. a false positive result. 左下角爲一類錯誤，即實驗結果觀察到有顯著的療效差異，然而，真實情況是並沒有差異的話，被認爲是假陽性判斷。\\(\\alpha\\) 表示一類錯誤發生的概率。 Type II error: A type II error is when in truth there exists a difference of \\(\\pi_1-\\pi_2\\) but the observed results fail to reach statistical significance, i.e. a false negative result. 右上角爲二類錯誤，即實驗結果觀察到沒有顯著的療效差異，然而，真實情況是有差異的話，被認爲是假陰性判斷。\\(\\beta\\) 表示二類錯誤發生的概率。 Alternative ways of describing \\(\\alpha\\) and \\(\\beta\\) are as follows: \\(\\alpha\\) is the risk of a Type I error; \\(\\alpha\\) 也被叫做檢驗的顯著水平, significant level。 \\(\\beta\\) is the risk of a Type II error. \\(1-\\beta\\) is termed statistical power. 其中 \\(1-\\beta\\) 被叫做檢驗效能。 \\(\\alpha, 1-\\beta\\) 的水平需要事先被確定，否則無法進行進一步的樣本量的計算。 29.4 比較兩組之間的百分比 (percentages or proportions) 29.4.1 樣本量計算公式 (使用顯著水平 5%, 和檢驗效能 90%) \\[n=10.5\\times\\frac{[\\pi_1\\times(100-\\pi_1)+\\pi_2\\times(100-\\pi_2)]}{(\\pi_1-\\pi_2)^2}\\times2\\] 注意： 上面的公式後面有 \\(\\times2\\) 是因爲前一半公式計算的只是一組（治療或對照組）所需的樣本量。 這裏使用的是百分比。所以當使用比例的時候，要把 \\(100\\) 改成 \\(1\\)。 使用公式計算的所需樣本量，並不是說我們需要的病例數就是計算出來的結果。上面的公式獲得的結果只是對所需樣本量的估算。 29.4.2 樣本量計算公式的一般化 (不同的顯著水平和檢驗效能條件下) \\[n=f(\\alpha, \\beta)\\times\\frac{[\\pi_1\\times(100-\\pi_1)+\\pi_2\\times(100-\\pi_2)]}{(\\pi_1-\\pi_2)^2}\\times2\\] 其中， \\(f(\\alpha, \\beta)\\) 指的是關於檢驗顯著水平 \\(\\alpha\\) 和檢驗效能 \\(\\beta\\) 的函數。 可以參考下面的表格： Table 3: Values of \\(f(\\alpha, \\beta)\\) for different levels of \\(\\alpha\\) and \\(\\beta\\) \\(\\alpha\\) \\(\\beta\\) 0.05 0.1 0.2 0.5 (\\(95\\%\\) power) (\\(90\\%\\) power) (\\(80\\%\\) power) (\\(50\\%\\) power) 0.05 13.0 10.5 7.85 3.84 0.01 17.8 14.9 11.7 6.63 要注意的是，除了上面表格中提供的 \\(f(\\alpha, \\beta)\\) 數值，可以通過以下公式計算得出： \\[f(\\alpha, \\beta)=(Z_{1-\\frac{\\alpha}{2}}+Z_{1-\\beta})^2\\] 例如： \\(\\alpha=0.05, \\beta=0.1\\) 時：\\(f(\\alpha, \\beta)=(1.96+1.282)^2=10.5\\); \\(\\alpha=0.05, \\beta=0.2\\) 時：\\(f(\\alpha, \\beta)=(1.96+0.84)^2=7.85\\)。 29.5 比較兩組之間的均值 許多臨牀實驗不光關心患者是否被治癒或者死亡，另外還有許多實驗的主要結果是連續變量：例如，腎功能（腎小球濾過率），或收縮期血壓。然而背後的原理其實還是一樣的。 29.5.1 樣本量計算公式 然而，另外一個必須考慮的因素：治療組對照組測量結果的標準差 (standard deviation, \\(sd, \\sigma\\))。這裏先考慮兩者標準差相同的情況。標準差的數據通常來自與先行研究的科學文獻，有些（土豪）實驗會先進行預實驗獲得想要的實驗數據–標準差。通常，建議像比較百分比那樣，調整改變一下不同的檢驗顯著水品和檢驗效能，計算多個所需樣本量來互相比較參考。 比較兩組均值時需要用到的數學標記： \\(\\mu_1=\\) 標準治療法（對照組）的期待平均值； \\(\\mu_2=\\) 新治療法（治療組）的期待平均值； \\(\\sigma=\\) 兩組的標準差（假設兩組標準差相同）； \\(\\alpha=\\) 一類錯誤發生的概率，檢驗顯著水平； \\(\\beta=\\) 二類錯誤發生的概率，\\(1-\\beta\\) 是檢驗效能。 用上面標記表示的公式如下： \\[n=f(\\alpha, \\beta)\\times\\frac{2\\sigma^2}{(\\mu_1-\\mu_2)^2}\\times2\\] 可以認爲，上面的公式中 \\(\\mu_1-\\mu_2\\) ，各組的平均值本身並不重要，兩組之間均值的差是我們關心的。如果用 \\(\\delta\\) 表示兩組之間均值差的期待值，那麼公式可以改寫爲： \\[n=f(\\alpha, \\beta)\\times\\frac{2\\sigma^2}{\\delta^2}\\times2\\] 29.6 樣本量計算的調整 如果我們無法成功隨訪部分患者，那麼這部分人的數據就無法獲得，實驗數據的說服力就會下降。如果我們預估計有 \\(Q\\%\\) 的人會失去隨訪，那麼我們可以將之前步驟中計算獲得的數字乘以 \\(\\frac{1}{1-Q\\%}\\)。 如果實驗設計是我們會在某個時間點允許治療組或對照組中的部分人變更自己的實驗方案（即治療組的參與者改進入對照組，反之亦然）。那麼所需樣本量的計算調整的方法爲： 令 \\(Q_1=\\) 第一組中改成第二組治療方案的人數比例； 令 \\(Q_2=\\) 第二組中改成第一組治療方案的人數比例； 將之前步驟中計算獲得的樣本量數字乘以 \\(\\frac{1}{(1-Q_1-Q_2)^2}\\)。 如果預期參與實驗治療組（而不是對照組）的人中有部分人（比例爲 \\(Q\\)）會中斷實驗進程，那麼調整公式爲：\\(\\frac{1}{(1-Q)^2}\\)。 還有的實驗會使用大於 \\(1:1\\) 的比例設計對照組和實驗組的人數。假設這一比例爲 \\(r:1\\) 那麼調整的樣本量數字還要乘以：\\(\\frac{(r+1)^2}{4r}\\)。 "],
["section-30.html", "第 30 章 穩健統計方法入門", " 第 30 章 穩健統計方法入門 "],
["section-31.html", "第 31 章 基於秩次的非參數檢驗 31.1 符號檢驗 the Sign test 31.2 Wilcoxon 符號秩和檢驗，the Wilcoxon signed-rank test 31.3 Wilcoxon-Mann-Whitney (WMW) 檢驗 31.4 秩相關，Spearman’s Rank Correlation Coefficient 31.5 基於秩次的非參數檢驗的優缺點", " 第 31 章 基於秩次的非參數檢驗 基於秩次的統計學方法不像其他參數檢驗那樣需要太多的假設和前提 (比如服從正態分佈或者其它假設)。這類方法其實放棄了數據的部分信息 – 那就是數據之間值的差距。通過給數據排序列，我們僅僅知道數據的排序。所以我們可以下結論說 \\(A\\) 大於 \\(B\\)，或者 \\(B\\) 大於 \\(C\\) (因此 \\(A\\) 也大於 \\(C\\))，但是他們之間數值的差距被忽略掉了 (所以我們不能比較 \\(A-B\\) 和 \\(B-C\\) 的大小)。 所以，基於秩次的統計學方法完全只依賴數據的大小排序，觀察獲得數據的真實大小被忽視了。 31.1 符號檢驗 the Sign test 我們以下列一組空腹血糖測量值的數據爲例： 表 31.1: Fasting Glucose Level (mmol/L) n=24 Diabetics 10.3 8.8 5.3 9.5 6.7 6.7 12.2 12.5 5.2 15.1 4.2 13.3 10.8 15.3 7.5 19.0 7.2 4.9 16.1 9.3 19.5 8.1 8.6 11.1 我們如果想要對這組數據的中位數做出假設檢驗，\\(H_0: \\theta = 10; \\text{ v.s. } H_1: \\theta\\neq10\\)。 該選擇哪種檢驗方法來回答這個假設檢驗提出的問題：中位數是否等於 \\(10\\)? 符號檢驗 (the sign test)，可以用來輔助我們對數據的中位數 (median \\(\\theta\\)) 作出推斷 (inference)。雖然往下看你會發現嚴格說來這並不算是基於秩次的檢驗方法。使用符號檢驗時我們需要的唯一假設：數據來自連續分佈 (continuous distribution)。 這種類型的檢驗方法常用的假設檢驗如下： \\[ H_0: \\theta=\\theta_0 \\\\ H_1: \\theta\\neq\\theta_0 \\] 其中，\\(\\theta_0\\) 就是我們想要檢驗的中位數的大小，在上面的例子中，\\(\\theta_0=10\\)。 此時我們用到的檢驗統計量，\\(X\\) 的定義是：樣本數據中比 \\(\\theta_0\\) 大的數據個數，和比 \\(\\theta_0\\) 小的數據個數，兩個個數中較小的那一個。因爲在零假設的條件下，如果觀察數據的中位數等於 \\(\\theta_0\\) 的話，數據中比 \\(\\theta_0\\) 大或者小的數據個數應該是相等的。可以用一個二項分佈，概率爲 \\(0.5\\) 的模型來模擬： \\[X\\sim Bin(n, 0.5)\\] 在本例中，觀察數據有 13 個小於 \\(\\theta_0=10\\)，有 11 個大於 \\(\\theta_0=10\\)。因此 \\(X=11, n=24\\)。假如 \\(\\pi\\) 是一個觀察數據大於 \\(\\theta_0\\) 的概率的話，在零假設的條件下，\\(H_0: \\pi=0.5\\)。檢驗這個假設的雙側概率的計算公式爲： \\[2\\times P(X\\leqslant x|\\pi=0.5)\\] 在本例中， \\(X=11\\)。如果（像在考試的時候沒有電腦輔助）要用查表的方式判斷 \\(p\\) 值大小。可以先下載 一份統計數據的表格。下載好了找到 “Statistical Table 7.1 Critical one- and two-tailed values of x for a Sign test” 在第185頁：(下面只是截圖) \\[\\cdots\\cdots\\cdots\\] 图 31.1: Critical Values for a Sign test 找到 \\(n=24\\) 這一行，發現顯著性水平是 \\(20\\%\\) 的拒絕域都要小於 \\(8\\)， 所以本例的 \\(p&gt;20\\%\\)。 如果你很幸運沒有在考場上，那麼可以找出自己的電腦下載好 R 之後執行下面的命令： 2*pbinom(11,24, 0.5) ## [1] 0.8388 或者可以使用命令 binom.test 來做一個二項分佈的概率檢驗： options(scipen = 1, digits = 8) # just to show the p values are exactly the same binom.test(11,24,0.5) ## ## Exact binomial test ## ## data: 11 and 24 ## number of successes = 11, number of trials = 24, ## p-value = 0.83882 ## alternative hypothesis: true probability of success is not equal to 0.5 ## 95 percent confidence interval: ## 0.25553020 0.67179192 ## sample estimates: ## probability of success ## 0.45833333 或者你也可以使用 BSDA 中的 SIGN.test 命令來進行一場轟轟烈烈的符號檢驗： options(scipen = 1, digits = 8) # input the data dt &lt;- c(10.3,9.5,12.2,15.1,10.8,19.0,16.1, 8.1, 8.8, 6.7,12.5, 4.2,15.3, 7.2, 9.3, 8.6, 5.3,6.7 ,5.2,13.3, 7.5, 4.9,19.5,11.1) BSDA::SIGN.test(dt, md=10, alternative=&quot;two.sided&quot;, conf.level=0.95) ## ## One-sample Sign-Test ## ## data: dt ## s = 11, p-value = 0.83882 ## alternative hypothesis: true median is not equal to 10 ## 95 percent confidence interval: ## 7.3988241 12.3011759 ## sample estimates: ## median of x ## 9.4 ## ## Achieved and Interpolated Confidence Intervals: ## ## Conf.Level L.E.pt U.E.pt ## Lower Achieved CI 0.9361 7.5000 12.2000 ## Interpolated CI 0.9500 7.3988 12.3012 ## Upper Achieved CI 0.9773 7.2000 12.5000 “據說”如果你用 Stata 的話還會給你一個絢麗的表格： 图 31.2: The Stata output of a Sign Test 總而言之無論你用的是哪個方法，查 (水) 表法或是統計武俠包，結果都是一樣的：數據無法提供足夠的證據拒絕零假設，即無證據證明中位數不等於10。 需要注意的是，如果觀察數據中有的值恰好等於 \\(\\theta_0\\) 那麼這些觀察數據就會被剔除之後再進行檢驗，相應的樣本量 (\\(n\\)) 也就變小了。如果觀察數據樣本兩足夠大，我們可以使用二項分佈的正態分佈近似 (Section 8.4) 法計算。近似法計算時記得要進行校正 (Section 8.6)： \\[ z=\\frac{\\frac{x}{n}-\\pi}{\\sqrt{\\pi(1-\\pi)/n}}=\\frac{\\frac{x}{n}-0.5}{\\sqrt{0.5(1-0.5)/n}}=\\frac{2x}{\\sqrt{n}}-\\sqrt{n}\\\\ \\text{With the continuity correction, this becomes: }\\\\ z=\\lvert\\frac{2x}{\\sqrt{n}}-\\sqrt{n}\\rvert-\\frac{1}{\\sqrt{n}} \\] 在本例中： \\[z=\\lvert\\frac{2\\times11}{\\sqrt{24}}-\\sqrt{24}\\rvert-\\frac{1}{\\sqrt{24}}=0.204\\] 標準正態分佈的 \\(z\\) 爲 \\(0.204\\) 時的雙側 \\(p\\) 值爲： (1-pnorm(0.204))*2 ## [1] 0.8383535 和前面的直接計算法的結果還算是十分接近滴。 31.1.1 符號檢驗的特點 符號檢驗十分的穩健 (Robust)，因爲我們除了數據連續性的假設之外沒有其他任何假設。但是穩健檢驗是有代價的。因爲進行符號檢驗的同時意味着我們要放棄一個個數據本身能提供的信息。結果導致這類檢驗敏感度較低，檢驗效能 (Power) 較差，以及獲得的信賴區間也就很寬 (不精確)。 31.2 Wilcoxon 符號秩和檢驗，the Wilcoxon signed-rank test Wilcoxon 符號秩和檢驗也可以用來檢驗一組數據的中位數是否和某個已知數字相等。進行本方法時除了像符號檢驗那樣要假設數據是連續的以外，還要假設數據的分佈是左右對稱的。因此，由於假定了左右對稱的前提，中位數也就等於均數，所以它也可以被用於檢驗均數是否等於某個已知的值。 Wilcoxon 符號秩和檢驗的前提可以被認爲是介於符號檢驗和單一樣本 \\(t\\) 檢驗 (還假設數據來自於正態分佈) 之間的一種檢驗。 當一組隨機觀察數據 \\(x_1,\\cdots,x_n\\) 來自於對稱的連續分佈數據。如果它的中位數是 \\(\\theta\\)，在零假設：\\(H_0: \\theta=\\theta_0\\) 的條件下： 將全部觀察數據一一和 \\(\\theta_0\\) 相減，那麼每一個 \\(d_i=x_i-\\theta_0, i=1,2,\\cdots,n\\) 的正負符號概率是相等的； 任何一個和 \\(\\theta_0\\) 相減之後的差 \\(\\lvert d_i \\lvert\\) ，取正負符號的概率是相等的。 這裏我們使用另一個例子來說明 Wilcoxon 檢驗法。下列數據爲，12名女性從平躺姿勢改成直立時心跳次數的變化 (次/分)。 -2, -5, 12, 4, 16, 17, 8, 3, 20, 25, 1, 9 下面來使用 Wilcoxon 符號秩和檢驗法來檢驗中位數 \\(\\theta=15\\)。 首先，先計算每個數據和 \\(15\\) 之間的差值： data &lt;- c(-2, -5, 12, 4, 16, 17, 8, 3, 20, 25, 1, 9) newdata &lt;- data-15 newdata ## [1] -17 -20 -3 -11 1 2 -7 -12 5 10 -14 -6 下一步，計算這些差值的絕對值： abs_newdata &lt;- abs(newdata) abs_newdata ## [1] 17 20 3 11 1 2 7 12 5 10 14 6 Dt &lt;- data.frame(data, newdata, abs_newdata) Dt &lt;- Dt[order(newdata), ] # sort the data by newdata Dt ## data newdata abs_newdata ## 2 -5 -20 20 ## 1 -2 -17 17 ## 11 1 -14 14 ## 8 3 -12 12 ## 4 4 -11 11 ## 7 8 -7 7 ## 12 9 -6 6 ## 3 12 -3 3 ## 5 16 1 1 ## 6 17 2 2 ## 9 20 5 5 ## 10 25 10 10 之後，給絕對值排序： Dt$ranks &lt;- rank(Dt$abs_newdata) Dt ## data newdata abs_newdata ranks ## 2 -5 -20 20 12 ## 1 -2 -17 17 11 ## 11 1 -14 14 10 ## 8 3 -12 12 9 ## 4 4 -11 11 8 ## 7 8 -7 7 6 ## 12 9 -6 6 5 ## 3 12 -3 3 3 ## 5 16 1 1 1 ## 6 17 2 2 2 ## 9 20 5 5 4 ## 10 25 10 10 7 接下來，給小於 \\(15\\) 的數據的排序加上負號： Dt$signed_ranks &lt;- ifelse(Dt$newdata &lt; 0, Dt$ranks*(-1), Dt$ranks) Dt &lt;- Dt[order(Dt$ranks),] Dt ## data newdata abs_newdata ranks signed_ranks ## 5 16 1 1 1 1 ## 6 17 2 2 2 2 ## 3 12 -3 3 3 -3 ## 9 20 5 5 4 4 ## 12 9 -6 6 5 -5 ## 7 8 -7 7 6 -6 ## 10 25 10 10 7 7 ## 4 4 -11 11 8 -8 ## 8 3 -12 12 9 -9 ## 11 1 -14 14 10 -10 ## 1 -2 -17 17 11 -11 ## 2 -5 -20 20 12 -12 對正的負的 signed_ranks 分別求和，絕對值較小的那個就是 Wilcoxon 檢驗的統計量。本例中：\\(S^+=\\) 14，\\(S^-=\\) -64，所以本例中的檢驗統計量等於 \\(14\\)。如果要繼續查表的話，可以找到 \\(0.05&lt;p&lt;0.1\\)： \\[\\cdots\\cdots\\cdots\\] 图 31.3: Critical Values for a Wilcoxon Signed-Ranks test 精確的 Wilcoxon 符號秩和檢驗可以通過下列代碼在 R 裏完成： wilcox.test(Dt$data, mu=15, paired = FALSE) ## ## Wilcoxon signed rank test ## ## data: Dt$data ## V = 14, p-value = 0.052246 ## alternative hypothesis: true location is not equal to 15 如果數據中有觀察值和我們希望比較的數值完全相等的話，和符號檢驗類似的，這些觀察值需要被剔除之後再進行上面的個步驟檢驗。記得還要將樣本量減去相應個數再去查表尋找 \\(p\\) 值。 另外，下面的代碼可以計算正態分佈近似的 Wilcoxon 秩和檢驗，結果十分接近： wilcox.test(Dt$data, mu=15, paired = FALSE, exact = FALSE, correct = FALSE) ## ## Wilcoxon signed rank test ## ## data: Dt$data ## V = 14, p-value = 0.04986 ## alternative hypothesis: true location is not equal to 15 值得注意的是，精確計算時，我們需要剔除那些和比較數值完全一致的觀察值。然而在正態分佈近似法的 Wilcoxon 檢驗中，這些數值並不會被剔除，而是保留下來，並且用於對方差進行調整。Wilcoxon 秩和檢驗可以在我們能夠假設數據左右對稱分佈，且明顯不服從正態分佈時使用 (即概率密度分布圖左右兩端的尾部較厚的時候)。如果數據左右完全對稱，本檢驗方法不太推薦採用。 31.3 Wilcoxon-Mann-Whitney (WMW) 檢驗 本方法用於比較兩組獨立樣本的分佈是否只是左右位移 (或者叫平移)。此檢驗需要的假設前提爲：兩組獨立樣本來自連續型分佈，且僅僅只存在整體的左右位移 (或者叫平移) (location shift)。 例如說，兩組數據各自的累積概率方程分別是 \\(F(\\cdot), G(\\cdot)\\) 時，由上面的假設可知： \\[G(y)=F(y-\\Delta), \\text{ where } -\\infty&lt;\\Delta&lt;\\infty\\] 上面式子中的 \\(\\Delta\\) 就是所謂的 “左右位移 (或者叫平移)”。因此本檢驗的零假設和替代假設爲： \\[H_0: \\Delta=0\\\\ H_1: \\Delta\\neq0\\] 在零假設的條件下，我們可以認爲兩個樣本來自相同的人羣分佈。假如，\\(F, G\\) 都是正態分佈，且同方差。那麼 \\(\\Delta\\) 就等於兩個分佈的均值差。在這種情況下就可以使用兩樣本 \\(t\\) 檢驗。所以說，WMW 檢驗其實就是把假設前提放寬了的 (免去了正態分佈假設) 兩樣本 \\(t\\) 檢驗。 如果兩個獨立樣本分別有樣本量 \\(n, m, \\text{ and } n&lt;m\\)：\\(X_1,\\cdots,X_n\\) 和 \\(Y_1, \\cdots, Y_m\\)。在零假設的條件下，將這兩個樣本合併之後的大樣本 (\\(m+n\\) 個樣本量) ：\\(X_1,\\cdots,X_n,Y_1, \\cdots, Y_m\\) 可以視爲來自同一分佈。那麼在合併後的樣本中，我們給每一個元素賦予它們的合併後數據中的排序 (\\(\\text{Rank}_i: i= 1,2,\\cdots,n, n+1, \\cdots, n+m\\))。那麼我們感興趣的 Wilcoxon 秩和統計量 (Wilcoxon rank sum statistic) \\(W_1\\) 是樣本量較小的那些數字在合併後數據中的排序之和。 \\[W_1=\\sum_{i=1}^n R_i\\] 對於大小相同的數據，排序取他們的排序的平均值。 之後再計算 ： \\[U_1=W_1+\\frac{n(n+1)}{2}\\] 最後拿來判斷的檢驗統計量是 \\(U_1\\) 和 \\(n\\times m -U_1\\) 兩者中較小的數字。(注：如果我們一開始計算樣本量較多的部分的秩和 \\(W_2=\\sum_{i=1}^m R_i\\) 時，將計算的 \\(U_2=W_2-\\frac{m(m+1)}{2}\\)，跟 \\(n\\times m - U_2\\) 中較小的數字作爲檢驗統計量的話，我們會在數學上獲得完全一樣的檢驗統計量。) 其實兩個統計量 \\(W, U\\) 均可以用來作相同的統計推斷，當然各自的 \\(p\\) 值表格不同。但是 \\(U\\) 有另外一種統計學含義：\\(U\\) 是 \\(X_i&gt;Y_j\\)， 也就是所有的 \\((X_i, Y_j)\\) 配對中 \\(X_i\\) 較大的對的個數。 這裏使用下面的例子來解釋如何操作 WMW 檢驗： 採集16名甲亢兒童的血清甲狀腺素濃度值列表如下， 表 31.2: Serum thyroxine levels n=16 hypothyroid children thyr group 34 Slight or no symptoms 45 Slight or no symptoms 49 Slight or no symptoms 55 Slight or no symptoms 58 Slight or no symptoms 59 Slight or no symptoms 60 Slight or no symptoms 62 Slight or no symptoms 86 Slight or no symptoms 5 Marked symptoms 8 Marked symptoms 18 Marked symptoms 24 Marked symptoms 60 Marked symptoms 84 Marked symptoms 96 Marked symptoms 這裏我們需要比較輕微症狀組和嚴重症狀組的血清甲狀腺濃度的分佈是否只是左右位移，\\(H_0: \\Delta=0\\)。 首先，我們要給兩組合併後的濃度排序： dt$rank &lt;- rank(dt$thyr) dt &lt;- dt[order(dt$rank),] dt ## thyr group rank ## 10 5 Marked symptoms 1.0 ## 11 8 Marked symptoms 2.0 ## 12 18 Marked symptoms 3.0 ## 13 24 Marked symptoms 4.0 ## 1 34 Slight or no symptoms 5.0 ## 2 45 Slight or no symptoms 6.0 ## 3 49 Slight or no symptoms 7.0 ## 4 55 Slight or no symptoms 8.0 ## 5 58 Slight or no symptoms 9.0 ## 6 59 Slight or no symptoms 10.0 ## 7 60 Slight or no symptoms 11.5 ## 14 60 Marked symptoms 11.5 ## 8 62 Slight or no symptoms 13.0 ## 15 84 Marked symptoms 14.0 ## 9 86 Slight or no symptoms 15.0 ## 16 96 Marked symptoms 16.0 Wilcoxon 統計量 \\(W_1\\) 是人數少的組的排序之數值和。本樣本中 7 人有嚴重症狀，9人有輕微或無症狀。所以 \\(W_1\\) 就是嚴重症狀組的秩和： \\[W_1=1+2+3+4+11.5+14+16=51.5\\] 再計算統計量 \\(U_1\\)： \\[U_1=W_1-\\frac{n(n+1)}{2}=51.5-\\frac{7\\times(7+1)}{2}=23.5\\] 所以 \\(n\\times m-U_1=7\\times9-23.5=39.5\\)，顯然這兩個數值中小的 \\(23.5\\) 就是我們尋找的 WMW 統計量。繼續查水錶： 图 31.4: Critical Values of U for a Wilcoxon-Mann-Whitney test 可知 \\(n_1=7, n_2=9\\) 時，統計量要低於 \\(15\\) \\(p\\) 值才會小於 \\(0.01\\)。所以數據給出的 \\(p&gt;0.1\\)。 在 R 裏面用下面的代碼進行 WHW 檢驗： wilcox.test(dt$thyr~dt$group, correct=FALSE) # without continuity correction ## ## Wilcoxon rank sum test ## ## data: dt$thyr by dt$group ## W = 23.5, p-value = 0.39675 ## alternative hypothesis: true location shift is not equal to 0 wilcox.test(dt$thyr~dt$group) # with continuity correction i.e. normal appriximation ## ## Wilcoxon rank sum test with continuity ## correction ## ## data: dt$thyr by dt$group ## W = 23.5, p-value = 0.42692 ## alternative hypothesis: true location shift is not equal to 0 31.4 秩相關，Spearman’s Rank Correlation Coefficient Spearman 的秩相關 (通常用 \\(\\rho\\))，是一種基於數據排序的相關係數算法。和傳統的 Pearson 相關係數類比，是當數據無法被認定是線性相關時的另一種相關關係檢驗方法。所以秩相關不假定兩組數據之間是線性相關 (linear association)。秩相關只關心一個數據遞增時，另一個數據是否單調遞增。所以可以用於傾向性檢驗。 具體的操作是，在兩組數據中先各自排序，像所有的排序檢驗一樣遇到相同大小的數值將排序取均值。之後使用一般的求相關係數的方法。本法中只用到了數值在各自組中的排序，並沒有使用他們的真實大小。近似法的秩相關計算公式爲： \\[\\hat\\tau=\\frac{\\hat\\rho}{\\sqrt{(1-\\hat\\rho^2)/(n-2)}}\\] 在零假設條件下 \\(H_0: \\rho=0\\)，上面的近似法秩相關服從 \\(t_{n-2}\\) 分佈。 下面用某血友病患者調查數據獲得的血液 \\(T_4, T_8\\) 淋巴球計數 \\((\\times10^9/\\ell)\\) 來詳細解釋計算過程： 表 31.3: Lymphocyte counts n=28 haemophiliacs th4 th8 0.20 0.17 0.27 0.52 0.28 0.25 0.37 0.34 0.38 0.14 0.48 0.10 0.49 0.58 0.56 0.23 0.60 0.24 0.64 0.67 0.64 0.90 0.66 0.26 0.70 0.61 0.77 0.18 0.88 0.74 0.88 0.54 0.88 0.76 0.90 0.62 1.02 0.48 1.10 0.58 1.10 0.34 1.18 0.84 1.20 0.63 1.30 0.46 1.40 0.84 1.60 1.20 1.64 0.59 2.40 1.30 給這組數據繪製散點圖： 图 31.5: Scatter plot of T4 and T8 counts 可以看見圖中右上角的兩個點幾乎可以認爲是異常值 (outliers)。 分別給 th4, th8 求各自的排序： dt$rank4 &lt;- rank(dt$th4) dt$rank8 &lt;- rank(dt$th8) kable(dt, &quot;html&quot;, align = &quot;c&quot;,caption = &quot;Lymphocyte counts&quot;) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;bordered&quot;)) %&gt;% # collapse_rows(columns = c(1)) %&gt;% add_header_above(c(&quot;n=28 haemophiliacs with ranks&quot; = 4)) %&gt;% scroll_box(width = &quot;400px&quot;, height = &quot;500px&quot;, extra_css=&quot;margin-left: auto; margin-right: auto;&quot;) 表 31.4: Lymphocyte counts n=28 haemophiliacs with ranks th4 th8 rank4 rank8 0.20 0.17 1.0 3.0 0.27 0.52 2.0 13.0 0.28 0.25 3.0 7.0 0.37 0.34 4.0 9.5 0.38 0.14 5.0 2.0 0.48 0.10 6.0 1.0 0.49 0.58 7.0 15.5 0.56 0.23 8.0 5.0 0.60 0.24 9.0 6.0 0.64 0.67 10.5 21.0 0.64 0.90 10.5 26.0 0.66 0.26 12.0 8.0 0.70 0.61 13.0 18.0 0.77 0.18 14.0 4.0 0.88 0.74 16.0 22.0 0.88 0.54 16.0 14.0 0.88 0.76 16.0 23.0 0.90 0.62 18.0 19.0 1.02 0.48 19.0 12.0 1.10 0.58 20.5 15.5 1.10 0.34 20.5 9.5 1.18 0.84 22.0 24.5 1.20 0.63 23.0 20.0 1.30 0.46 24.0 11.0 1.40 0.84 25.0 24.5 1.60 1.20 26.0 27.0 1.64 0.59 27.0 17.0 2.40 1.30 28.0 28.0 接下來再給 th4, th8 的排序做散點圖： 图 31.6: Scatter plot of T4 and T8 ranks 此時也就沒有了異常值的存在。對二者的排序計算相關係數： cor.test(dt$rank4, dt$rank8) ## ## Pearson&#39;s product-moment correlation ## ## data: dt$rank4 and dt$rank8 ## t = 4.11513, df = 26, p-value = 0.00034606 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.33297073 0.81107104 ## sample estimates: ## cor ## 0.62803129 cor.test(dt$th4, dt$th8) ## ## Pearson&#39;s product-moment correlation ## ## data: dt$th4 and dt$th8 ## t = 5.28031, df = 26, p-value = 0.000016061 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.47328797 0.86128090 ## sample estimates: ## cor ## 0.71934766 秩相關的相關係數爲 \\(0.628\\)。而原始數據的相關係數爲 \\(0.719\\)。 31.5 基於秩次的非參數檢驗的優缺點 優點： 可以讓我們拋棄很多 (正態分佈等的) 前提假設，許多真實數據本身並不能滿足這些條件，這些情況下，基於秩次的非參數檢驗能提供更高的統計效能 (power)。 缺點： 如果數據本身能夠滿足如正態分佈之類的假設，那麼相比較與一般的參數檢驗，基於秩次的非參數檢驗法效能就偏低。 基於秩次的非參數檢驗較難推廣到更加複雜的情況。 這些檢驗法僅僅只能幫助我們進行假設檢驗。但是多數情況下，我們更加希望能使用觀察數據對總體進行點估計 (point estimates) 並且給出信賴區間 (CIs)。通常情況下，基於秩次的非參數檢驗法就很難給出這樣的估計。 "],
["section-32.html", "第 32 章 貝葉斯統計入門 32.1 概率論推斷的複習 32.2 貝葉斯概率推理/逆概率 Bayesian reasoning/inverse probability 32.3 貝葉斯推理的統計學實現 32.4 練習題", " 第 32 章 貝葉斯統計入門 A Bayesian statistician is one who, vaguely expecting a horse and catching a glimpse of a donkey, strongly concludes he has seen a mule. — Guernsey McPearson’s Drug Development Dictionary3 本章節之目的： 介紹 (啓發) 貝葉斯推斷 Bayesian inference 的基本概念。並且與概率論 frequentist inference 推斷實例作比較。 介紹共軛分佈的概念 conjugate distributions。用單一參數家族 (single parameter family) ，特別是二項分佈的圖形來描述共軛分佈；用方差已知的正態分佈均值來描述共軛分佈。 介紹貝葉斯預測分佈 Bayesian prediction distribution。 推薦書目： “Principles of Statistical Inference” by D.R. Cox (D. Cox 2006) “Bayesian Data Analysis” by Gelman, Carlin, Stern, Dunson, Vehtari, and Rubin (Gelman et al. 2013), website for the book “Bayesian Biostatistics” by Vehtari and Rubin (Lesaffre and Lawson 2012) 貝葉斯統計推斷，提供了不同於概率論推斷的另一種考察和解決問題的思路。所有的思考，都源於貝葉斯定理 Bayes’ Theorem (Section 2)。起源於英國統計學家托馬斯貝葉斯 (Thomas Bayes) 死後被好友 Richard Price 整理發表的論文: “An essay towards solving a problem in the doctrine of chances.” 概率論推斷與貝葉斯推斷的中心都圍繞似然 likelihood (Section 12) 的概念。然而二者對似然提供的信息之理解和解釋完全不同。即在對於觀察數據提供的信息的理解，和如何應用已有信息來影響未來決策（或提供預測）的問題上常常被認爲是統計學中形成鮮明對比的兩種哲學理念。過去幾個世紀二者之間孰優孰劣的爭論相當激烈。但是，從實際應用的角度來看，我們目前更關心哪種思維能更加實用地描述和模擬真實世界。幸運地是，多數情況下，二者的差距不大。所以無法簡單地從一個實驗或者一次爭論中得出誰更出色的結論。現在的統計學家們通常不再如同信仰之爭那樣的互相水火不容，而是從實用性角度來判斷一些實際情況下，採用哪種思想能使計算過程更加簡便或者計算結果更加接近真實情況。 請思考如下的問題： 什麼是概率？ What is probability? 概率論思想下的定義：某事件在多次重複觀察實驗結果中發生次數所佔的比例。 The probability of an event is the limit of its relative frequency in a large number of trials.&quot; 貝葉斯思想下的定義：概率是你相信某事件會發生的可能性。 Probability is a measure of the degree of belief about an event. 32.1 概率論推斷的複習 思考不同場景： 場景 A：假如我們在監測一個製造鐵絲的工廠，需要測量該工廠生產的鐵絲的強度。 場景 B：假如我們正在進行一個大型隊列研究，該研究是關於心臟病和與之相關的某個危險因子的評價。數據來源是家庭醫生的診療數據庫。 場景 C：假如一名警察凌晨三點在空無一人的街頭巡邏時，突然聽見防盜自動警鈴的報警聲。他立刻循聲望去，對面街上的珠寶店玻璃碎了一地。一個戴着巴拉克拉瓦頭套的人正揹着一個大包從破碎玻璃窗中爬出。該警察毫不猶豫地判定該人就是劫匪，立刻將其逮捕。 在這些場景下，請用概率論思想思考如下幾個問題： 事件是什麼？ 如何解讀總體參數？ 如何使用參數進行概率推斷？ 用經典概率論時，有什麼缺點嗎？ 場景 A： 事件：該工廠製造的鐵絲，長期以來的強度大小是多少。 總體參數：鐵絲的真實強度，或者與鐵絲強度相關的特性。 概率推斷：我們進行鐵絲的強度實驗，即從該工廠已經生產的鐵絲中大量抽取樣本逐一進行強度檢測。用相應的概率模型來模擬抽取的樣本數據，並且使用極大似然估計找到最能體現抽樣數據的參數估計，然後對獲得的極大似然估計進行95%信賴區間的計算。然後如果我們重複這樣相同的實驗無數次，那麼我們計算的所有的信賴區間中，有95%包含了真實的鐵絲強度大小。 在鐵絲強度測量的場景中，經典概率論顯得十分自然，因爲我們真的可以重複這樣的實驗很多很多次以獲得想要的參數的精確估計。 場景 B： 事件：由於我們用的是整個隊列研究的數據。所以從概率論的角度來看，本事件就是假定我們可以在人數無限多的人羣中重複同樣的隊列研究。 總體參數：我們感興趣的心臟病相關危險因子，在抽取該隊列作爲樣本的人羣中的真實值大小。 概率推斷：我們用泊松分佈的概率模型來模擬人羣中從開始觀察時起，至心臟病發病這段時間內和該危險因子之間的關係大小。然後用傳統的極大似然估計法計算獲得 HR, OR 等值來表示危險因素和心臟病的關係。 缺點：實際情況是，經費時間和人力資源的限制下，我們無法“重複相同的隊列研究”。而且該對列本身可能就是十分獨特的，比如只有男性，或者有年齡限制，或者其他的特性使隊列本身在理論上就是不可能被重複的。所以，在這樣的場景下，用經典的概率論思想作統計推斷常常會被認爲是不自然不妥當的。 場景 C： 事件：警察無數次在同樣的時間同樣的地點巡邏時，聽見防盜自動警鈴的報警聲，他看見頭戴巴拉克拉瓦頭套的人從破碎的玻璃窗中爬出…… 總體參數：在無數次上面描述的場景時，發生盜竊案的真實概率。 概率推斷：使用某種可以描述該事件（巡邏時。。。發生盜竊案的概率）的數學模型，我們用極大似然估計來計算發生盜竊案概率的估計和95%信賴區間，然後警察同志再來決定是否要去抓眼前這個頭戴巴拉克拉瓦頭套的人。 缺點：經典概率論在如此場景下很明顯是完全不適用的。1) 這裏經典概率論思維下的概率實際上無法準確定義，充其量是一種發生盜竊案可能性的估計。2) 在如此場景下，警察會根據已經觀察到的現象（已知信息），來判斷一場盜竊案發生的概率是多少。 通過上面不同場景的下的思考，應該能看到傳統概率論中始終假設我們可以重複相同的實驗多次，然後從長遠來估測相關事件發生的概率。許多場景下，即使事件概率能被準確定義，我們是很難知道我們關心的參數的分佈的，從而導致我們常常要用到漸進法估算 (asymptotic approximation)。 32.2 貝葉斯概率推理/逆概率 Bayesian reasoning/inverse probability 首先，不得不承認的一個事實是，所有的概率都是條件概率。 要麼是根據已知的信息。 要麼是一般性大家都接受的某種假設條件。 其次，概率，並不是“長遠”地重複觀察獲得的事件發生頻率。相反地，概率的大小取決與你自己和你感興趣的話題（事件）。思考下列例子： 明天會下雨嗎？ 阿森納下一場比賽會贏還是會輸？ 你的期末考試能不能過？ 32.2.1 演繹推理 deductive reasoning 和 三段論 weak syllogisms 數學要用到邏輯，假設我們用 \\(A,B,C\\) 標記不同的事件。 如果 \\(A\\Rightarrow B\\) (事件 A 可以推導出事件 B) 那麼當我們知道“事件 B 爲真”時，雖然B不一定能倒推回 A，但是我們會相信事件 A 很可能發生了。 例如，A 表示“正在下雨”這件事，B 表示 “天上有烏雲”。那麼從邏輯學上來說，\\(A\\Rightarrow B\\) 。然而有烏雲本身不一定會下雨。但是會讓我們覺得下雨的可能性增加了。 再來思考警察巡邏的例子。A 表示 “在珠寶店正在發生盜竊案”，B 表示 “一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出”。也是一樣的道理。 所以警察薯熟在做判斷的時候，需要判斷 Pr(A|B)。他需要如下的信息： 珠寶店發生盜竊案的前提下，有個人從碎玻璃窗中爬出來的概率。 該警察薯熟正處於的環境（半夜三點無人的街頭，等場景） 所以，看到這裏是不是覺得貝葉斯使用的是我們的“常識”在思考決斷問題？因爲我們的先驗概率 (prior) 至關重要。這是我們的背景知識和解釋參數似然（推斷）的依據。 32.2.2 如何給可能性定量 Quantifying plausibility 進行可能性定量之前，R.T. Cox 制定了如下的規則(R. T. Cox 1946)： \\(\\text{plausibility}(A)\\) 是一個有邊界的實數； 傳遞性，transitivity：如果 \\(\\text{plaus}(C)&gt;\\text{plaus}(B)\\) and \\(\\text{plaus}(B)&gt;\\text{plaus}(A)\\) then \\(\\text{plaus}(C)&gt;\\text{plaus}(A)\\) 一致性，consistency：事件 \\(A\\) 發生的可能性只取決於所有與 \\(A\\) 直接相關的信息，而不包括那些推理到與 \\(A\\) 相關信息之前的信息。 The plausibility of proposition \\(A\\) depends only on the relevant information on \\(A\\) and not on the path of reasoning followed to arrive at \\(A\\). R.T. Cox 證明了他提出的這些規則可以完全適用於所有的可能性計算，而且可能性 (plausibility) 的這些規則和概率 (probability) 的微積分計算完全一致。 所以利用上面的可能性規則，我們可以對條件概率進行更深層次的定義： \\[\\text{Pr}(A|B)=\\frac{\\text{Pr}(B|A)\\text{Pr}(A)}{\\text{Pr}(B)}\\propto \\text{Pr}(B|A)\\text{Pr}(A)\\] 用文字表述爲： 事後概率 \\(\\propto\\) 似然 \\(\\times\\) 先驗概率 其中： 事後概率，posterior probability：\\(B\\) 發生的條件下, \\(A\\) 發生的概率； \\(\\propto\\) ：與…成正比； 似然，likelihood：\\(A\\) 發生的條件下，\\(B\\) 發生的概率； 先驗概率，prior probability：事件 \\(A\\) 發生的概率。 這就是貝葉斯定理。這個定理也告訴我們爲什麼貝葉斯論證在18，19世紀時被叫做“逆概率推理, inverse probability reasoning”。因爲似然 (\\(A\\) 發生的條件下，\\(B\\) 發生的概率) 在與先驗概率相乘以後，概率發生了逆轉–事後概率 (\\(B\\) 發生的條件下, \\(A\\) 發生的概率)。 回頭再來看之前的珠寶店盜竊案： 事件 \\(A\\)：珠寶店正在發生盜竊案； 事件 \\(B\\)：一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出。 所以： \\(\\text{Pr}(A)=\\) 珠寶店發生盜竊案的概率 – 先驗概率 (prior probability); \\(\\text{Pr}(B|A)=\\) 當珠寶店發生盜竊案時，觀察到“一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出”事件的可能性 – 似然 (likelihood); \\(\\text{Pr}(A|B)\\) 當觀察到“一個頭戴巴拉克拉瓦頭套的人正在從玻璃窗中爬出”事件時，倒推珠寶店發生了盜竊案的概率 – 事後概率 (posterior probability)。 用例子來解釋貝葉斯推理之後你會發現，其實貝葉斯思想也是純粹的概率理論。與經典概率論不同的是，我們沒有必要認爲某些事件發生的概率需要被重複實驗驗證。貝葉斯對整個世界的理解源於我們每個人自己認爲的事件發生概率 (personalisitic probability)，或者叫信念度（degree of belief）（不需要大量的实验）。 32.3 貝葉斯推理的統計學實現 在經典概率論中，概率分佈的標記 \\(f_X(x;\\theta)\\) 的涵義爲： 對於一個隨機變量 \\(X\\)，它在我們假設的某種固定的真實（上帝才知道是多少的）參數 \\(\\theta\\) 的分佈框架下，不斷重複相同的實驗之後獲得的概率分佈。 在貝葉斯統計推理中，一切都被看作是一個服從概率分佈的隨機變量。利用貝葉斯定理，我們將先驗隨機概率分佈 (prior probability distribution)，和觀察數據作條件概率 (condition on the observed data)，從而獲得事後概率分佈 (posterior probability distribution)。 32.3.1 醫學診斷測試 diagnostic testing 貝葉斯推理最常用的實例是在診斷測試中，即當一個人拿着陽性的檢驗報告結果來找你，你如何判斷這個人有多大的概率真的患有該疾病。 用 \\(D\\) 標記患病， \\(\\bar{D}\\) 標記不患病；\\(T\\) 標記檢查結果爲陽性，\\(\\bar{T}\\) 標記檢查結果爲陰性。那麼，陽性檢查結果時，真的患病的概率 \\(\\text{Pr}(D|T)\\)： \\[ \\begin{aligned} \\text{Pr}(D|T) &amp;= \\frac{\\text{Pr}(T|D)\\text{Pr}(D)}{\\text{Pr}(T)}\\\\ &amp;=\\frac{\\text{Pr}(T|D)\\text{Pr}(D)}{\\text{Pr}(T|D)\\text{Pr}(D)+\\text{Pr}(T|\\bar{D})\\text{Pr}(\\bar{D})} \\end{aligned} \\] 其中分母的轉換用到了 Law of Total Probability (L.T.P): \\[ \\begin{aligned} \\text{Pr}(T) &amp;= \\text{Pr}(T \\cap D) + \\text{Pr}(T \\cap \\bar{D}) \\\\ &amp;= \\text{Pr}(T|D)\\text{Pr}(D)+\\text{Pr}(T|\\bar{D})\\text{Pr}(\\bar{D}) \\end{aligned} \\] 所以說，貝葉斯定理在這裏告訴我們，要計算 \\(\\text{Pr}(D|T)\\) 我們只需要下列幾個信息： 患病率： \\(\\text{Pr}(D)\\) 檢測手段的敏感度 (sensitivity)： \\(\\text{Pr}(T|D)\\) 檢測手段的 1 - 特異度 (specificity)： \\(\\text{Pr}(T|\\bar{D})=1-\\text{Pr}(\\bar{T}|\\bar{D})\\) 32.3.2 HIV 檢查時的應用 假設人羣中患病率爲 \\(1/1000\\)，所用的 HIV 檢測手段的敏感度爲 \\(0.99\\)， 特異度爲 \\(0.98\\)。試計算該檢測HIV手段的事後概率（即拿到陽性結果時，患病的概率 \\(\\text{Pr}(D|T)\\)）。 解 令 \\(D=\\text{HIV positive}, \\bar{D}=\\text{HIV negative}\\\\ T=\\text{test postive}, \\bar{T}=\\text{test negative}\\) \\[ \\begin{aligned} \\text{Pr}(D|T) &amp;= \\frac{\\text{Pr}(T|D)\\text{Pr}(D)}{\\text{Pr}(T|D)\\text{Pr}(D)+\\text{Pr}(T|\\bar{D})\\text{Pr}(\\bar{D})} \\\\ &amp;= \\frac{0.99\\times0.001}{0.99\\times0.001+(1-0.98)\\times0.999} \\\\ &amp;= 0.0472 \\end{aligned} \\] 如果 特異度能達到 \\(0.99\\) \\[ \\begin{aligned} \\text{Pr}(D|T) &amp;= \\frac{\\text{Pr}(T|D)\\text{Pr}(D)}{\\text{Pr}(T|D)\\text{Pr}(D)+\\text{Pr}(T|\\bar{D})\\text{Pr}(\\bar{D})} \\\\ &amp;= \\frac{0.99\\times0.001}{0.99\\times0.001+(1-0.99)\\times0.999} \\\\ &amp;= 0.0901 \\end{aligned} \\] 如果特異度能達到 \\(0.999\\) \\[ \\begin{aligned} \\text{Pr}(D|T) &amp;= \\frac{\\text{Pr}(T|D)\\text{Pr}(D)}{\\text{Pr}(T|D)\\text{Pr}(D)+\\text{Pr}(T|\\bar{D})\\text{Pr}(\\bar{D})} \\\\ &amp;= \\frac{0.99\\times0.001}{0.99\\times0.001+(1-0.999)\\times0.999} \\\\ &amp;= 0.497 \\end{aligned} \\] 可見，對於像 HIV 這樣人羣中患病率較爲罕見的疾病，其檢驗手段的敏感度，特異度都要達到極高才能讓檢驗結果可靠，即拿到陽性結果的人的確患有該疾病。其中當敏感度爲 \\(0.99\\)，特異度爲 \\(0.999\\) 時，才能讓這樣的檢驗手段達到接近一半的可靠程度 (即只有接近一半的陽性結果是真陽性)。 注意本例爲貝葉斯理論的特例，即我們使用的是一個固定的先驗概率 (prior) 和似然 (likelihood)。一般情況下，先驗概率和似然會有自己的概率分佈 (probability distribution)，而不會是一個固定的值， 其相應的事後概率 (posterior) 也擁有概率分佈，並且使用它本身的均值和方差來描述。 32.3.3 說點小歷史 图 32.1: Sir Ronald Fisher Ronald Aylmer Fisher (1890-1962) 推動了統計學在20世紀前半頁的重大發展。他鞏固了概率論統計學堅實的基礎，並且積極提倡這一套理論(Fisher 1922)。但是 Fisher 本人對於統計學的“統計學意義, level of significance” 的認識卻是隨着時間和他年齡的變化而變化的： 表 18.1: Fisher’s interpretation of ‘level of significance’ and the Neyman-Pearson interpretation 早期 Fisher (1935) 晚期 Fisher (1956) Neyman and Pearson 統計學有意義的水平（傳統上使用 \\(\\alpha=5\\%\\)），必須在實施統計檢驗之前就被決定。因此，統計學意義的水平是相應統計學檢驗本身的性質之一。 Thus, the level of significance is a property of the test. 統計學意義的水平，應該被精確計算並且在報告中明確 \\(p\\) 值的大小，故統計學意義的水平本身是在實施了統計檢驗之後計算的。它應該是屬於觀察數據的固有性質。 Here the level of significance is a property of the data. \\(\\alpha\\) 和 \\(\\beta\\) 作爲統計檢驗的第一類錯誤和第二類錯誤指標，應該在實施統計檢驗之前被決定。所以 \\(\\alpha, \\beta\\) 是屬於統計檢驗的性質。 Yet, to determine \\(\\alpha, \\beta\\) no convention is required, but rather a cost-benefit estimation of the severity of the two kinds of error. 隨着马尔科夫蒙特卡洛 (Markov-Chain Monte Carlo, MCMC) 法的廣泛應用，貝葉斯統計學在事後概率計算上（計算量超大的）棘手問題，得到了解決。 32.4 練習題 從經典概率論的角度，準確定義 \\(95\\%\\) 信賴區間。思考，在貝葉斯統計理論中，它會如何被定義。 解 概率論： 對於一個總體參數 \\(\\theta\\) 來說，\\(95\\%\\) 信賴區間是一個從觀察數據中計算得到的數值區間。如果重複相同的實驗無數次，我們從無數個觀察數據中計算這個區間，那麼這些無數多的信賴區間 (confidence interval, CI) 裏有 \\(95\\%\\) 包含了總體參數 \\(\\theta\\)。 貝葉斯： 對於一組觀察數據，它可以計算獲得可信區間 (credible interval, CI)。如果使用 \\(L, U\\) 分別表示下限和上限的值，\\(\\theta\\) 表示參數，\\(x\\) 表示觀察數據，\\(\\pi(\\theta|x)\\) 表示事後概率分佈的密度方程， posterior distribution。那麼有： \\[\\text{Pr}(\\theta \\in (L,U)) = \\int_L^U\\pi(\\theta|x)\\text{d}\\theta = 95\\%\\] 即，在貝葉斯理論下，95% 可信區間就是這一個區間包含了參數的概率是95%。 證明貝葉斯定理。 並且用二項分佈隨機變量的例子來證明：\\(\\text{posterior odds} = \\text{prior odds}\\times\\text{likelihood ratio}\\) 用前面提到的 HIV 的案例來說明這個公式的實際應用。 解 參照上面的標記法： \\(\\theta\\) 表示參數 \\(x\\) 表示觀察數據 \\(\\pi(\\theta|x)\\) 表示事後概率分佈的密度方程， posterior distribution \\(f(\\theta,x)\\) 表示參數和數據的聯合分佈， joint distribution \\(f(x)\\) 表示先驗概率分佈的密度方程， prior distribution \\[ \\begin{aligned} \\pi(\\theta|x) &amp;= \\frac{f(\\theta, x)}{f(x)} \\\\ &amp;=\\frac{f(\\theta, x)}{f(x)}\\cdot\\frac{1/\\pi(\\theta)}{1/\\pi(\\theta)} \\\\ &amp;=\\frac{\\frac{f(\\theta,x)}{\\pi(\\theta)}}{\\frac{f(x)}{\\pi(\\theta)}} \\end{aligned} \\] 其中分子部分 \\(\\frac{f(\\theta,x)}{\\pi(\\theta)}\\) 就是條件概率 \\(f(x|\\theta)\\)。 分母的 \\(f(x)\\) 部分 \\[ \\begin{aligned} f(x) &amp;= \\int f(x,\\theta) \\text{d}\\theta \\\\ &amp;= \\int \\frac{f(x,\\theta)}{\\pi(\\theta)} \\cdot \\pi(\\theta) \\text{d}\\theta \\\\ &amp;= \\int f(x|\\theta) \\cdot \\pi(\\theta) \\text{d}\\theta \\end{aligned} \\] 所以， \\[\\pi(\\theta|x)=\\frac{f(x|\\theta)\\pi(\\theta)}{\\int f(x|\\theta) \\cdot \\pi(\\theta) \\text{d}\\theta}\\] 用二項分佈隨機變量 (\\(\\theta=1, 0\\)) 來證明：\\(\\text{posterior odds} = \\text{prior odds}\\times\\text{likelihood ratio}\\) 解 假設 \\(\\theta\\) 是一個二項分佈的隨機變量，那麼 \\(f(\\theta|x)=\\text{Pr}(\\theta |x)\\)。 \\[ \\begin{aligned} \\text{posterior odds} &amp;= \\frac{\\text{Pr}(\\theta=1|x)}{\\text{Pr}(\\theta=0|x)} \\\\ &amp;= \\frac{\\frac{\\text{Pr}(x|\\theta=1)\\text{Pr}(\\theta=1)}{\\text{Pr}(x)}}{\\frac{\\text{Pr}(x|\\theta=0)\\text{Pr}(\\theta=0)}{\\text{Pr}(x)}}\\\\ &amp;=\\frac{\\text{Pr}(\\theta=1)}{\\text{Pr}(\\theta=0)}\\cdot\\frac{\\text{Pr}(x|\\theta=1)}{\\text{Pr}(x|\\theta=0)} \\\\ &amp;=\\text{prior odds}\\times\\text{likelihood ratio} \\end{aligned} \\] 用前面提到的 HIV 案例來驗證： HIV的患病率爲 \\(1/1000\\)，所以 \\(\\text{prior odds}=1:999\\)，似然比 \\(\\text{likelihood ratio}=0.99:(1-0.98)\\)。所以就有： \\[ \\begin{aligned} \\text{posterior odds} &amp;=\\text{prior odds}\\times\\text{likelihood} \\\\ &amp;= \\frac{1}{999}\\times\\frac{0.99}{1-0.98} \\\\ &amp;= \\frac{0.99}{19.98} \\\\ &amp;= \\frac{1}{20.18182} \\end{aligned} \\] 所以事後概率（陽性結果患病的概率）爲 \\(1/(1+20.18182)=0.0472\\)。 史密斯先生有2個孩子，其中之一是男孩。另一個孩子是女孩的概率是多少？ 如下前提默認成立： 男女比例爲: 50-50。 這個家庭中沒有對男孩或者女孩的偏好。 這兩個孩子不是同胞雙胞胎。 一個家庭有兩個孩子的性別組合的所有可能性： 第一個孩子性別 第二個孩子性別 男孩 男孩 男孩 女孩 女孩 男孩 女孩 女孩 所以根據已知條件，其中之一是男孩，所以最後一種情況：“兩個女孩” 是不可能的。故另一孩子是女孩的概率就是 \\(\\frac{2}{3}\\)。 如果用貝葉斯理論來正式計算的話： \\[ \\begin{aligned} &amp; \\text{Pr (1 girl in family of 2 | family does not have 2 girls)} \\\\ &amp;= \\frac{\\text{Pr(family doesn&#39;t have 2 girls|1 girl in a family of 2)}\\times \\\\ \\text{Pr(1 girl in a family of 2 )}}{\\sum_{j=0,1,2}\\text{Pr(family doesn&#39;t have 2 girls|1 girl in a family of 2)}\\times\\\\\\text{Pr(j girl in a family of 2)}} \\\\ &amp;= \\frac{1\\times\\frac{1}{2}}{1\\times\\frac{1}{4}+1\\times\\frac{1}{2}+0\\times\\frac{1}{4}} \\\\ &amp;= \\frac{\\frac{1}{2}}{\\frac{3}{4}}=\\frac{2}{3} \\end{aligned} \\] 也是一樣的結論。 下表是全國普查以後得出的家庭有兩個孩子，且至少一個是男孩的數據分佈： 第一個孩子性別 第二個孩子性別 家庭數量 男孩 男孩 657 男孩 女孩 591 女孩 男孩 610 女孩 女孩 0 求同樣的概率問題： 解 另一個孩子是女孩的概率是：\\(\\frac{610+591}{610+591+657}=0.646\\) 参考文献 "],
["section-33.html", "第 33 章 貝葉斯定理的應用：單一參數模型 33.1 貝葉斯理論下的事後二項分佈概率密度方程 notation for probability density functions 33.2 \\(\\theta\\) 的先驗概率 33.3 練習題", " 第 33 章 貝葉斯定理的應用：單一參數模型 從前一章節我們可以深切體會到，貝葉斯統計是如何讓我們的先驗概率，在觀察到數據之後，更新信息，獲得事後概率 (這是一個通過數據自我學習，進化的過程)。(How a prior belief about an event can be updated, given data, to a posterior belief.) 所以說，在貝葉斯模型中，我們期待使用觀察數據來學習，以增加現有的對相關參數的知識和信息。本章我們把重點放在二項分佈，用二項分佈作爲單一參數模型來瞭解怎樣推導事後分佈。 33.1 貝葉斯理論下的事後二項分佈概率密度方程 notation for probability density functions \\(R\\) 用來表示服從一個二項分佈的隨機變量， \\(R\\sim Bin(n, \\theta)\\)。 \\(r\\) 表示觀察到 \\(r\\) 次成功實驗，實驗次數爲 \\(n\\)。 先驗概率分佈： \\(\\pi_\\Theta(\\theta)\\) 應用貝葉斯定理： \\[ \\begin{aligned} \\pi_{\\Theta|R}(\\theta|r) &amp;= \\frac{f_R(r|\\theta)\\pi_\\Theta(\\theta)}{\\int_0^1f_R(r|\\theta)\\pi_\\Theta(\\theta)\\text{ d}\\theta}\\\\ &amp;= \\frac{f_R(r|\\theta)\\pi_\\Theta(\\theta)}{f_R(r)} \\end{aligned} \\] 如果我們的先驗概率分佈： \\[\\begin{equation} \\pi_\\Theta(\\theta)=\\begin{cases} 1 \\text{ if } \\theta=0.2\\\\ 0 \\text{ otherwise} \\end{cases} \\end{equation}\\] 意思就是，我們 100% 相信 \\(\\theta\\) 絕對就等於 0.2，不相信 \\(\\theta\\) 竟然還能取任何其他值（霸道自大又狂妄的我們）。 如果先驗概率分佈： \\[\\begin{equation} \\pi_\\Theta(\\theta)=\\begin{cases} 0.4 \\text{ if } \\theta=0.2\\\\ 0.6 \\text{ if } \\theta=0.7 \\end{cases} \\end{equation}\\] 意思就是，我們有 60% 的把握相信 \\(\\theta=0.7\\)，有 40% 的把握相信 \\(\\theta=0.2\\)，稍微傾向於 \\(\\theta=0.7\\)。 假設進行10次實驗，觀察到3次成功。當 \\(\\theta=0.2\\) 時，觀察數據的似然 (likelihood) 爲： \\[f_R(3|\\theta=0.2)=\\binom{10}{3}0.2^3(1-0.2)^7\\] 當 \\(\\theta=0.7\\) 時，觀察數據的似然爲： \\[f_R(3|\\theta=0.7)=\\binom{10}{3}0.7^3(1-0.7)^7\\] 應用貝葉斯定理計算事後概率分佈： \\[\\begin{equation} \\pi_{\\Theta|R}(\\theta|3)=\\begin{cases} \\frac{\\binom{10}{3}0.2^3(1-0.2)^7\\times0.4}{\\binom{10}{3}0.2^3(1-0.2)^7\\times0.4+\\binom{10}{3}0.7^3(1-0.7)^7\\times0.6}=0.937 \\text{ if } \\theta=0.2\\\\ \\frac{\\binom{10}{3}0.7^3(1-0.7)^7\\times0.6}{\\binom{10}{3}0.7^3(1-0.7)^7\\times0.6+\\binom{10}{3}0.2^3(1-0.2)^7\\times0.4}=0.063 \\text{ if }\\theta=0.7 \\end{cases} \\end{equation}\\] 所以，我們從一開始認爲只有40%的把握相信 \\(\\theta=0.2\\)，觀察數據告訴我們 10 次實驗，3次獲得了成功。所以我們現在有 93.7% 的把握相信 \\(\\theta=0.2\\)。也就是說，觀察數據讓我們對參數 \\(\\theta\\) 的取值可能性發生了質的變化，從原先的傾向於 \\(\\theta=0.7\\) 到現在幾乎接近 100% 的認爲 \\(\\theta=0.2\\)。也就是，觀察數據獲得的信息改變了我們的立場。 上面的例子很直觀，但是有下面幾個問題： 如果我們無法對參數 \\(\\theta\\) 賦予先驗概率的點估計時，該怎麼辦？ 如果事後概率不是一個離散的分佈時，該如何才能表達事後概率？ 33.2 \\(\\theta\\) 的先驗概率 一種選擇是，我們用均一分佈 (uniform distribution)，即我們對數據一無所知，認爲所有的 \\(\\theta\\) 的可能性都一樣，概率密度方程爲 \\(1\\)。在這一情況下，先驗概率爲 1： \\(\\pi_\\Theta(\\theta)=1\\)，其事後概率分佈爲： \\[ \\begin{equation} \\pi_{\\Theta|R}(\\theta|r)=\\frac{\\binom{n}{r}\\theta^r(1-\\theta)^{n-r}}{\\int_0^1\\binom{n}{r}\\theta^r(1-\\theta)^{n-r} \\text{ d}\\theta} \\tag{33.1} \\end{equation} \\] 看到即使在如此簡單的先驗概率下，我們還是要使用複雜的微積分進行計算。幸運的是，像 (33.1) 的分母這樣的積分公式其實是有跡可循的。這就是 beta (\\(\\beta\\)) 分佈。 33.2.1 beta 分佈 the beta distribution 图 33.1: Beta distribution functions for various values of a, b 我們定義 \\(a&gt;0\\) 時伽馬方程爲 \\[\\Gamma(a)=\\int_0^\\infty x^{a-1}e^{-ax}\\text{ d}x\\] 當 \\(a\\) 取正整數時， \\(\\Gamma(a)\\) 是 \\((a-1)!\\)。例如，當 \\(a=4, \\Gamma(a)=3\\times2\\times1=6\\)。 對於 \\(\\theta\\in[0,1]\\) 時，beta 方程 \\(Beta(a,b)\\) 被定義爲： \\[ \\begin{aligned} \\pi\\Theta(\\theta|a,b) &amp;= \\theta^{a-1}(1-\\theta)^{b-1}\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\\\ &amp;= \\frac{\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)} \\end{aligned} \\] 其中 \\[B(a,b)=\\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}\\] 莫要混淆 B 方程和 Beta 方程。 利用 Beta 方程作爲前概率顯得十分便捷且靈活。圖 33.1 展示的是 6 種不同的 \\((a,b)\\) 取值下的先驗概率分佈示意圖。其實我們可以看到，包括均一分佈在內的各種可能性都可以通過 Beta 分佈實現。其中 \\((a,b)\\) 被叫做超參數 (hyperparameter)。\\((a,b)\\) 取值越大，先驗概率分佈的方差越小。 關於 Beta 分佈的幾個性質： 均值：\\(\\text{mean}=\\frac{a}{a+b}\\)； 衆數：\\(\\text{mode}=\\frac{a-1}{a+b-2}\\)； 方差：\\(\\text{variance}=\\frac{ab}{(a+b)^2(a+b+1)}\\)。 回到均一分佈的簡單例子 (33.1) 上： \\(\\pi_\\Theta(\\theta)=Beta(1,1)\\) 是 \\(\\theta\\in[0,1]\\) 上的均一分佈。所以事後概率 posterior 和下面的式子成正比： \\[\\theta^r(1-\\theta)^{n-r}\\] 換句話說，事後概率分佈服從 \\(Beta(r+1,n-r+1)\\)，均值爲 \\(\\frac{r+1}{n+2}\\)，方差爲 \\(\\frac{(1+r)(n-r+1)}{(n+2)^2(n+3)}\\)。 由此可見，在貝葉斯統計思維下，先驗概率爲均一分佈的二項分佈數據，其事後概率分佈的均值和方差，和經典概率論下的極大似然估計 \\(r/n\\) 不同，和它的漸進樣本方差 \\(r(n-r)/n^3\\) 也不同。但是，當 \\(n\\) 越來越大，獲得的觀察數據越多提供的信息越來越多以後，我們會發現事後概率分佈的均值和方差也會越來越趨近於經典概率論下的極大似然估計和它的方差。 於是這裏可以總結以下兩點： 即使先驗概率對參數毫無用處（不能提供有效信息，或者我們對所觀察的數據一無所知），也可能會對事後概率分佈結果提供一些意外的信息。 當樣本量增加，似然就主導了整個貝葉斯方程，在數學計算上，經典概率論和貝葉斯推理的估計結果將會十分接近。當然，其各自的意義還是截然不同的。 33.2.2 二項分佈數據事後概率分佈的一般化：共軛性 當 \\(r\\sim \\text{Binomial}(n,\\theta)\\) 時，如果先驗概率 \\(\\pi_\\Theta(\\theta)=\\text{Beta}(a,b)\\)。那麼參數 \\(\\theta\\) 的事後概率分佈的密度方程滿足： \\[\\pi_{\\Theta|r}(\\theta|r)=\\text{Beta}(a+r, b+n-r)\\] 它的事後概率分佈均值爲： \\[E[\\theta|r]=\\frac{a+r}{a+b+n}\\] 事後概率分佈的衆數爲： \\[\\text{Mode}[\\theta|r]=\\frac{a+r-1}{n+a+b-2}\\] 事後概率分佈方差爲： \\[\\text{Var}[\\theta|r]=\\frac{(a+r)(b+n-r)}{(a+b+n)^2(a+b+n+1)}\\] 因此，我們看到先驗概率服從 \\(\\text{Beta}(a,b)\\) 分佈，觀察數據爲二項分佈時，事後概率分佈還是服從 \\(\\text{Beta}\\) 分佈，僅僅只是超參數發生了轉變（更新）。這就是共軛分佈的實例。\\(\\text{Beta}\\) 分佈是二項分佈的共軛先驗概率分佈 (the \\(\\text{Beta}(a,b)\\) is the conjugate prior for the binomial likelihood)。 在經典概率論的框架下，參數 \\(\\theta\\) 的估計就是極大似然估計 (MLE)。在二項分佈的例子中， \\(\\text{MLE}=\\hat\\theta=r/n\\)，當樣本量 \\(n\\rightarrow\\infty\\) 時，事後概率分佈均值： \\[E[\\theta|r]=\\frac{a+r}{a+b+n}=\\frac{\\frac{r}{n}+\\frac{a}{n}}{1+\\frac{a+b}{n}}\\approx\\frac{r}{n}=\\text{MLE}\\] 事後概率分佈的衆數爲： \\[ \\begin{aligned} \\text{Mode}[\\theta|r] &amp;=\\frac{a+r-1}{n+a+b-2} \\\\ &amp;= \\frac{\\frac{r}{n}+\\frac{a-1}{n}}{1+\\frac{a+b-2}{n}}\\\\ &amp;\\approx \\frac{r}{n} \\end{aligned} \\] 事後概率分佈的方差爲： \\[\\frac{(a+r)(b+n+r)}{(a+b+n)^2(a+b+n+1)}\\approx0\\] 當 \\(n\\)，樣本越來越大時，我們獲得更多的來自數據的信息，所以來自數據的信息逐漸主導 (dominate) 了整個貝葉斯推斷的過程，事後均值等衆多統計結果都越來越趨近於概率論統計思想下的極大似然估計等結論。 我們也可以注意到，當 \\(a\\rightarrow0, b\\rightarrow0\\) 時，事後概率分佈的均值 \\(E[\\theta|r] = \\frac{a+r}{a+b+n} \\rightarrow \\frac{r}{n}\\)，方差也趨向於樣本漸進方差 (asymptotic sample variance)。但是當 \\(a\\rightarrow 0, b\\rightarrow0\\) 時，先驗概率是沒有被定義的，可是此例下事後概率卻可以正常被定義。所以當先驗概率分佈無法被定義，或者被定義的不恰當時，事後概率分佈依然不受太大影響。所以特別是對於均值（或迴歸係數，regression coefficients）等參數，我們常常會使用均一分佈這樣的無信息先驗概率。 33.3 練習題 33.3.1 Q1 當先驗概率分佈服從 \\(\\text{Beta}(0.5,0.5)\\)，觀察數據記錄到 \\(5\\) 個患者中 \\(3\\) 人死亡的事件。 試求： 死亡發生概率 \\(\\theta\\) 的 95% 可信區間 (credible intervals)。 解 根據 Section 33.2.2 的公式，當先驗概率爲 \\(\\pi_{\\Theta|r}(\\theta|r)=\\text{Beta}(a=0.5,b=0.5)\\) ，數據 \\(n=5, r=3\\)。參數 \\(\\theta\\) 的事後概率分佈 \\(\\pi_{\\Theta|r}(\\theta|r)=\\text{Beta}(a+r,b+n-r)=\\text{Beta}(3.5,2.5)\\)。 在 R 裏進行貝葉斯計算十分簡便： # 95% Credible Intervals L &lt;- qbeta(0.025, 3.5, 2.5) U &lt;- qbeta(0.975, 3.5, 2.5) print(c(L,U)) ## [1] 0.20941666 0.90560967 事後分佈 \\(\\pi_{\\Theta|r}(\\theta|r)=\\text{Beta}(3.5,2.5)\\) 的分佈圖形如下： post &lt;- Vectorize(function(theta) dbeta(theta, 3.5, 2.5)) # Illustration x &lt;- seq(0,1,length=10000) y &lt;- post(x) plot(x,y, type = &quot;l&quot;, xlab=~theta, ylab=&quot;Density&quot;, lwd=2, frame.plot = FALSE) polygon(c(L, x[x&gt;=L &amp; x&lt;= U], U), c(0, y[x&gt;=L &amp; x&lt;=U], 0), col=&quot;grey&quot;) 图 33.2: Posterior distribution of Beta(3.5,2.5) 我們可以自己寫一個求可信區間的公式來計算： # Credible Interval function: # a, b : shape / super parameters # level: probability level (0,1) cred.int &lt;- function(a,b,level){ L &lt;- qbeta((1-level)/2, a, b) # Lower limit U &lt;- qbeta((1+level)/2, a, b) # Upper limit return(c(L,U)) } cred.int(3.5,2.5,0.95) ## [1] 0.20941666 0.90560967 95%可信區間 \\((0.2094, 0.9056)\\) 告訴我們，參數 \\(\\theta\\in(0.2094, 0.9056)\\) 的概率是 \\(0.95\\)。 下面我們嘗試寫 Beta 分佈的其他統計量：均值，衆數，方差等。 # a, b: shape / super parameters MeanBeta &lt;- function(a,b) a/(a+b) ModeBeta &lt;- function(a,b) { m &lt;- ifelse(a&gt;1 &amp; b&gt;1, (a-1)/(a+b-2), NA) return(m) } VarianceBeta &lt;- function(a,b) (a*b)/((a+b)^2*(a+b+1)) # mean MeanBeta(3.5,2.5) ## [1] 0.58333333 # mode ModeBeta(3.5,2.5) ## [1] 0.625 # Variance VarianceBeta(3.5, 2.5) ## [1] 0.034722222 # SD sqrt(VarianceBeta(3.5, 2.5)) ## [1] 0.186339 33.3.2 Q2 假如數據還是 Q1 的數據，然而先驗概率讓我們認爲可能在 5 名受試對象中觀察到 1 次事件。 試求超參數 \\((a,b)\\) 滿足先驗概率的 Beta 分佈。(不止一組) 解 我們認爲最有可能發生 “5 名受試對象中觀察到 1 次事件” 的情況，所以先驗概率的均值爲 \\(\\frac{a}{a+b}=0.2\\)。所以，在實數中有無數組超參數都可以用來模擬先驗概率分佈。例如 \\(a=1, b=4; a=10, b=40; a=100, b=400; a=0.317, b=1.268, \\cdots\\)。 假如觀察數據是 \\(n=5, r=1\\)，計算事後概率分佈及其均值，標準差。 解 先來嘗試寫一個計算貝葉斯二項分佈的方程： # binbayes function in R #------------------------------ # a, b: shape / super parameters # r : number of successes # n : number of trials binbayes &lt;- function(a, b, r, n) { prior &lt;- c(a, b, NA, MeanBeta(a,b), sqrt(VarianceBeta(a, b)), qbeta(0.025, a, b), qbeta(0.5, a, b), qbeta(0.975, a, b)) posterior &lt;- c(a+r, b+n-r, r/n, MeanBeta(a+r, b+n-r), sqrt(VarianceBeta(a+r, b+n-r)), qbeta(0.025, a+r, b+n-r), qbeta(0.5, a+r, b+n-r), qbeta(0.975, a+r, b+n-r)) out &lt;- rbind(prior, posterior) out &lt;- round(out, 4) colnames(out) &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;r/n&quot;,&quot;Mean&quot;, &quot;SD&quot;, &quot;2.5%&quot;, &quot;50%&quot;, &quot;97.5%&quot;) return(out) } # a=1, b=4, r=1, n=5 binbayes(1,4,1,5) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 1 4 NA 0.2 0.1633 0.0063 0.1591 0.6024 ## posterior 2 8 0.2 0.2 0.1206 0.0281 0.1796 0.4825 # a=10, b=40, r=1, n=5 binbayes(10,40,1,5) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 10 40 NA 0.2 0.0560 0.1024 0.1960 0.3202 ## posterior 11 44 0.2 0.2 0.0535 0.1063 0.1963 0.3143 通過繪製先驗概率分佈圖和事後概率分佈圖來比較二者的變化： # Prior vs posterior graphs # a,b : shape / super parameters # r : number of successes # n : number of trials graph.binbayes &lt;- function(a,b,r,n) { prior &lt;- Vectorize(function(theta) dbeta(theta, a,b)) posterior &lt;- Vectorize(function(theta) dbeta(theta, a+r, b+n-r)) YL &lt;- max(prior(seq(0.001,0.999,by=0.001)),posterior(seq(0.001,0.999,by=0.001))) curve(prior, xlab=~theta, ylab=&quot;Density&quot;, lwd=1, lty=2,n=10000,ylim=c(0,YL), frame.plot = FALSE) curve(posterior, xlab=~theta,lwd=2,lty = 1, add=T,n=10000) } graph.binbayes(1,4,1,5) 图 33.3: Prior (dashed) Beta(1,4) vs. Posterior (cont.) Beta(2,8) graph.binbayes(10,40,1,5) 图 33.4: Prior (dashed) Beta(10,40) vs. Posterior (cont.) Beta(11, 44) 我們可以很清楚的看見，先驗概率相同時，\\(\\text{Beta} (a,b)\\) 的超參數如果越大，先驗概率的分佈就越趨近與對稱圖形，且極大值也就越出現在均值的地方 (本例中是 \\(0.2\\))。而且也會使事後概率的 HPD (highest posterior density) 的區間更狹窄 (意爲對事後概率的預測越準確)，同時事後概率分佈也更加接近左右對稱。 33.3.3 Q3 我們事先估計某個事件在 \\(n=20\\) 名患者中發生的概率爲 \\(15\\%\\)。當實際觀察數據爲 \\(n=15,r=3\\) 時，計算相應的事後概率。 解 # because 15% events happened in 20 subjects, assuming prior Beta(a=3, b=17) # observed n=15, r=3 binbayes(3,17,3,15) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 3 17 NA 0.1500 0.0779 0.0338 0.1383 0.3314 ## posterior 6 29 0.2 0.1714 0.0628 0.0676 0.1651 0.3106 graph.binbayes(3,17,3,15) 图 33.5: Prior (dashed) Beta(3,17) vs. Posterior (cont.) Beta(6,29) 試着繪製先驗概率服從 \\(\\text{Beta} (1,1)\\)，回憶之前本章開頭的圖 33.1，這個先驗概率的含義就是我們沒有任何背景知識，對數據完全陌生的情況： graph.binbayes(1,1,3,15) 图 33.6: Prior (dashed) Beta(1,1) vs. Posterior (cont.) Beta(4,13) 33.3.4 Q4 試給出上面各題中參數 \\(\\theta\\) 落在 \\((0.1,0.25)\\) 之間的概率。 # function to calculate probabilities in a interval # a, b: super parameters # r : number of successes # n : number of trials # L : Lower limit of the probability interval # U : Upper limit of the probability interval prob.int &lt;- function(a,b,r,n,L,U){ prior0 &lt;- pbeta(U,a,b) - pbeta(L,a,b) posterior0 &lt;- pbeta(U,a+r,n-r+b) - pbeta(L,a+r,n-r+b) prob &lt;- as.matrix(c(prior0, posterior0)) prob &lt;- round(prob,4) colnames(prob) &lt;- paste(&quot;Probability of theta lies between the Interval&quot;, L, U) rownames(prob) &lt;- c(&quot;Prior&quot;,&quot;Posterior&quot;) return(prob) } # Prior Beta(0.317,1.286) n = 5, r=1 binbayes(0.317, 1.286, 1, 5) ## a b r/n Mean SD 2.5% 50% ## prior 0.317 1.286 NA 0.1978 0.2469 0.000 0.0823 ## posterior 1.317 5.286 0.2 0.1995 0.1449 0.013 0.1684 ## 97.5% ## prior 0.8516 ## posterior 0.5493 prob.int(0.317, 1.286, 1, 5,0.1,0.25) ## Probability of theta lies between the Interval 0.1 0.25 ## Prior 0.1711 ## Posterior 0.3911 graph.binbayes(0.317, 1.286, 1, 5) 图 33.7: Prior (dashed) Beta(0.317,1.286) vs. Posterior (cont.) Beta(1.317, 5.286) # Prior Beta(10,40) n = 5, r=1 binbayes(10, 40, 1, 5) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 10 40 NA 0.2 0.0560 0.1024 0.1960 0.3202 ## posterior 11 44 0.2 0.2 0.0535 0.1063 0.1963 0.3143 prob.int(10, 40, 1, 5,0.1,0.25) ## Probability of theta lies between the Interval 0.1 0.25 ## Prior 0.7951 ## Posterior 0.8099 所以在範圍固定的時候，事後概率分佈總是能夠比先驗概率分佈給出更高的累計概率。 33.3.5 Q5 一個臨牀試驗要進行兩個階段 (two phases)，第一階段我們觀察到 \\(10\\) 個患者中 \\(1\\) 個事件。第二階段，觀察到 \\(n=50, r=5\\)。 兩個階段都使用 \\(\\text{Beta}(1,1)\\) 作先驗概率。求兩個實驗階段參數 \\(\\theta&lt;0.1\\) 的概率。 # Phase I binbayes(1, 1, 1, 10) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 1 1 NA 0.5000 0.2887 0.0250 0.500 0.9750 ## posterior 2 10 0.1 0.1667 0.1034 0.0228 0.148 0.4128 prob.int(1,1,1,10,0,0.1) ## Probability of theta lies between the Interval 0 0.1 ## Prior 0.1000 ## Posterior 0.3026 graph.binbayes(1, 1, 1, 10) 图 33.8: Prior (dashed) Beta(1,1) vs. Posterior (cont.) Beta(2, 10) # Phase II binbayes(1, 1, 5, 50) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 1 1 NA 0.5000 0.2887 0.0250 0.5000 0.9750 ## posterior 6 46 0.1 0.1154 0.0439 0.0444 0.1105 0.2141 prob.int(1,1,5,50,0,0.1) ## Probability of theta lies between the Interval 0 0.1 ## Prior 0.1000 ## Posterior 0.4024 graph.binbayes(1, 1, 5, 50) 图 33.9: Prior (dashed) Beta(1,1) vs. Posterior (cont.) Beta(6, 46) 繼續使用先驗概率分佈 \\(\\text{Beta}(1,1)\\)，合併兩個實驗階段，求此時的事後概率分佈，以及參數 \\(\\theta&lt;0.1\\) 的概率。 # Combining both phases binbayes(1, 1, 6, 60) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 1 1 NA 0.5000 0.2887 0.0250 0.5000 0.9750 ## posterior 7 55 0.1 0.1129 0.0399 0.0474 0.1087 0.2019 prob.int(1,1,6,60,0,0.1) ## Probability of theta lies between the Interval 0 0.1 ## Prior 0.1000 ## Posterior 0.4105 graph.binbayes(1, 1, 6, 60) 图 33.10: Prior (dashed) Beta(1,1) vs. Posterior (cont.) Beta(7, 55) 用第一階段的實驗結果做第二階段實驗的先驗概率分佈，再計算事後概率分佈，以及 \\(\\theta&lt;0.1\\) 的概率。 # Using Phase I results as a prior for Phase II binbayes(2, 10, 5, 50) ## a b r/n Mean SD 2.5% 50% 97.5% ## prior 2 10 NA 0.1667 0.1034 0.0228 0.1480 0.4128 ## posterior 7 55 0.1 0.1129 0.0399 0.0474 0.1087 0.2019 prob.int(2,10,5,50,0,0.1) ## Probability of theta lies between the Interval 0 0.1 ## Prior 0.3026 ## Posterior 0.4105 graph.binbayes(2, 10, 5, 50) 图 33.11: Prior (dashed) Beta(2,10) vs. Posterior (cont.) Beta(7, 55) 第2，3兩個小問題提示我們，無論是將第一階段實驗結果作爲第二階段實驗的先驗假設還是將兩次實驗合併，最終的結果是不會改變的。Both approaches are equivalent. 33.3.6 Q6 藥物 A 和藥物 B 都被批准用於治療某種疾病。在 5000 例病例中使用藥物 A，發現有 3 人發生了不良副作用。在另外 7000 例病例中使用藥物 B，發現只有 1 例發生了副作用。 先使用單一分佈作爲先驗概率 (uniform prior: \\(\\text{Beta}(1,1)\\))。求藥物 A 和藥物 B 各自發生不良反應的事後概率。 藥物 A # Drug A binbayes(1,1, 3, 5000) ## a b r/n Mean SD 2.5% 50% ## prior 1 1 NA 0.5000 0.2887 0.0250 0.5000 ## posterior 4 4998 0.0006 0.0008 0.0004 0.0002 0.0007 ## 97.5% ## prior 0.9750 ## posterior 0.0018 图 33.12: Prior (dashed) Beta(1,1) vs. Posterior (cont.) Beta(4, 4998) 藥物 B # Drug B binbayes(1,1, 1, 7000) ## a b r/n Mean SD 2.5% 50% ## prior 1 1 NA 0.5000 0.2887 0.025 0.5000 ## posterior 2 7000 0.0001 0.0003 0.0002 0.000 0.0002 ## 97.5% ## prior 0.9750 ## posterior 0.0008 图 33.13: Prior (dashed) Beta(1,1) vs. Posterior (cont.) Beta(2, 7000) 使用 \\(\\text{Beta}(0.00001,0.00001)\\) 作爲先驗概率，重複上面的計算 藥物 A # Drug A binbayes(0.00001, 0.00001, 3, 5000) ## a b r/n Mean SD 2.5% 50% ## prior 0 0 NA 0.5000 0.5000 0.0000 0.5000 ## posterior 3 4997 0.0006 0.0006 0.0003 0.0001 0.0005 ## 97.5% ## prior 1.0000 ## posterior 0.0014 图 33.14: Prior (dashed) Beta(0.00001,0.00001) vs. Posterior (cont.) Beta(3, 4997) 藥物 B # Drug B binbayes(0.00001, 0.00001, 1, 7000) ## a b r/n Mean SD 2.5% 50% ## prior 0 0 NA 0.5000 0.5000 0 0.5000 ## posterior 1 6999 0.0001 0.0001 0.0001 0 0.0001 ## 97.5% ## prior 1.0000 ## posterior 0.0005 图 33.15: Prior (dashed) Beta(0.00001,0.00001) vs. Posterior (cont.) Beta(1, 6999) 現在使用概率論的計算信賴區間 (confidence intervals) 的方法，求上面數據的精確二項分佈 95% 信賴區間。之前兩問中使用的哪個先驗概率更加接近概率論算法？ #------------------------------------------------ # Binomial confidence intervals #------------------------------------------------ # r : number of successes # n : number of trials # level: confidence level binom.confint &lt;- function(r,n,level){ p &lt;- r/n conf &lt;- as.vector(binom.test(r,n,conf.level = 0.95)$conf.int) out &lt;- c(p,conf) out &lt;- as.matrix(t(round(out,8))) colnames(out) &lt;- c(&quot;MLE&quot;, &quot;L&quot;, &quot;U&quot;) return(out) } # Drug A binom.confint(3,5000,0.95) ## MLE L U ## [1,] 0.0006 0.00012375 0.00175244 # Drug B binom.confint(1,7000,0.95) ## MLE L U ## [1,] 0.00014286 3.62e-06 0.00079569 明顯可以看到，先驗概率使用 \\(\\text{Beta}(0.00001,0.00001)\\) 時，事後概率的均值和可信區間的下限值更接近概率論算法。使用先驗概率 \\(\\text{Beta}(1,1)\\) 時，事後概率的可信區間的上限值更接近概率論算法。 如果需要你來下結論說，藥物 B 和藥物 A 哪個更加安全？ 求 \\(\\text{Pr}(\\theta_B &lt; \\theta_A|data)\\)。 解 貝葉斯 在計算機的輔助下，這是一個十分簡單的計算。我們從各自的事後分佈中採集大量隨機樣本，然後求 \\(\\theta_B-\\theta_A\\) 然後看有多少比例這個數值是小於零的就可以得出結論： # Simulating from each posterior set.seed(1001) post.thetaA &lt;- rbeta(1000000, 3, 4997) post.thetaB &lt;- rbeta(1000000, 1, 6999) # Taking the differences theta.diff0 &lt;- post.thetaB - post.thetaA # Histogram of the differences hist(theta.diff0,probability = TRUE, breaks = 50, xlab=expression(theta[B] - theta[A]),main = &quot;&quot;) abline(v=0, col=&quot;red&quot;, lwd=2) box() 图 33.16: Histogram of Drug B - Drug A 也可以不採用直方圖而是使用連續曲線： # Continuous version plot(density(theta.diff0), xlab=expression(theta[B] - theta[A]), lwd=2, main = &quot;&quot;, frame=FALSE) abline(v=0, col=&quot;red&quot;, lwd=2) box() 图 33.17: Density of Drug B - Drug A 計算 \\(\\text{Pr}(\\theta_B &lt; \\theta_A|data)\\) 和可信區間： # P(theta[B] &lt; theta[A] | Data) mean(theta.diff0 &lt;0) ## [1] 0.927829 # Credible interval for theta[B] - theta[A] quantile(theta.diff0, c(0.05,0.95)) ## 5% 95% ## -0.001142862872 0.000052484912 quantile(theta.diff0, c(0.10,0.90)) ## 10% 90% ## -0.00094605885 -0.00004674857 # Simulating from each posterior set.seed(1001) post.thetaA &lt;- rbeta(1000000, 4, 4998) post.thetaB &lt;- rbeta(1000000, 2, 7000) # Taking the differences theta.diff1 &lt;- post.thetaB - post.thetaA hist(theta.diff1,probability = TRUE, breaks = 50, xlab=expression(theta[B] - theta[A]),main = &quot;&quot;) abline(v=0, col=&quot;red&quot;, lwd=2) box() 图 33.18: Histogram of Drug B - Drug A # Continuous version plot(density(theta.diff1), xlab=expression(theta[B] - theta[A]), lwd=2,main = &quot;&quot;) abline(v=0, col=&quot;red&quot;, lwd=2) box() 图 33.19: Density of Drug B - Drug A 計算 \\(\\text{Pr}(\\theta_B &lt; \\theta_A|data)\\) 和可信區間： # P(theta[B] &lt; theta[A] | Data) mean(theta.diff1 &lt;0) ## [1] 0.899677 # Credible interval for theta[B] - theta[A] quantile(theta.diff1, c(0.05,0.95)) ## 5% 95% ## -0.00131289334 0.00013423126 quantile(theta.diff1, c(0.10,0.90)) ## 10% 90% ## -1.0955252e-03 6.5146438e-07 概率論算法 # Normal Approximation diff_mle &lt;- (1/7000)-(3/5000) diff_se &lt;- sqrt( 1*6999/7000^3 + 3*4997/5000^3 ) U &lt;- diff_mle + 1.28*diff_se L &lt;- diff_mle - 1.28*diff_se print(c(U,L)) ## [1] 0.000022358961 -0.000936644675 norm.app &lt;- Vectorize(function(x) dnorm(x,diff_mle,diff_se)) # Comparison plot(density(theta.diff0), xlab=expression(theta[B] - theta[A]), xlim= c(-0.002,0.001), lwd=2, col=&quot;red&quot;,main = &quot;&quot;) points(density(theta.diff1), xlab=expression(theta[B] - theta[A]), type = &quot;l&quot;, col = &quot;blue&quot;, lwd=2) curve(norm.app,-0.0045,0.002,add=T,lwd=2,n=10000) legend(-0.0021, 1300, c(&quot;Posterior with B(0,0)&quot;,&quot;Posterior with B(1,1)&quot;,&quot;Frequentist Normal App&quot;), col=c(&quot;red&quot;,&quot;blue&quot;,&quot;black&quot;), text.col = &quot;black&quot;, lty = c(1, 1, 1), lwd = c(2,2,2), merge = TRUE, bg = &quot;gray90&quot;,cex=0.8) box() 图 33.20: Comparison of different prior distribution and frequentist approximation "],
["references.html", "参考文献", " 参考文献 "]
]
