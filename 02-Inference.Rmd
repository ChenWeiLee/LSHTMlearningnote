# (PART) 統計推斷 Inference {-}

# 統計推斷的概念

## 人羣與樣本 (population and sample)

討論樣本時，需考慮下面幾個問題：

1. 樣本是否具有代表性？
2. 人羣被準確定義了嗎？
3. 我們感興趣的“人羣”是否可以是無限大 (多) 的？
4. 我們研究的樣本，是僅僅用來觀察，亦或是計劃對之進行某種干預呢？
5. 我們從所有可能的人羣中抽樣了嗎？


## 樣本和統計量 (sample and statistic)

通常我們在進行實驗或觀察時只是獲得了樣本的數據。而希望從樣本數據去推斷 (inference) 總體 (或人羣) 的一些特徵。我們也許只是想用樣本的平均值來估計整體人羣的某個特徵的平均值。不管是何種估計和推斷，都是基於對樣本數據的計算，從樣本中獲得想要推斷總體的**統計量 (statistics)**。我們用已知樣本去推斷未知總體的過程就叫做**估計 (estimate)**。這個想要被推斷的總體或人羣的值，被叫做**參數 (parameter)**，常常使用希臘字母來標記。用來估計總體或人羣的，從樣本數據計算得來的統計量，叫做**估計量 (estimator)**。

所有的統計量，都有**樣本分佈 (sampling distributions，意爲重複無限次取樣後獲得的無限次統計量的分佈)**。推斷的過程歸納如下：

1. 從總體或人羣中抽樣 (樣本量 $n$)
2. 計算這個樣本的合適統計量，從而用於估計它在整體或人羣中的值。
3. 我們還需要決定計算獲得的統計量的樣本分佈 (假定會抽樣無數次) 。
4. 一旦可以精確地確認樣本分佈，我們就可以定量地計算出使用步驟2中獲得的統計量估計總體或人羣的參數時的準確度。

## 估計 Estimation

從樣本的均值，推斷總體或人羣的均值是一種估計。我們的目的是，從已知樣本中計算一個儘可能接近那個未知的總體或人羣參數的值。一個估計量有兩個與生俱來的性質 (properties)：1) 偏倚 (bias); 2) 精確度 (precision)。這兩個性質都可以從樣本分佈和估計量獲得。

1. 偏倚： 偏倚簡單說就是樣本分佈的均值，也就是我們從樣本中計算獲得的估計量，和我們想要拿它來估計的總體或人羣的參數之間的差距。(The bias is the difference between the mean of the sampling distribution -- the expected or average value of the estimator -- and the population parameter being estimated.) 一個小的偏倚，確保了我們從樣本中計算獲得的估計值 (假設我們抽樣無數次，計算無數個樣本估計值) **均勻地**分佈在總體或人羣參數的左右兩邊。偏倚本身並不是太大的問題，但是假如樣本量增加，偏倚依然存在 (估計量不一致, inconsistent) ，那常常意味着是抽樣過程出現了問題。例如：<br>用簡單隨機抽樣法獲得的樣本均值，就是總體或人羣均值的無偏估計 (unbiased estimator)。如果抽樣時由於某些主觀客觀的原因導致較小的樣本很少被抽樣 (抽樣過程出了問題，脫離了簡單隨機抽樣原則) ，那麼此時得到的樣本均值就會是一個過高的估計值 (upward biased estimator)。

2. 精確度：估計值的精確度可以通過樣本分佈的方差或標準差來評價 (簡單說是樣本分佈的方差越低，波動越小，精確度越高) 。樣本分佈的標準差被定義爲估計值的標準誤。假如估計量是樣本均值，那麼樣本分佈的標準差 (估計量的標準誤) 和樣本數據之間有如下的關係：

$$true\; stantdard\; error\;of\;the\;mean  = \frac{true\;standard\;deviation}{\sqrt{sample\;size}}$$


在一些簡單的情況下，通常估計值的選用不言自明 (例如均值，或者百分比) 。但是在複雜的情況下，我們可能可以有多個不同類型的估計量可以選擇，他們也常常各有利弊，需要我們做出取捨。

## 信賴區間  confidence intervals

從樣本中計算估計量獲得的一個估計值，只是一個**點估計 (point estimate)**。對比之下，信賴區間就是一個對這個點估計的精確度的體現。信賴區間越窄，說明我們對於總體或人羣的參數的可能取值的範圍估計越精確。

信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。**每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平 ($95\%$) 。** 常用的這個概率值就是 $95\%, 90\%, 99\%$。

當從樣本數據計算獲得的估計量的信賴區間很寬，說明了這個收集來的數據提供了很少的參數信息，導致估計變得很不精確。




~~看到這裏的都是好漢一條啊！ 我不知道你暈了麼有，反正我是已經暈了。。。~~


# 估計和精確度 Estimation and Precision

## 估計量和他們的樣本分佈 {#CI-for-sample-mean}

例子： 最大呼氣量 (Forced Expoiratory Volume in one second, FEV1) 用於測量一個人的肺功能，它的測量值是連續的。我們從前來門診的人中隨機抽取 $n$ 人作爲樣本，用這個樣本的 FEV1 平均值來估計這個診所的患者的平均肺功能。

**模型假設：** 在這個例子中，我們的假設有如下：每個隨機抽取的 FEV1 測量值都是從同一個總體 (人羣) 中抽取，每一個觀察值 $Y_i$ 都互相獨立互不影響。我們用縮寫 iid 表示這些隨機抽取的樣本是服從獨立同分佈 (independent and identically distributed)。另外，總體的分佈也假定爲正態分佈，且總體均值爲 $\mu$，總體方差爲 $\sigma^2$。那麼這個模型可以簡單的被寫成：

$$Y_i \stackrel{i.i.d}{\sim} N(\mu, \sigma^2), i=1,2,\dots,n$$

**總體均值 $\mu$ 的估計量：** 顯然算術平均值: $\bar{Y}=\frac{1}{n}\sum_{i=1}^ny_i$ 是我們用於估計總體均值的估計量。

**估計量的樣本分佈：**
$$\bar{Y}\stackrel{i.i.d}{\sim}N(\mu, \frac{\sigma^2}{n})$$

 **證明**
$$

\begin{aligned}
E(\bar{Y}) &= E(\frac{1}{n}\sum Y_i) \\
           &= \frac{1}{n}E(\sum Y_i) \\
           &= \frac{1}{n}\sum E(Y_i) \\
           &= \frac{1}{n}n\mu = \mu \\
Var(\bar{Y}) &= Var(\frac{1}{n}\sum Y_i) \\
\because Y_i \;are &\; independent   \\
            &= \frac{1}{n^2}\sum Var(Y_i) \\
            &= \frac{1}{n^2} n Var(Y_i) \\
            &= \frac{\sigma^2}{n}
\end{aligned}
$$

**證明當 $Z=\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}}$ 時， $Z\sim N(0,1)$:**

由式子可知， $Z$ 只是由一組服從正態分佈的數據 $\bar{Y}$ 線性轉換 (linear transformation) 而來，所以 $Z$ 本身也服從正態分佈

$$
\begin{aligned}
E(Z) &= \frac{1}{\sqrt{Var(\bar{Y})}}E[\bar{Y}-\mu] \\
     &= \frac{1}{\sqrt{Var(\bar{Y})}}[\mu-\mu] = 0 \\
Var(Z) &= \frac{1}{Var(\bar{Y})}Var[\bar{Y}-\mu] \\
       &= \frac{1}{Var(\bar{Y})}Var(\bar{Y}) =1 \\
\therefore Z \;&\sim N(0,1)
\end{aligned}
$$

**均值 $\mu$ 的信賴區間：** 上節說道，

> 信賴區間通常是成對成對的出現的，即有上限和下限。這樣的一對從樣本數據中計算得來的統計量，同樣也是有樣本分佈的。**每次我們重新從總體或人羣中抽樣，計算獲得的信賴區間都不同，這些信賴區間就組成了信賴區間的樣本分佈。總體和人羣的參數落在這些信賴區間範圍內的概率，就是我們常說的信賴區間的水平($95\%$) 。** 常用的這個概率值就是 $95\%, 90\%, 99\%$。

假定我們用 $95\%$ 作爲信賴區間的水平。那麼下面我們嘗試推導一下信賴區間的計算公式。從長遠來說 (也就是假設我們從總體中抽樣無數次，每次都進行信賴區間的計算，也獲得無數個信賴區間) ，這些信賴區間中有 $95\%$ 是包含了總體的真實均值 (但是卻是未知) 的，而且這些信賴區間由於是從一個服從正態分佈的數據而來，它們也服從正態分佈 (對真實均值左右對稱) 。所以我們有理由相信，可以找到一個數值 $c$：

$$Prob(\bar{Y} > \mu+c) = 0.025 \\
  Prob(\bar{Y} < \mu-c) = 0.025$$


因此，我們可以定義 $95\%$ 信賴區間的上限和下限分別是：

$$L=\bar{Y}-c \Rightarrow Prob(L>\mu)=0.025 \\
  U=\bar{Y}+c \Rightarrow Prob(U<\mu)=0.025$$


![](img/Selection_082.png)

接下來就是推倒 (故意的) $c$ 的過程啦：

$$
\begin{aligned}
Prob(\bar{Y}>\mu+c)=Prob(\bar{Y}-\mu>c) \;&= 0.025 \\
\Rightarrow Prob(\frac{\bar{Y}-\mu}{\sqrt{Var(\bar{Y})}} > \frac{c}{\sqrt{Var(\bar{Y})}}) \;&= 0.025 \\
\Rightarrow Prob(Z>\frac{c}{\sqrt{Var(\bar{Y})}}) \;&= 0.025 \\
we\;have\;proved\; Z\sim N(0,1) \\
we\;also\;know\; Prob(Z>1.96) \;&= 0.025 \\
so\;let\; \frac{c}{\sqrt{Var(\bar{Y})}} =1.96 \\
\Rightarrow c=1.96\sqrt{Var(\bar{Y})} \\
the\;95\%\;confidence\;interval \;of\; &the\;population\;mean\;is\\
\mu = \bar{Y}\pm1.96\sqrt{Var(\bar{Y})}=\bar{Y}\pm & 1.96\frac{\sigma}{\sqrt{n}}
\end{aligned}
$$

其中，$\sqrt{Var(\bar{Y})}$ 就是我們熟知的估計量 $\bar{Y}$ 的標準誤。



## 估計量的特質

考慮以下的問題：

1. 什麼因素決定了一個估計量 (estimator) 的好壞，是否實用？
2. 如果有其他的可選擇估計量，該如何取捨呢？
3. 當情況複雜的時候，我們該如何尋找合適的估計量？

### 偏倚

假設 $T$ 是我們估計總體參數 $\theta$ 的一個估計量。一般來說我們希望估計量的樣本分佈可以在 `“正確的位置”` 左右均勻分佈。換句話說我們希望：

$$E(T)=\theta$$

如果實現了這個條件，我們說這樣的估計量是無偏的 (`unbiased`)。然而，天下哪有這等好事，我們叫真實值和估計量之間的差距爲偏倚：

$$bias(T) = E(T)-\theta$$

其實偏倚完全等於零並不是最重要，許多常見的估計量都是有偏倚的。重要的是，這個偏倚會隨着樣本量的增加而逐漸趨近於零。所以我們就可以認爲這樣的估計量是漸進無偏的 (asymptotically unbiased)：

$$T\;is\;an\;\textbf{unbiased}\;estimator\;for\;\theta\;if\;\\E(T)=\theta\\
T\;is\;an\;\textbf{asymptotically unbiased}\;estimator\;for\;\theta\;if\;\\lim_{n\rightarrow\infty}E(T)=\theta$$

### 估計量的效能 Efficiency

通常，我們希望一個估計量 (estimator) 的偏倚要小，同時，它的樣本分佈也希望能儘可能的不要波動太大。換句話說，我們還希望估計量的方差越小越好。

如果說，兩個估計量有相同的偏倚，均可以選擇來推斷總體，我們說，其中樣本分佈的方差小的那個 (波動幅度小) 的那個估計量是相對更好的。因爲樣本分佈方差越小，說明可以**更加精確的**估計總體參數。這兩個估計量的方差之比：$Var(S)/Var(T)$ 被叫做這兩個估計量的**相對效能 (relative efficiency)**。所以我們用估計量去推斷總體時，需要選用效能最高，精確度最好的估計量 **(the minimum variance unbiased estimator/an efficient estimator)**。

### 均值和中位數的相對效能

在一個服從 $N(\mu,\sigma^2)$ 正態分佈的數據中，中位數和均值是一樣的，也都同時等於總體均值參數 $\mu$。而且，樣本均數 $\bar{Y}$ 和樣本中位數 $\dot{Y}$ 都是對總體均值的無偏估計量。那麼應該選用中位數還是平均值呢？

之前證明過當 $Y_i \sim N(\mu,\sigma^2)$ 時， $Var(\bar{Y}=\sigma^2/n)$。然而，當 $n$ 較大的時候，可以證明的是：

$$Var(\dot{Y})=\frac{\pi}{2}\frac{\sigma^2}{n}\approx1.571\frac{\sigma^2}{n}$$

因此，這兩個估計量的相對效能就是：

$$\frac{Var(\dot{Y})}{Var(\bar{Y})}\approx1.571$$

所以總體是正態分佈時，平均值就是較中位數更適合用來估計總體的估計量。


### 均方差 mean square error (MSE)

兩個估計量的偏倚不同時，可以比較他們和總體參數之間的差距，這被叫做均方差, Mean Square Error (MSE)。

$$MSE(T)=E[(T-\theta)^2]$$

這裏用一個數學技巧，將式子中的估計量和總體參數之間的差，分成兩個部分：一是估計量本身的方差 ($T-E(T)$)，一是估計量的偏倚 ($E(T)-\theta$)。

$$
\begin{aligned}
MSE(T) &= E[(T-\theta)^2] \\
       &= E\{[T-E(T)+E(T)-\theta]^2\} \\
       &= E\{[T-E(T)]^2+[E(T)-\theta]^2 \\
       & \;\;\;\;\; \;\;+2[T-E(T)][E(T)-\theta]\} \\
       &= E\{[T-E(T)]^2\}+E\{[E(T)-\theta]^2\} + 0\\
       &= Var(T) + [bias(T)^2]
\end{aligned}
$$

## 總體方差的估計，自由度 {#samplevarbias}

如果 $Y_i \sim (\mu, \sigma^2)$，並不需要默認或者假定它服從正態分佈或者任何分佈。那麼它的方差我們會用：

$$V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)^2$$

**證明 $V_{\mu}$ 是 $\sigma^2$ 的無偏估計：**

$$
\begin{aligned}
V_{\mu} &= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu)^2 \\
 we\;need\;to\;prove &E(V_{\mu}) = \sigma^2 \\
\Rightarrow E(V_{\mu}) &= \frac{1}{n}\sum_{i=1}^nE(Y_i-\mu)^2 \\
        &= \frac{1}{n}\sum_{i=1}^nVar(Y_i) \\
        &= \frac{1}{n}\sum_{i=1}^n\sigma^2 \\
        &= \sigma^2
\end{aligned}
$$

然而通常情況下，我們並不知道總體的均值 $\mu$。因此，只好用樣本的均值 $\bar{Y}$ 來估計 $\mu$。所以上面的方程就變成了：

$$V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})^2$$

你如果仔細觀察認真思考，就會發現，上面這個式子是`有問題的`。這個大問題就在於，$Y_i-\bar{Y}$ 中我們忽略掉了樣本均值 $\bar{Y}$ 和總體均值 $\mu$ 之間的差 ($\bar{Y}-\mu$)。因此上面的計算式來估計總體方差時，很顯然是會低估平均平方差，從而低估了總體方差。

這裏需要引入**自由度 (degree of freedom)** 在參數估計中的概念。

字面上可以理解爲：自由度是估計過程中使用了多少互相獨立的信息。所以在上面第一個公式中：$V_{\mu}=\frac{1}{n}\sum_{i=1}^n(Y_i-\mu)^2$。所有的 $n$ 個觀察值互相獨立，不僅如此，他們還對總體均值獨立。然而在第二個我們用 $\bar{Y}$ 取代了 $\mu$ 的公式中，樣本均數則與觀察值不互相獨立。因爲**樣本均數必然總是落在觀察值的中間**。然而總體均數並不一定就會落在觀察值中間。總體均數，和觀察值之間是自由，獨立的。因此，當我們觀察到 $n-1$ 個觀察值時，剩下的最後一個觀察值，決定了樣本均值的大小。所以說，樣本均值的自由度，是 $n-1$。

所以，加入了自由度的討論，我們可以相信，用樣本估計總體的方差時，使用下面的公式將會是總體方差的無偏估計：

$$V_{n-1}=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})=\frac{n}{n-1}V_n$$


**證明**

利用上面也用到過的證明方法 -- 把樣本和總體均值之間的差分成兩部分：

$$
\begin{aligned}
V_{\mu} &= \frac{1}{n}\sum_{i=1}^n(Y_i-\mu)^2 \\
        &= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})+(\bar{Y}-\mu)]^2 \\
        &= \frac{1}{n}\sum_{i=1}^n[(Y_i-\bar{Y})^2+(\bar{Y}-\mu)^2\\
        &\;\;\;\;\;\;\;\;\;\;\;\;+2(Y_i-\bar{Y})(\bar{Y}-\mu)]\\
        &=\frac{1}{n}\sum_{i=1}^n(Y_i-\bar{Y})^2+\frac{1}{n}\sum_{i=1}^n(\bar{Y}-\mu)^2\\
        &\;\;\;\;\;\;\;\;\;\;\;\;+\frac{2}{n}(\bar{Y}-\mu)\sum_{i=1}^n(Y_i-\bar{Y}) \\
        &= V_n+(\bar{Y}-\mu)^2 \\ &\;\;\;\;\;\;\;\;\;\;\;\;(\text{note that}\;\sum_{i=1}^n(Y_i-\bar{Y})=0) \\
\Rightarrow  V_n &= V_{\mu}-(\bar{Y}-\mu)^2  \\
\therefore E(V_n)&= E(V_{\mu}) - E[(\bar{Y}-\mu)^2] \\
                 &= Var(Y)-Var(\bar{Y}) \\
                 &= \sigma^2-\frac{\sigma^2}{n} \\
                 &= \sigma^2(\frac{n-1}{n})
\end{aligned}
$$

因此，我們看見 $V_n$ 正如上面討論的那樣，是低估了總體方差的。雖然當 $n\rightarrow\infty$ 時無限接近 $\sigma^2$ 但是依然是低估了的。所以，我們可以對之進行修正：

$$
\begin{aligned}
E[\frac{n}{n-1}V_n]     &= \frac{n}{n-1}E[V_n] =\sigma^2 \\
\Rightarrow E[V_{n-1}]  &= \sigma^2
\end{aligned}
$$

## 樣本方差的樣本分佈 

 $S^2$ 常用來標記樣本方差，取代上面我們用到的 $V_{n-1}$：

 $$S^2=\frac{1}{n-1}\sum_{i=1}^n(Y_i-\bar{Y})^2$$

而且上面也證明了，$E(S^2)=\sigma^2$ 是總體方差的無偏估計。然而，要注意的是，樣本標準差 $\sqrt{S^2}$ 卻不是總體標準差 $\sigma$ 的無偏估計(因爲並不是線性變換，而是開了根號) 。

 **證明樣本標準差 $S$ 不是總體標準差 $\sigma$ 的無偏估計**

$$
\begin{aligned}
Var(S)               &=E(S^2)-[E(S)]^2 \\
\Rightarrow [E(S)]^2 &=E(S^2)-Var(S) \\
\because E(S^2)      &=\sigma^2 \\
\therefore   [E(S)]^2 &=\sigma^2-Var(S) \\
             E(S)     &=\sqrt{\sigma^2-Var(S)} \\
\end{aligned}$$

**可見樣本標準差是低估了總體標準差的。**

另外可以被證明的是：

$$\frac{n-1}{\sigma^2}S^2\sim \mathcal{X}_{n-1}^2\\
Var(S^2)=\frac{2\sigma^4}{n-1}$$

$\mathcal{X}^2_m$： 自由度爲 $m$ 的卡方分佈 (Section \@ref(chi-square-distribution))。是在圖形上向右歪曲的分佈。當自由度增加時，會越來越接近正態分佈。


# 卡方分佈 Chi-square distribution {#chi-square-distribution}

## 卡方分佈的期望和方差的證明


當 $X\sim N(0,1)$ 時， $X^2\sim \mathcal{X}_1^2$

如果 $X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)$，
那麼 $\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2$

其中： $\mathcal{X}_n^2$ 表示自由度爲 $n$ 的卡方分佈。

且 $X_m^2+X_n^2=\mathcal{X}_{m+n}^2$

## 卡方分佈的期望

$$E(X_1^2)=Var(X)+[E(X)]^2=1+0=1$$

$$\Rightarrow E(X_n^2)=n$$


## 卡方分佈的方差

$$
\begin{aligned}
Var(X_1^2) &= E(X_1^{2^2}) - E(X_1^2)^2 \\
           &= E(X_1^4)-1
\end{aligned}
$$

### 下面來求 $E(X_1^4)$

$$
\begin{aligned}
\because E(X_1) &= \int_{-\infty}^{+\infty} xf(x)dx \\
\therefore E(X_1^4) &= \int_{-\infty}^{+\infty} x^4f(x)dx
\end{aligned}$$

已知： $f(x)=\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}$ 代入上式：

$$
\begin{aligned}
E(X_1^4) &= \int_{-\infty}^{+\infty} x^4f(x)dx \\
         &= \int_{-\infty}^{+\infty} x^4\frac{1}{\sqrt{2\pi}}e^{(-\frac{x^2}{2})}dx\\
         &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^4e^{(-\frac{x^2}{2})}dx\\
         &=\frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}x^3(-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
$$

令 $u=x^3, v=e^{(-\frac{x^2}{2})},t=-\frac{x^2}{2}$
可以推導：

$$
\begin{aligned}
\frac{dv}{dx} &= \frac{dv}{dt}\frac{dt}{dx} \\
              &= e^t(-\frac{1}{2}\times2x) \\
              &= (-x)e^{(-\frac{x^2}{2})} \\
\Rightarrow dv &= (-x)e^{(-\frac{x^2}{2})}dx
\end{aligned}
$$

再代入上面的式子：


$$
\begin{aligned}
E(X_1^4) &= \frac{-1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}u\:dv \\
integrate\; &by\; parts:\\
E(X_1^4) &= \frac{-1}{\sqrt{2\pi}}\{[u\:v] \rvert_{-\infty}^{+\infty}-\int_{-\infty}^{+\infty}v\:du\} \\
&= \frac{-1}{\sqrt{2\pi}}\{[x^3e^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}v\:du\} \\
&=\frac{-1}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx^3\} \\
&=\frac{-1}{\sqrt{2\pi}}[-3\int_{-\infty}^{+\infty}x^2e^{(-\frac{x^2}{2})}dx] \\
&=\frac{-3}{\sqrt{2\pi}}[\int_{-\infty}^{+\infty}x(-x)e^{(-\frac{x^2}{2})}dx] \\
\end{aligned}
$$

再來一次分部積分：

令 $a=x,b=e^{(-\frac{x^2}{2})},d\:b = (-x)e^{(-\frac{x^2}{2})}dx$

$$
\begin{aligned}
E(X_1^4) &= \frac{-3}{\sqrt{2\pi}}\{[a\:b] \rvert_{-\infty}^{+\infty} - \int_{-\infty}^{+\infty}b\:da\} \\
&=\frac{-3}{\sqrt{2\pi}}\{[xe^{(-\frac{x^2}{2})}]\rvert_{-\infty}^{+\infty} -\int_{-\infty}^{+\infty}b\:da\} \\
&=\frac{-3}{\sqrt{2\pi}}\{0-0-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\} \\
&=\frac{-3}{\sqrt{2\pi}}[-\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx] \\
&=\frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx
\end{aligned}
$$

下面令 $I=\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx\\
\Rightarrow I^2=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy$

接下來需要用到 [座標轉換](https://www.youtube.com/watch?v=r0fv9V9GHdo)的知識，將 $x,y$ 表示的笛卡爾座標，轉換爲用角度 $\theta$ 和半徑 $r$ 表示的形式。之後的證明可以在[油管](https://www.youtube.com/watch?v=fWOGfzC3IeY)上看到，但是我還是繼續證明下去。


直角座標系 (cartesian coordinators) 和
極座標系 (polar coordinators) 之間轉換的關係如下：


$$
\begin{aligned}
x&=r\:cos\theta\\
y&=r\:sin\theta\\
r^2&=x^2+y^2\\
\end{aligned}
$$

座標轉換以後可以繼續求 $E(X_1^4)$。 在那之前我們先求 $I^2$。
注意轉換座標系統以後，$\theta\in[0,2\pi], r\in[0,+\infty]$

$$
\begin{aligned}
I^2 &= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}e^{(-\frac{x^2+y^2}{2})}dxdy \\
&= \int_{0}^{+\infty}\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta dr \\
\end{aligned}
$$

由於先從中間的 $\int_{0}^{2\pi}e^{(-\frac{r^2}{2})}rd\theta$ 開始積分，$\theta$ 以外都可以視爲常數，那麼這個 $[0,2\pi]$ 上的積分就的等於 $2\pi e^{(-\frac{r^2}{2})}r$。

因此上面的式子又變爲：


$$
\begin{aligned}
I^2 &=  2\pi\int_{0}^{+\infty}e^{(-\frac{r^2}{2})}r\:dr \\
\because \frac{d(e^{\frac{-r^2}{2}})}{dr} &= -e^{(-\frac{r^2}{2})}r \\
\therefore I^2 &= 2\pi(-e^{\frac{-r^2}{2}})\rvert_0^{+\infty} \\
               &= 0-(2\pi\times(-1)) \\
               &= 2\pi\\
\Rightarrow I  &= \sqrt{2\pi}
\end{aligned}
$$

所以，


$$
\begin{aligned}
E(X_1^4) &= \frac{3}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{(-\frac{x^2}{2})}dx \\
&= \frac{3}{\sqrt{2\pi}}\times I \\
&= 3 \\
\Rightarrow Var(X_1^2) &= E(X_1^4) - 1 \\
                       &= 3-1 =2
\end{aligned}
$$

## 把上面的推導擴展

$$
\text{Suppose } \mathcal{X}^2_1, \cdots \mathcal{X}^2_k \stackrel{i.i.d}{\sim} \mathcal{X}^2_1 \\
\Rightarrow \sum_{i=1}^k \mathcal{X}^2_i \sim \mathcal{X}^2_k \\
\Rightarrow \text{E}(\sum_{i=1}^n\mathcal{X}^2_i)=\sum_{i-1}^n\text{E}(\mathcal{X}^2_i)=n\times1=n\\
\text{Var}(\sum_{i=1}^n\mathcal{X}^2_i)=\sum_{i=1}^n\text{Var}(\mathcal{X}^2_i) = n\times2=2n
$$


結論：$X_1, \dots, X_n\stackrel{i.i.d}{\sim} N(0,1)$ 時，$\sum_{i=1}^nX_i^2\sim\mathcal{X}_n^2$ 服從卡方分佈，其期望 $E(X_n^2)=n$，方差 $Var(X_n^2)=2n$。
根據**中心極限定理**(Section \@ref(CLT))

$$n\rightarrow \infty, X_n^2\sim N(n, 2n)$$


# 似然 Likelihood {#likelihood-definition}

## 概率 vs. 推斷 Probability vs. Inference
在概率論的環境下，我們常常被告知的前提是：某某事件發生的概率是多少。例如： 一枚硬幣正面朝上的概率是 $0.5\; Prob(coin\;landing\;heads)=0.5$。然後在這個前提下，我們又繼續去計算複雜的事件發生的概率(例如，10次投擲硬幣以後4次正面朝上的概率是多少？) 。

$$
\binom{10}{4}\times(0.5^4)\times(0.5^{10-4}) = 0.205
$$

```{r}
dbinom(4, 10, 0.5)

# or you can calculate by hand:
factorial(10)*(0.5^10)/(factorial(4)*(factorial(6)))
```

在統計推斷的理論中，我們考慮實際的情況，這樣的實際情況就是，我們通過觀察獲得數據，然而我們並不知道某事件發生的概率到底是多少(神如果存在話，只有神知道) 。故這個 $Prob(coin\;landing\;heads)$ 的概率大小對於“人類”來說是未知的。我們可能觀察到投擲了10次硬幣，其中有4次是正面朝上的。那麼我們從這一次觀察實驗中，需要計算的是能夠符合觀察結果的“最佳”概率估計 (best estimate)。在這種情況下，**似然法 (likelihood)** 就是我們進行參數估計的最佳手段。

## 似然和極大似然估計 Likelihood and maximum likelihood estimators

此處用二項分佈的例子來理解似然法的概念：假設我們觀察到10個對象中有4個患~~中二~~病，我們假定這個患病的概率爲 $\pi$。於是我們就有了下面的模型：

**模型：** 我們假定患病與否是一個服從**二項分佈的隨機變量**，$X\sim Bin(10,\pi)$。同時也默認每個人之間是否患病是相互獨立的。

**數據：** 觀察到的數據是，10人中有4人患病。於是 $x=4$。

現在按照觀察到的數據，參數 $\pi$ 變成了未知數：

$$Prob(X=4|\pi)=\binom{10}{4}\pi^4(1-\pi)^{10-4}$$

此時我們會很自然的考慮，當 $\pi$ 是未知數的時候，**它取值爲多大的時候才能讓這個事件(即：10人中4人患病) 發生的概率最大？** 所以我們可以將不同的數值代入 $\pi$ 來計算該事件在不同概率的情況下發生的可能性到底是多少：

```{r echo=FALSE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("/home/ccwang/Documents/full-website-content/static/files/pi.csv", header = T)
#names(dt) <- c(" ", "0.05", "0.1", "0.2", "0.5")
#dt[,1] <- c(" ", 0.05, 0.01)
kable(dt, "html",align = "c",caption = "The probability of observing X=4") %>%
#   column_spec(1:5, bold = T, border_right = T)  %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))# %>%
#  add_header_above(c("\u03b1" = 1, "\u03b2" = 4))
```

<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption>Table 12.1: The probability of observing $X=4$</caption>
 <thead><tr>
<th style="text-align:center;"> $\pi$ </th>
   <th style="text-align:center;"> 事件 $X=4$ 發生的概率 </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:center;"> 0.0 </td>
   <td style="text-align:center;"> 0.000 </td>
  </tr>
<tr>
<td style="text-align:center;"> 0.2 </td>
   <td style="text-align:center;"> 0.088 </td>
  </tr>
<tr>
<td style="text-align:center;"> **0.4** </td>
   <td style="text-align:center;"> **0.251** </td>
  </tr>
<tr>
<td style="text-align:center;"> 0.5 </td>
   <td style="text-align:center;"> 0.205 </td>
  </tr>
<tr>
<td style="text-align:center;"> 0.6 </td>
   <td style="text-align:center;"> 0.111 </td>
  </tr>
<tr>
<td style="text-align:center;"> 0.8 </td>
   <td style="text-align:center;"> 0.006 </td>
  </tr>
<tr>
<td style="text-align:center;"> 1.0 </td>
   <td style="text-align:center;"> 0.000 </td>
  </tr>
</tbody>
</table>

很顯然，如果 $\pi=0.4$ 時，我們觀察到的事件發生的概率要比 $\pi$ 取其它值時更大。於是小總結一下目前爲止的步驟如下：

- 觀察到實驗數據(10人中4個患病) ；
- 假定這數據服從二項分佈的概率模型，計算不同($\pi$ 的取值不同的) 情況下，該事件按照假定模型發生的概率；
- 通過比較，我們選擇了能夠讓觀察事件發生概率最高的參數取值 ($\pi=0.4$)。

至此，我們可以知道，似然方程，是一個關於未知參數 $\pi$ 的函數，我們目前位置做的就是找到這個函數的最大值 (maximised)，和使之成爲最大值時的 $\pi$ ：

$$L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}$$

我們可以畫出這個似然方程的形狀， $\pi\in[0,1]$

```{r binomial-likelihood, fig.asp=.7, fig.width=6,  fig.cap='Binomial Likelihood', fig.align='center', out.width='90%'}
x <- seq(0,1,by=0.001)
y <- (factorial(10)/(factorial(4)*(factorial(6))))*(x^4)*((1-x)^6)
plot(x, y, type = "l", ylim = c(0,0.3), ylab = "L(\U03C0)", xlab = "\U03C0")
#title("Figure 1. Binomial Likelihood")
abline(h=0.251, lty=2)
abline(v=0.4, lty=2)
```

從圖形上我們也能確認，$\pi=0.4$ 時能夠讓這個似然方程取得最大值。


## 似然方程的一般化定義

對於一個概率模型，如果其參數爲 $\theta$，那麼在給定觀察數據 $\underline{x}$ 時，該參數的似然方程被定義爲：

$L(\theta|\underline{x})=P(\underline{x}|\theta)$

注意：

1. $P(\underline{x}|\theta)$ 可以是概率(離散分佈) 方程，也可以是概率密度(連續型變量) 方程。對於此方程，$\theta$ 是給定的，然後再計算某些事件發生的概率。
2. $L(\theta|\underline{x})$ 是一個關於參數 $\theta$ 的方程，此時，$\underline{x}$ 是固定不變的(觀察值) 。我們希望通過這個方程求出能夠使觀察到的事件發生概率最大的參數值。
3. 似然方程**不是**一個概率密度方程。

另一個例子：

有一組觀察數據是離散型隨機變量 $X$，它符合概率方程 $f(x|\theta)$。下表羅列了當 $\theta$ 分別取值 $1,2,3$ 時的概率方程的值，試求每個觀察值 $X = 0,1,2,3,4$ 的最大似然參數估計：


```{r echo=FALSE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("/home/ccwang/Documents/full-website-content/static/files/likelihoodtable1.csv", header = T)
#names(dt) <- c(" ", "0.05", "0.1", "0.2", "0.5")
#dt[,1] <- c(" ", 0.05, 0.01)
kable(dt, "html",align = "c",caption = "Exercise 1") %>%
#   column_spec(1:5, bold = T, border_right = T)  %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))# %>%
#  add_header_above(c("\u03b1" = 1, "\u03b2" = 4))
```
<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption>Exercise 12.3</caption>
 <thead><tr>
<th style="text-align:center;"> $x$ </th>
   <th style="text-align:center;"> $f(x|1)$ </th>
   <th style="text-align:center;"> $f(x|2)$ </th>
   <th style="text-align:center;"> $f(x|3)$ </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> 1/3 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 0 </td>
  </tr>
<tr>
<td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 1/3 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 0 </td>
  </tr>
<tr>
<td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 1/6 </td>
  </tr>
<tr>
<td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 1/6 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 1/2 </td>
  </tr>
<tr>
<td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 1/6 </td>
   <td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> 1/3 </td>
  </tr>
</tbody>
</table>


```{r echo=FALSE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("/home/ccwang/Documents/full-website-content/static/files/likelihoodtable2.csv", header = T)
#names(dt) <- c(" ", "0.05", "0.1", "0.2", "0.5")
#dt[,1] <- c(" ", 0.05, 0.01)
kable(dt, "html",align = "c",caption = "Exercise 1") %>%
#   column_spec(1:5, bold = T, border_right = T)  %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))# %>%
#  add_header_above(c("\u03b1" = 1, "\u03b2" = 4))
```

<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption>Exercise 12.3 answer</caption>
 <thead><tr>
<th style="text-align:center;"> $x$ </th>
   <th style="text-align:center;"> $f(x|1)$ </th>
   <th style="text-align:center;"> $f(x|2)$ </th>
   <th style="text-align:center;"> $f(x|3)$ </th>
   <th style="text-align:center;"> $\theta$ </th>
  </tr></thead>
<tbody>
<tr>
<td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> 1/3 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> **1** </td>
  </tr>
<tr>
<td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 1/3 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> **1** </td>
  </tr>
<tr>
<td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 1/6 </td>
   <td style="text-align:center;"> **2** </td>
  </tr>
<tr>
<td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 1/6 </td>
   <td style="text-align:center;"> 1/4 </td>
   <td style="text-align:center;"> 1/2 </td>
   <td style="text-align:center;"> **3** </td>
  </tr>
<tr>
<td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 1/6 </td>
   <td style="text-align:center;"> 0 </td>
   <td style="text-align:center;"> 1/3 </td>
   <td style="text-align:center;"> **3** </td>
  </tr>
</tbody>
</table>

## 對數似然方程 log-likelihood

似然方程的最大值，可通過求 $L(\theta|data)$ 的最大值獲得，也可以通過求該方程的對數方程 $\ell(\theta|data)$ 的最大值獲得。傳統上，我們估計最大方程的最大值的時候，會給參數戴一頂“帽子”(因爲這是觀察獲得的數據告訴我們的參數) ： $\hat{\theta}$。並且我們發現對數似然方程比一般的似然方程更加容易微分，因此求似然方程的最大值就變成了求對數似然方程的最大值：

$$\frac{d\ell}{d\theta}=\ell^\prime(\theta)=0\\
AND\\
\frac{d^2\ell}{d\theta^2}<0$$

要注意的是，微分不一定總是能幫助我們求得似然方程的最大值。如果說參數本身的定義域是有界限的話，微分就行不通了：

```{r likelihood-limited, fig.asp=.7, fig.width=6, fig.cap='Likelihood function with a limited domain', fig.align='center', out.width='90%'}
x <- seq(0,3,by=0.001)
y <- (x-1)^2-5
plot(x, y, type = "l", ylim = c(-5,0-1), ylab = "L(\U03B8)", xlab = "\U03B8")
#title("Figure 2. Likelihood function with \n a limited domain")
abline(v=3, lty=2)
```

**證明：當 $L(\theta|data)$ 取最大值時，該方程的對數方程 $\ell(\theta|data)$ 也是最大值：**

如果似然方程是連續可導，只有一個最大值，且可以二次求導，假設 $\hat{\theta}$ 使該方程取最大值，那麼：

$$\frac{dL}{d\theta}=0, \frac{d^2L}{d\theta^2}<0 \Rightarrow \theta=\hat{\theta}$$

令 $\ell=\text{log}L$ 那麼 $\frac{d\ell}{dL}=\ell^\prime=\frac{1}{L}$：

$$\frac{d\ell}{d\theta}=\frac{d\ell}{dL}\cdot\frac{dL}{d\theta}=\frac{1}{L}\cdot\frac{dL}{d\theta}$$

當 $\ell(\theta|data)$ 取最大值時：

$$\frac{d\ell}{d\theta}=0\Leftrightarrow\frac{1}{L}\cdot\frac{dL}{d\theta}=0\\
\because \frac{1}{L}\neq0 \\
\therefore \frac{dL}{d\theta}=0\\
\Leftrightarrow \theta=\hat{\theta}$$

$$
\begin{aligned}
\frac{d^2\ell}{d\theta^2} &= \frac{d}{d\theta}(\frac{d\ell}{dL}\cdot\frac{dL}{d\theta})\\
 &= \frac{d\ell}{dL}\cdot\frac{d^2L}{d\theta^2} + \frac{dL}{d\theta}\cdot\frac{d}{d\theta}(\frac{d\ell}{dL})
\end{aligned}
$$

當 $\theta=\hat{\theta}$ 時，$\frac{dL}{d\theta}=0$ 且 $\frac{d^2L}{d\theta^2}<0 \Rightarrow \frac{d^2\ell}{d\theta^2}<0$

所以，求獲得 $\ell(\theta|data)$ 最大值的 $\theta$ 即可令 $L(\theta|data)$ 獲得最大值。

## 極大似然估計 (maximum likelihood estimator, MLE) 的性質：

1. 漸進無偏 Asymptotically unbiased: <br> $n\rightarrow \infty \Rightarrow E(\hat{\Theta}) \rightarrow \theta$
2. 漸進最高效能 Asymptotically efficient: <br> $n\rightarrow \infty \Rightarrow Var(\hat{\Theta})$ 是所有參數中方差最小的估計
3. 漸進正態分佈 Asymptotically normal: <br> $n\rightarrow \infty \Rightarrow \hat{\Theta} \sim N(\theta, Var(\hat{\Theta}))$
4. 變形後依然保持不變 Transformation invariant: <br> $\hat{\Theta}$ 是 $\theta$ 的MLE時 $\Rightarrow g(\hat{\Theta})$ 是 $g(\theta)$ 的 MLE
5. 信息足夠充分 Sufficient：<br> $\hat{\Theta}$ 包含了觀察數據中所有的能夠用於估計參數的信息
6. 始終不變 consistent: <br> $n\rightarrow\infty\Rightarrow\hat{\Theta}\rightarrow\theta$ 或者可以寫成：$\varepsilon>0, lim_{n\rightarrow\infty}P(|\hat{\Theta}-\theta|>\varepsilon)=0$

## 率的似然估計 Likelihood for a rate {#likelihood-poi}

如果在一項研究中，參與者有各自不同的追蹤隨訪時間(長度) ，那麼我們應該把事件(疾病) 的發病率用率的形式(多少事件每單位人年, e.g. per person year of observation) 。如果這個發病率的參數用 $\lambda$ 來表示，所有參與對象的隨訪時間之和爲 $p$ 人年。那麼這段時間內的期望事件(疾病發病) 次數爲：$\mu=\lambda p$。假設事件(疾病發病) 發生是相互獨立的，可以使用泊松分佈來模擬期望事件(疾病發病) 次數 $D$：

$$D\sim Poi(\mu)$$

假設我們觀察到了 $D=d$ 個事件，我們獲得這個觀察值的概率應該用以下的模型：

$$Prob(D=d)=e^{-\mu}\frac{\mu^d}{d!}=e^{-\lambda p}\frac{\lambda^dp^d}{d!}$$

因此，$\lambda$ 的似然方程是：

$$L(\lambda|observed \;data)=e^{-\lambda p}\frac{\lambda^dp^d}{d!}$$

所以，$\lambda$ 的對數似然方程是：

$$
\begin{aligned}
\ell(\lambda|observed\;data) &= \text{log}(e^{-\lambda p}\frac{\lambda^dp^d}{d!}) \\
  &= -\lambda p+d\:\text{log}(\lambda)+d\:\text{log}(p)-\text{log}(d!) \\
\end{aligned}
$$

解 $\ell^\prime(\lambda|data)=0$:

$$
\begin{aligned}
\ell^\prime(\lambda|data) &= -p+\frac{d}{\lambda}=0\\
\Rightarrow \hat{\lambda} &= \frac{d}{p} \\
\end{aligned}
$$

**注意：**
在對數似然方程中，不包含參數的部分，對與似然方程的形狀不產生任何影響，我們在微分對數似然方程的時候，這部分也都自動消失。所以不包含參數的部分，與我們如何獲得極大似然估計是無關的。因此，我們常常在寫對數似然方程的時候就把其中沒有參數的部分直接忽略了。例如上面泊松分佈的似然方程中，$d\:\text{log}(p)-\text{log}(d!)$ 不包含參數 $\lambda$ 可以直接不寫出來。


## 有 $n$ 個獨立觀察時的似然方程和對數似然方程
當有多個獨立觀察時，總體的似然方程等於各個觀察值的似然方程之**乘積**。如果 $X_1,\dots,X_n\stackrel{i.i.d}{\sim}f(\cdot|\theta)$

$$L(\theta|x_1,\cdots,x_n)=f(x_1,\cdots,x_n|\theta)=\prod_{i=1}^nf(x_i|\theta)\\
\Rightarrow \ell(\theta|x_1,\cdots,x_n)=\sum_{i=1}^n\text{log}(f(x_i|\theta))$$




# 對數似然比 Log-likelihood ratio {#llr}

對數似然比的想法來自於將對數似然方程圖形的 $y$ 軸重新調節 (rescale) 使之最大值爲零。這可以通過計算該分佈方程的**對數似然比 (log-likelihood ratio)** 來獲得：

$$llr(\theta)=\ell(\theta|data)-\ell(\hat{\theta}|data)$$

由於 $\ell(\theta)$ 的最大值在 $\hat{\theta}$ 時， 所以，$llr(\theta)$ 就是個當 $\theta=\hat{\theta}$ 時取最大值，且最大值爲零的方程。很容易理解我們叫這個方程爲對數似然比，因爲這個方程就是將似然比 $LR(\theta)=\frac{L(\theta)}{L(\hat{\theta})}$ 取對數而已。

[之前](https://winterwang.github.io/post/likelihood/)我們也確證了，不包含我們感興趣的參數的方程部分可以忽略掉。還是用上一節 10人中4人患病的例子：

$$L(\pi|X=4)=\binom{10}{4}\pi^4(1-\pi)^{10-4}\\
\Rightarrow \ell(\pi)=\text{log}[\pi^4(1-\pi)^{10-4}]\\
\Rightarrow llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=\text{log}\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}$$

其實由上也可以看出 $llr(\theta)$ 只是將對應的似然方程的 $y$ 軸重新調節了一下而已。形狀是沒有改變的：

```{r binomial-logornot, fig.asp=.7, fig.width=6,fig.cap='Binomial likelihood ratio and log-likelihood ratio', fig.align='center', out.width='90%'}

par(mfrow=c(1,2))
x <- seq(0,1,by=0.001)
y <- (x^4)*((1-x)^6)/(0.4^4*0.6^6)
z <- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, y, type = "l", ylim = c(0,1.1),yaxt="n",
     frame.plot = FALSE, ylab = "LR(\U03C0)", xlab = "\U03C0")
axis(2, at=seq(0,1, 0.2), las=2)
title(main = "Binomial likelihood ratio")
abline(h=1.0, lty=2)
segments(x0=0.4, y0=0, x1=0.4, y1=1, lty = 2)
plot(x, z, type = "l", ylim = c(-10, 1), yaxt="n", frame.plot = FALSE,
     ylab = "llr(\U03C0)", xlab = "\U03C0" )
axis(2, at=seq(-10, 0, 2), las=2)
title(main = "Binomial log-likelihood ratio")
abline(h=0, lty=2)
segments(x0=0.4, y0=-10, x1=0.4, y1=0, lty = 2)
```

## 正態分佈數據的極大似然和對數似然比

假設單個樣本 $y$ 是來自一組服從正態分佈數據的觀察值：$Y\sim N(\mu, \tau^2)$

那麼有：

$$
\begin{aligned}
f(y|\mu) &= \frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow L(\mu|y) &=\frac{1}{\sqrt{2\pi\tau^2}}e^{(-\frac{1}{2}(\frac{y-\mu}{\tau})^2)} \\
\Rightarrow \ell(\mu)&=\text{log}(\frac{1}{\sqrt{2\pi\tau^2}})-\frac{1}{2}(\frac{y-\mu}{\tau})^2\\
omitting&\;terms\;not\;in\;\mu \\
&= -\frac{1}{2}(\frac{y-\mu}{\tau})^2 \\
\Rightarrow \ell^\prime(\mu) &= 2\cdot[-\frac{1}{2}(\frac{y-\mu}{\tau})\cdot\frac{-1}{\tau}] \\
&=\frac{y-\mu}{\tau^2} \\
let \; \ell^\prime(\mu) &= 0 \\
\Rightarrow \frac{y-\mu}{\tau^2} &= 0 \Rightarrow \hat{\mu} = y\\
\because \ell^{\prime\prime}(\mu) &=  \frac{-1}{\tau^2} < 0 \\
\therefore \hat{\mu} &= y \Rightarrow \ell(\hat{\mu}=y)_{max}=0 \\
llr(\mu)&=\ell(\mu)-\ell(\hat{\mu})=\ell(\mu)\\
&=-\frac{1}{2}(\frac{y-\mu}{\tau})^2
\end{aligned}
$$

## $n$ 個獨立正態分佈樣本的對數似然比 {#llr-chi1}

假設一組觀察值來自正態分佈 $X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)$，先假設 $\sigma^2$ 已知。將觀察數據 $x_1,\cdots, x_n$ 標記爲 $\underline{x}$。 那麼：

$$
\begin{aligned}
L(\mu|\underline{x}) &=\prod_{i=1}^nf(x_i|\mu)\\
\Rightarrow \ell(\mu|\underline{x}) &=\sum_{i=1}^n\text{log}f(x_i|\mu)\\
&=\sum_{i=1}^n[-\frac{1}{2}(\frac{x_i-\mu}{\sigma})^2]\\
&=-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\\
&=-\frac{1}{2\sigma^2}[\sum_{i=1}^n(x_i-\bar{x})^2+\sum_{i=1}^n(\bar{x}-\mu)^2]\\
omitting&\;terms\;not\;in\;\mu \\
&=-\frac{1}{2\sigma^2}\sum_{i=1}^n(\bar{x}-\mu)^2\\
&=-\frac{n}{2\sigma^2}(\bar{x}-\mu)^2 \\
&=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\\
\because \ell(\hat{\mu}) &= 0 \\
\therefore llr(\mu) &= \ell(\mu)-\ell(\hat{\mu}) = \ell(\mu)
\end{aligned}
$$

## $n$ 個獨立正態分佈樣本的對數似然比的分佈 {#llr-chi}

假設我們用 $\mu_0$ 表示總體均數這一參數的值。要注意的是，每當樣本被重新取樣，似然，對數似然方程，對數似然比都隨着觀察值而變 (即有自己的分佈)。

考慮一個服從正態分佈的單樣本 $Y$: $Y\sim N(\mu_0,\tau^2)$。那麼它的對數似然比：

$$llr(\mu_0|Y)=\ell(\mu_0)-\ell(\hat{\mu})=-\frac{1}{2}(\frac{Y-\mu_0}{\tau})^2$$

根據**卡方分佈** (Section \@ref(chi-square-distribution)) 的定義：

$$\because \frac{Y-\mu_0}{\tau}\sim N(0,1)\\
\Rightarrow (\frac{Y-\mu_0}{\tau})^2 \sim \mathcal{X}_1^2\\
\therefore -2llr(\mu_0|Y) \sim \mathcal{X}_1^2$$

所以，如果有一組服從正態分佈的觀察值：$X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu_0,\sigma^2)$，且 $\sigma^2$ 已知的話：

$$-2llr(\mu_0|\bar{X})\sim \mathcal{X}_1^2$$

根據**中心極限定理** (Section \@ref(CLT))，可以將上面的結論一般化：

```{theorem}
如果 $X_1,\cdots,X_n\stackrel{i.i.d}{\sim}f(x|\theta)$。 那麼當重複多次從參數爲 $\theta_0$ 的總體中取樣時，那麼統計量 $-2llr(\theta_0)$ 會漸進於自由度爲 $1$ 的卡方分佈： $$-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\xrightarrow[n\rightarrow\infty]{}\;\sim \mathcal{X}_1^2$$
```


## 似然比信賴區間

如果樣本量 $n$ 足夠大 (通常應該大於 $30$)，根據上面的定理：

$$-2llr(\theta_0)=-2\{\ell(\theta_0)-\ell(\hat{\theta})\}\sim \mathcal{X}_1^2$$

所以：

$$Prob(-2llr(\theta_0)\leqslant \mathcal{X}_{1,0.95}^2=3.84) = 0.95\\
\Rightarrow Prob(llr(\theta_0)\geqslant-3.84/2=-1.92) = 0.95$$

故似然比的 $95\%$ 信賴區間就是能夠滿足 $llr(\theta)=-1.92$ 的兩個 $\theta$ 值。

### 以二項分佈數據爲例  {#binomial-ex}
繼續用本文開頭的例子：

$$llr(\pi)=\ell(\pi)-\ell(\hat{\pi})=\text{log}\frac{\pi^4(1-\pi)^{10-4}}{0.4^4(1-0.4)^{10-4}}$$

如果令 $llr(\pi)=-1.92$ 在代數上可能較難獲得答案。然而從圖形上，如果我們在 $y=-1.92$ 畫一條橫線，和該似然比方程曲線相交的兩個點就是我們想要求的信賴區間的上限和下限：

```{r bin-llr-95,fig.height=5, fig.width=8, warning=FALSE, message=FALSE, fig.cap='Log-likelihood ratio for binomial example, with 95% confidence intervals shown', fig.align='center', out.width='90%'}
x <- seq(0,1,by=0.001)
z <- log((x^4)*((1-x)^6))-log(0.4^4*0.6^6)
plot(x, z, type = "l", ylim = c(-10, 1), yaxt="n", frame.plot = FALSE,
     ylab = "llr(\U03C0)", xlab = "\U03C0" )
axis(2, at=seq(-10, 0, 2), las=2)
abline(h=0, lty=2)
abline(h=-1.92, lty=2)
segments(x0=0.15, y0=-12, x1=0.15, y1=-1.92, lty = 2)
segments(x0=0.7, y0=-12, x1=0.7, y1=-1.92, lty = 2)
axis(1, at=c(0.15,0.7))
text(0.9, -1, "-1.92")
arrows(0.8, -1.92, 0.8, 0, lty = 1, length = 0.08)
arrows( 0.8, 0, 0.8, -1.92, lty = 1, length = 0.08)
```

從上圖中可以讀出，$95\%$ 對數似然比信賴區間就是 $(0.15, 0.7)$


### 以正態分佈數據爲例 {#normal-ex}

本文前半部分證明過，
$X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(\mu,\sigma^2)$，先假設 $\sigma^2$ 已知。將觀察數據 $x_1,\cdots, x_n$ 標記爲 $\underline{x}$。 那麼：

$$llr(\mu|\underline{x}) = \ell(\mu|\underline{x})-\ell(\hat{\mu}) = \ell(\mu|\underline{x}) \\
=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2$$

很顯然，這是一個關於 $\mu$ 的二次方程，且最大值在 MLE $\hat{\mu}=\bar{x}$ 時取值 $0$。所以可以通過對數似然比法求出均值的 $95\%$ 信賴區間公式：

$$-2\times[-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2]=3.84\\
\Rightarrow L=\bar{x}-\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
U=\bar{x}+\sqrt{3.84}\frac{\sigma}{\sqrt{n}} \\
note: \;\sqrt{3.84}=1.96$$

注意到這和我們之前求的正態分佈均值的信賴區間公式 (Section \@ref(CI-for-sample-mean)) 完全一致。

## 練習題

### Q1

a) 假設十個對象中有三人死亡，用二項分佈模型來模擬這個例子，求這個例子中參數 $\pi$ 的似然方程和圖形 (likelihood) ?

**解**

$$\begin{aligned}
 L(\pi|3) &= \binom{10}{3}\pi^3(1-\pi)^{10-3} \\
 omitting\;&terms\;not\;in\;\pi \\
 \Rightarrow \ell(\pi|3) &= \text{log}[\pi^3(1-\pi)^7] \\
 &= 3\text{log}\pi+7\text{log}(1-\pi)\\
 \Rightarrow \ell^\prime(\pi|3)&= \frac{3}{\pi}-\frac{7}{1-\pi} \\
 let \; \ell^\prime& =0\\
 &\frac{3}{\pi}-\frac{7}{1-\pi} = 0 \\
 &\frac{3-10\pi}{\pi(1-\pi)} = 0 \\
 \Rightarrow MLE &= \hat\pi = 0.3
\end{aligned}$$


```{r bin3-10,fig.width=6, echo=FALSE, message=FALSE, fig.cap='Binomial likelihood function 3 out of 10 subjects', fig.align='center', out.width='90%'}
pi <- seq(0, 1, by=0.01)

L <- (pi^3)*((1-pi)^7)
plot(pi, L, type = "l", ylim = c(0, 0.0025),yaxt="n", col="darkblue",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
grid(NA, 5, lwd = 1)
abline(v=0.3, lty=2)
axis(1, at=0.3, las=0)
axis(2, at=seq(0,0.0025,0.0005), las=2)
#title(main = "Binomial likelihood function\n 3 out of 10 subjects")
```


b) 計算似然比，並作圖，注意方程圖形未變，$y$ 軸的變化；取對數似然比，並作圖

```{r bin3-10-ratio,fig.width=6,  message=FALSE, warning=FALSE, message=FALSE, fig.cap='Binomial likelihood ratio function 3 out of 10 subjects', fig.align='center', out.width='90%'}
LR <- L/max(L) ; head(LR)
plot(pi, LR, type = "l", ylim = c(0, 1),yaxt="n", col="darkblue",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
grid(NA, 5, lwd = 1)
axis(2, at=seq(0,1,0.2), las=2)
title(main = "Binomial likelihood ratio function\n 3 out of 10 subjects")
```

```{r bin3-10-logratio,fig.width=6,  message=FALSE, warning=FALSE, message=FALSE, fig.cap='Binomial log-likelihood ratio function 3 out of 10 subjects', fig.align='center', out.width='90%'}
logLR <- log(L/max(L))
plot(pi, logLR, type = "l", ylim = c(-4, 0),yaxt="n", col="darkblue",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
grid(NA, 5, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Binomial log-likelihood ratio function\n 3 out of 10 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=0)
```

### Q2

a) 與上面用同樣的模型，但是觀察人數變爲 $100$ 人 患病人數爲 $30$ 人，試作對數似然比方程之圖形，與上圖對比：

```{r bin3-10-30-100-logllr,fig.asp=.7, fig.width=6,  echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Binomial log-likelihood ratio function 3 out of 10 and 30 out of 100 subjects', fig.align='center', out.width='90%'}
par(mai = c(1.2, 0.5, 1, 0.7))
logLR_30 <- log((pi^30)*((1-pi)^70)/max((pi^30)*((1-pi)^70)))
plot(pi, logLR, type = "l", ylim = c(-4, 0),yaxt="n", col="darkblue",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR_30, col="darkred")
grid(NA, 5, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Binomial log-likelihood ratio function\n 3 out of 10 and 30 out of 100 subjects")
abline(h=-1.92, lty=1, col="darkred")
axis(4, at=-1.92, las=0)
legend(x=0.1, y= -5.8 ,xpd = TRUE,  legend=c("logLR","LogLR_30"), bty = "n",
       col=c("black","darkred"), lty=c(1,1), horiz = TRUE) #the legend is below the graph
```

可以看出，兩組數據的 MLE 都是一致的， $\hat\pi=0.3$，但是對數似然比方程圖形在 樣本量爲 $n=100$ 時比 $n=10$ 時窄很多，由此產生的似然比信賴區間也就窄很多(精確很多) 。所以對數似然比方程的曲率(二階導數) ，反映了觀察獲得數據提供的對總體參數 $\pi$ 推斷過程中的信息量。而且當樣本量較大時，對數似然比方程也更加接近左右對稱的二次方程曲線。


### Q3

在一個實施了160人年的追蹤調查中，觀察到8個死亡案例。使用泊松分佈模型，繪製對數似然比方程圖形，從圖形上目視推測極大似然比的 $95\%$ 信賴區間。

**解**

$$\begin{aligned}
 d = 8, \;p &= 160\; person\cdot year \\
  \Rightarrow D\sim Poi(\mu &=\lambda p) \\
 L(\lambda|data) &= Prob(D=d=8) \\
   &=  e^{-\mu}\frac{\mu^d}{d!} \\
   &=   e^{-\lambda p}\frac{\lambda^d p^d}{d!} \\
  omitting&\;terms\;not\;in\;\lambda \\
   &= e^{-\lambda p}\lambda^d \\
\Rightarrow \ell(\lambda|data)&= \text{log}(e^{-\lambda p}\lambda^d) \\
     &= d\cdot \text{log}(\lambda)-\lambda p \\
     & = 8\times \text{log}(\lambda) - 160\times\lambda
\end{aligned}$$


```{r Poi-llr-8-160,fig.width=6,  echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Poisson log-likelihood ratio function\n 8 events in 160 person-years', fig.align='center', out.width='90%'}
lambda <- seq(0.01, 0.10, by=0.001)
LogLR <- 8*log(lambda)-lambda*160-max(8*log(lambda)-lambda*160)
plot(lambda, LogLR, type = "l", ylim = c(-4, 0),yaxt="n", col="darkblue",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
grid(NA, 5, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Poisson log-likelihood ratio function\n 8 events in 160 person-years")
abline(h=-1.92, lty=1, col="darkred")
axis(4, at=-1.92, las=0)
```

```{r echo=FALSE}
library(knitr)
library(kableExtra)
kable(data.frame(lambda, LogLR), "html") %>%
  kable_styling("striped", full_width = F, position = "center") %>%
  row_spec(c(13,14,84,85), bold = T, color = "white", background = "#D7261E")
```

所以從列表數據結合圖形， 可以找到信賴區間的下限在 0.022~0.023 之間， 上限在 0.093～0.094 之間。


# 二次方程近似法求對數似然比  approximate log-likelihood ratios {#quadratic-llr}

爲什麼要用二次方程近似對數似然比方程？

1. 上節也看到，我們會碰上難以用代數學計算獲得對數似然比信賴區間的情況 (Section \@ref(binomial-ex): binomial example)。
2. 我們同時知道，對數似然比方程會隨着樣本量增加而越來越漸進於二次方程，且左右對稱。
3. 所以，我們考慮當樣本量足夠大時，用二次方程來近似對數似然比方程從而獲得參數估計的信賴區間。



## 正態近似法求對數似然 Normal approximation to the log-likelihood {#quadratic-llr2}

根據前一節 (Section \@ref(normal-ex))，如果樣本均數的分佈符合正態分佈：$\bar{X}\sim N(\mu, \sigma^2/n)$。那麼樣本均數的對數似然比爲：

$$llr(\mu|\bar{X})=\ell(\mu|\bar{X})=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2$$

其中， $\bar{x}$ 是正態分佈總體均數 $\mu$ 的極大似然估計 (maximum likelihood estimator, MLE)。如果已知總體的方差參數，那麼 $\sigma/\sqrt{n}$ 是 $\bar{x}$ 的標準誤 (standard error)。

因此，假設 $\theta$ 是我們想尋找的總體參數。有些人提議可以使用下面的關於 $\theta$ 的二次方程來做近似：

$$f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2$$

上述方程具有一個正態二次對數似然 (比) 的形式，而且該方程的極大似然估計(MLE)， $M$ 的標準誤爲 $S$。如果我們正確地選用 $M$ 和 $S$，那我們就可以用這樣的方程來近似求真實觀察數據的似然 $\ell(\theta|data)$。

通過近似正態對數似然比，$M$ 應當選用使方程取最大值時，參數 $\theta$ 的極大似然估計 $M=\hat{\Theta}$。

但是在選用標準誤 $S$ 上必須滿足下列條件：

1. $S$ 是極大似然估計 $\hat{\Theta}$ 的標準誤。
2. 被選擇的 $S$ 必須儘可能的使該二次方程形成一個十分接近真實的對數似然比方程。特別是在最大值的部分必須與之無限接近或者一致。所以二者在 MLE 的位置應當有相同的曲率(二階導數) 。

由於，一個方程的曲率是該方程的二階導數(斜線斜率變化的速度) 。所以對數似然比方程在 MLE 取最大值時的曲率(二階導數) 爲：

$$\left.\frac{d^2}{d\theta^2}\ell(\theta)\right\vert_{\theta=\hat{\theta}}=\ell^{\prime\prime}(\hat{\theta})=-\frac{1}{S^2}\\
\Rightarrow S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}
$$

在正態分佈的例子下，$M=\bar{x}, S=\sigma/\sqrt{n}$。對數似然比方程最大值時的曲率(二階導數) 恰好就爲標準誤的平方的負倒數：

$\ell^{\prime\prime}(\theta)=-\frac{1}{SE^2}$ $\Rightarrow$ 被叫做 **Fisher information**。

稍微總結一下：

1. 任意的對數似然比方程 $llr(\theta)$ 都可以考慮用一個二次方程來近似：
    $$f(\theta|data)=-\frac{1}{2}(\frac{\theta-M}{S})^2$$
2. 其中<br>
    $\begin{aligned}
    &M=\hat\theta\\
    &S^2=\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}\\
    &when \\
     & n\rightarrow\infty \Rightarrow
     \begin{cases}
     S^2\rightarrow Var(\hat\theta) \\
     S\rightarrow SE(\hat\theta)
     \end{cases}
    \end{aligned}$


### 近似法估算對數似然比的信賴區間

一旦我們決定了使用正態近似法來模擬對數似然比方程，對數似然比的信賴區間算法就回到了前一節中我們算過的方法，也就是：

$$-2f(\theta)<\mathcal{X}_{1,(1-\alpha)}^2$$

故信賴區間爲： $m\pm\sqrt{\mathcal{X}_{1,(1-\alpha)}^2}S$。求$95\%$ 水平的信賴區間時，$\mathcal{X}_{1,0.95}^2=3.84$，所以就又看到了熟悉的 $M\pm1.96S$。

### 以泊松分佈爲例

一個被追蹤的樣本，經過了 $p$ 人年的觀察，記錄到了 $d$ 個我們要研究的事件：

$$D\sim Poi(\mu), where \mu=\lambda p$$

Step 1. 找極大似然估計 (MLE)，之前介紹似然方程時推導過的泊松分佈的似然方程 (Section \@ref(likelihood-poi))：

$$\begin{aligned}
P(D=d|\lambda) &= \frac{e^{-\mu}\cdot\mu^d}{d!} \\
 &=\frac{e^{-\lambda p}\cdot\lambda^d p^d}{d!} \\
omitting&\;terms\;not\;in\;\mu \\
&\Rightarrow \ell(\lambda) = d\text{log}\lambda - \lambda p \\
&\Rightarrow \ell^\prime(\lambda) = \frac{d}{\lambda} -p \\
&\Rightarrow \hat\lambda=\frac{d}{p} = \textbf{M}
\end{aligned}$$

Step 2. 求似然方程的二階導數，確認 MLE 是使方程獲得最大值的點，然後確定 $S^2$：

$$\begin{aligned}
& \ell^\prime(\lambda) = \frac{d}{\lambda} -p \\
& \Rightarrow \ell^{\prime\prime}(\lambda) = -\frac{d}{\lambda^2}<0 \Rightarrow \textbf{MLE is maximum} \\
& S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\lambda)}\right\vert_{\lambda=\hat{\lambda}=d/p} = -\frac{1}{-d/\hat\lambda^2} = -\frac{1}{-d/(d/p)^2} \\
&\Rightarrow S^2 = \frac{d}{p^2} \\
\end{aligned}$$

Step 3. 把前兩部求得的 $MLE$ 和 $S^2$ 代入近似的二次方程：

$$\begin{aligned}
& \hat\lambda=\frac{d}{p}=M,\; S^2 = \frac{d}{p^2}  \\
& using\;approximate\;quadratic\;llr \\
& q(\lambda) = -\frac{1}{2}(\frac{\lambda-M}{S})^2\\
&\Rightarrow q(\lambda) = -\frac{1}{2}(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2\\
& let \; q(\lambda)=-1.92\\
&\Rightarrow -\frac{1}{2}(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2=-1.92\\
&(\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}})^2=3.84\\
&\frac{\lambda-\frac{d}{p}}{\frac{\sqrt{d}}{p}} = \pm1.96\\
&\Rightarrow 95\%CI \;for \;\lambda = \frac{d}{p}\pm1.96\frac{\sqrt{d}}{p}
\end{aligned}$$

結論就是： 發病(死亡) 率 $\lambda$ 的 $95\%$ 信賴區間爲： $M\pm1.96S$。所以我們不需要每次都代入對數似然比方程，只要算出 $MLE = M$ 和 $S$ 之後代入這個公式就可以用二次方程近似法算出信賴區間。

### 以二項分佈爲例 {#quadratic-binomial-approx}

$$K\sim Bin(n,\pi)$$

Step 1. 找極大似然估計 (MLE)：

$$
\begin{aligned}
& Prob(K=k) = \pi^k(1-\pi)\binom{n}{k}\\
&\Rightarrow L(\pi|k) = \pi^k(1-\pi)\binom{n}{k}\\
&omitting\;terms\;not\;in\;\pi \\
&\Rightarrow \ell(\pi) = k\:\text{log}\pi+(n-k)\text{log}(1-\pi) \\
&\ell^\prime(\pi) = \frac{k}{\pi}-\frac{n-k}{1-\pi} \\
& let\;\ell^\prime(\hat\pi) =0 \\
&\Rightarrow \frac{k}{\hat\pi}-\frac{n-k}{1-\hat\pi}=0\\
&\Rightarrow \frac{\hat\pi}{1-\hat\pi}=\frac{k}{n-k}\\
&\Rightarrow \frac{\hat\pi}{1-\hat\pi}=\frac{k/n}{1-k/n}\\
&\Rightarrow \hat\pi=\frac{k}{n} = p = \textbf{M}
\end{aligned}
$$

Step 2. 將對數似然方程的二次微分 (二階導數)，確認在 MLE 爲極大值，並確認 $S^2$：

$$
\begin{aligned}
&\ell^\prime(\pi) = \frac{k}{\pi}-\frac{n-k}{1-\pi} \\
&\ell^{\prime\prime}(\pi)=\frac{-k}{\pi^2}-\frac{n-k}{(1-\pi)^2} <0 \\
&\therefore at\;\textbf{MLE}\;\ell(\pi)\;has\;maximum \\
S^2&=\left.-\frac{1}{\ell^{\prime\prime}(\pi)}\right\vert_{\pi=\hat\pi=k/n=p}\\
&=\frac{1}{\frac{k}{\hat\pi^2}+\frac{n-k}{(1-\hat\pi)^2}}\\
&=\frac{\hat\pi^2(1-\hat\pi)^2}{k(1-\hat\pi)^2+(n-k)\hat\pi^2}\\
&=\frac{P^2(1-P)^2}{np(1-p)^2+(n-np)p^2}\\
&=\frac{p(1-p)}{n(1-p)+np}\\
&=\frac{p(1-p)}{n}\\
&\Rightarrow S=\sqrt{\frac{p(1-p)}{n}}
\end{aligned}
$$

Step 3. 將求得的 MLE 和 $S^2$ 代入近似信賴區間：

$$
95\% CI \;for \; \pi:\\
M\pm1.96S=p\pm1.96\sqrt{\frac{p(1-p)}{n}}\\
$$


## 參數转换 parameter transformations {#para-trans}

如果將參數 $\theta$ 通過某種數學方程轉化成 $g(\theta)$，那麼我們可以認爲，轉化後的方程的 MLE 爲 $g(\hat\theta)$，其中 $\hat\theta$ 是參數 $\theta$ 的 MLE。

類似地，如果 $\theta_1 \sim \theta_2$ 是參數 $\theta$ 的似然比信賴區間，那麼 $g(\theta_1)\sim g(\theta_2)$ 就是 $g(\theta)$ 的似然比信賴區間。

以下爲轉換參數以後獲取信賴區間的步驟：

1. 將參數通過某些數學方程(通常是取對數) 轉化，使新的對數似然比方程更加接近二次方程的對稱圖形。<br> Transform parameter so that $llr$ is closer to a quadratic shape.
2. 用本節學到的二次方程近似法，求得轉化後的參數的似然比信賴區間。 <br> Use our quadratic approximation on the transformed parameter to calculate our likelihood ratio confidence intervals.
3. 將第2步計算獲得的似然比信賴區間再通過轉化參數時的逆函數轉換回去，以獲得原參數的似然比信賴區間。<br> Transform the confidence intervals back, or to any scale we wish – they remain valid.

### 以泊松分佈爲例

當我們用泊松分佈模擬事件在某段時間內發生率 $\lambda$ 時，注意到這個事件發生率必須滿足 $\lambda>0$。當事件發生次數較低時，會讓似然方程的圖形被擠壓在低值附近。如果嘗試用對數轉換 $\lambda \rightarrow \text{log}(\lambda)$ 此時 $\text{log}(\lambda)$ 就不再被限制與 $>0$。下面我們嘗試尋找對數轉換過後的 $M$ 和 $S$。

令 $\beta=\text{log}(\lambda), \Rightarrow e^\beta=\lambda$ 從本文上半部分中我們已知 $\hat\lambda=\frac{d}{p}$。

- 對數轉換以後的 $M$ 是什麼? <br>根據定義，$MLE(\beta)=MLE[\text{log}(\lambda)]=\text{log}(\hat\lambda)$
  $\Rightarrow M=\hat\beta=\text{log}(\frac{d}{p})$
- 對數轉換以後的 $S$ 是什麼? <br> 泊松分佈的對數似然方程是：$\ell(\lambda|d)=d \text{log}(\lambda) - \lambda p$ 用 $\beta$ 替換掉 $\lambda$

    $\begin{aligned}
    &  \ell(\beta|d)=d \beta - pe^\beta\\
    & \Rightarrow \ell^\prime(\beta)=d-pe^\beta \Rightarrow \ell^{\prime\prime}(\beta)=-pe^\beta \\
    & S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\beta)}\right\vert_{\beta=\hat{\beta}} = \left.\frac{1}{pe^\beta}\right\vert_{\beta=\hat{\beta}} = \frac{1}{pe^{\text{log}(d/p)}}\\
    &\Rightarrow S^2=\frac{1}{d} \therefore S=\frac{1}{\sqrt{d}}    \end{aligned}$
- 轉換後的近似二次方程：<br>
    $\begin{aligned}
    & q(\beta) = -\frac{1}{2}(\frac{\beta-M}{S})^2 = -\frac{1}{2}(\frac{\beta-\text{log}(\frac{d}{p})}{\frac{1}{\sqrt{d}}})^2
    \end{aligned}$

-  $\beta$ 的 $95\%$ 信賴區間 $=\text{log}(\frac{d}{p})\pm1.96\frac{1}{\sqrt{d}}$

- $\lambda$ 的 $95\%$ 信賴區間 $=exp(\text{log}(\frac{d}{p})\pm1.96\frac{1}{\sqrt{d}})$

### 以二項分佈爲例

在研究對象 $n$ 人中觀察到 $k$ 個人患有某種~~中二~~疾病。

令 $\beta=\text{log}(\pi) \Rightarrow \pi=e^\beta$ 從上文的推倒也已知 $\hat\pi=\frac{k}{n}=p$

$\begin{aligned}
&\Rightarrow \ell(\beta)=k\text{log}\pi+(n-k)\text{log}(1-\pi)=k\beta+(n-k)\text{log}(1-e^\beta) \\
&\Rightarrow \ell^{\prime}(\beta)=k-\frac{(n-k)(e^\beta)}{1-e^\beta} \\
&\Rightarrow \ell^{\prime\prime}(\beta)=-(n-k)\frac{e^\beta(1-e^\beta)+e^{2\beta}}{(1-e^\beta)^2} \\
& \ell^{\prime\prime}(\beta)= -(n-k)\frac{e^\beta}{(1-e^\beta)^2}\\
&\Rightarrow S^2 = \left.-\frac{1}{\ell^{\prime\prime}(\beta)}\right\vert_{\beta=\hat{\beta}} = \frac{(1-e^{\hat\beta})^2}{(n-k)e^{\hat\beta}} \\
&\because \hat\beta=\text{log}(\hat\pi) \\
&\therefore e^{\hat\beta} = \frac{k}{n}\\
&\Rightarrow S^2=\frac{(1-\frac{k}{n})^2}{(n-k)\frac{k}{n}}=\frac{n-k}{nk}=\frac{1}{k}-\frac{1}{n}\\
& \Rightarrow S=\sqrt{\frac{1}{k}-\frac{1}{n}}\\
\end{aligned}$


## 練習題

### Q1

a) 在$n=100$人中觀察到有$k=40$人患病，假設每個人只有患病，不患病兩個狀態，用二項分佈來模擬這個數據，$\pi$ 爲患病的概率。下面是 $\pi \in [0.2,0.6]$ 區間的對數似然比方程曲線。

```{r bin-llr-40-100, warning=FALSE, cache=TRUE, fig.asp=.7, fig.width=6, fig.cap='Binomial log-likelihood ratio between 0.2-0.6', fig.align='center', out.width='90%'}
pi <- seq(0.2, 0.6, by=0.01)
L <- (pi^40)*((1-pi)^60)
Lmax <- rep(max(L), 41)
LR <- L/Lmax
logLR <- log(LR)

plot(pi, logLR, type = "l", ylim = c(-11, 0),yaxt="n",
     frame.plot = FALSE, ylab = "logLR(\U03C0)", xlab = "\U03C0")
grid(NA, 5, lwd = 2) # add some horizontal grid on the background
axis(2, at=seq(-12,0,2), las=2)
#title(main = "Figure 1. Binomial log-likelihood ratio")
```

b) 用一個二次方程來模擬上面的對數似然比曲線：$f(\pi)=-\frac{(\pi-M)^2}{2S^2}$，其中 $M=\hat\pi=\frac{k}{n}=0.4$，$S^2=\frac{p(1-p)}{n}=0.0024$

```{r qua-apprx-bin, warning=FALSE, cache=TRUE, fig.asp=.7, fig.width=6, fig.cap='Quadratic approximation\n of binomial log-likelihood ratio 40 out of 100 subjects', fig.align='center', out.width='90%'}
par(mai = c(1.2, 0.5, 1, 0.7))
quad <- -(pi-0.4)^2/(2*0.0024)
plot(pi, quad, type = "l", ylim = c(-4, 0),yaxt="n", col="red",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR, col="black")
grid(NA, 4, lwd = 1) # add some horizontal grid on the background
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Figure 2. Quadratic approximation\n of binomial log-likelihood ratio \n 40 out of 100 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=2)
legend(x=0.27, y= -5.5 ,xpd = TRUE,  legend=c("logLR","Quadratic"), bty = "n",
       col=c("black","red"), lty=c(1,1), horiz = TRUE) #the legend is below the graph
```

### Q2

依舊使用二項分佈數據來模擬，觀察不同的事件數量和樣本量對近似計算的影響。

1. 類比上面的問題，用同樣的 $\hat\pi=0.4$，但是 $n=10, k=4$ 時的圖形：

```{r qua-apprx-bin4-10, fig.asp=.7, fig.width=6, fig.cap='Quadratic approximation of binomial log-likelihood ratio 4 out of 10 subjects', fig.align='center', out.width='90%', warning=FALSE, cache=TRUE}
par(mai = c(1.2, 0.5, 1, 0.7))
pi <- seq(0.0, 0.85, by=0.01)
L <- (pi^4)*((1-pi)^6)
logLR <- log(L/max(L))

quad <- -(pi-0.4)^2/(2*0.4*0.6/10)
plot(pi, quad, type = "l", ylim = c(-5, 0),yaxt="n", col="red",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR, col="black")
grid(NA, 4, lwd = 1)
axis(2, at=seq(-5,0,1), las=2)
#title(main = "Figure 3. Quadratic approximation\n of binomial log-likelihood ratio\n 4 out of 10 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=2)

legend(x=0.17, y= -6.5 ,xpd = TRUE,  legend=c("logLR","Quadratic"), bty = "n",
       col=c("black","red"), lty=c(1,1), horiz = TRUE) #the legend is below the graph
```

2. $\hat\pi=0.4, n=1000, k=400$


```{r qua-apprx-bin400-1000, fig.asp=.7, fig.width=6, fig.cap='Quadratic approximation of binomial log-likelihood ratio 400 out of 1000 subjects', fig.align='center', out.width='90%', warning=FALSE, cache=TRUE, echo=FALSE}
par(mai = c(1.2, 0.5, 1, 0.7))
pi <- seq(0.35, 0.45, by=0.001)
L <- (pi^400)*((1-pi)^600)
logLR <- log(L/max(L))

quad <- -(pi-0.4)^2/(2*0.4*0.6/1000)
plot(pi, quad, type = "l", ylim = c(-4, 0),yaxt="n", col="red",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR, col="black")
grid(NA, 4, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Figure 4. Quadratic approximation\n of binomial log-likelihood ratio\n 400 out of 1000 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=2)

legend(x=0.37, y= -5.5 ,xpd = TRUE,  legend=c("logLR","Quadratic"), bty = "n",
       col=c("black","red"), lty=c(1,1), horiz = TRUE) #the legend is below the graph

```

3. $\hat\pi=0.01, n=100, k=1$

注意此圖中紅線提示的近似二次曲線，信賴區間的下限已經低於0，是無法接受的近似。

```{r qua-apprx-bin1-1000, fig.asp=.7, fig.width=6, fig.cap='Quadratic approximation of binomial log-likelihood ratio 1 out of 100 subjects', fig.align='center', out.width='90%', warning=FALSE, cache=TRUE, echo=FALSE}
par(mai = c(1.2, 0.5, 1, 0.7))
pi <- seq(0.0003, 0.04, by=0.000001)
L <- (pi^1)*((1-pi)^99)
logLR <- log(L/max(L))

quad <- -(pi-0.01)^2/(2*0.01*0.99/100)

plot(pi, quad, type = "l", ylim = c(-4, 0),yaxt="n", col="red",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR, col="black")
grid(NA, 4, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Figure 5. Quadratic approximation\n of binomial log-likelihood ratio\n 1 out of 100 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=2)

legend(x=0.007, y= -5.5 ,xpd = TRUE,  legend=c("logLR","Quadratic"), bty = "n",
       col=c("black","red"), lty=c(1,1), horiz = TRUE) #the legend is below the graph
```


4. $\hat\pi=0.01, n=1000, k=10$

```{r qua-apprx-bin10-1000, fig.asp=.7, fig.width=6, fig.cap='Quadratic approximation of binomial log-likelihood ratio 10 out of 1000 subjects', fig.align='center', out.width='90%', warning=FALSE, cache=TRUE, echo=FALSE}
par(mai = c(1.2, 0.5, 1, 0.7))
pi <- seq(0.0005, 0.02, by=0.000001)
L <- (pi^10)*((1-pi)^990)
logLR <- log(L/max(L))

quad <- -(pi-0.01)^2/(2*0.01*0.99/1000)

plot(pi, quad, type = "l", ylim = c(-4, 0),yaxt="n", col="red",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR, col="black")
grid(NA, 4, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Figure 6. Quadratic approximation\n of binomial log-likelihood ratio\n 10 out of 1000 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=2)

legend(x=0.004, y= -5.5 ,xpd = TRUE,  legend=c("logLR","Quadratic"), bty = "n",
       col=c("black","red"), lty=c(1,1), horiz = TRUE) #the legend is below the graph
```

5. $\hat\pi=0.01, n=10000, k=100$

```{r qua-apprx-bin100-1000, fig.asp=.7, fig.width=6, fig.cap='Quadratic approximation of binomial log-likelihood ratio 100 out of 10000 subjects', fig.align='center', out.width='90%', warning=FALSE, cache=TRUE, echo=FALSE}
par(mai = c(1.2, 0.5, 1, 0.7))
pi <- seq(0.007, 0.013, by=0.000001)
L <- (pi^100)*((1-pi)^9900)
logLR <- log(L/max(L))

quad <- -(pi-0.01)^2/(2*0.01*0.99/10000)

plot(pi, quad, type = "l", ylim = c(-4, 0),yaxt="n", col="red",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR, col="black")
grid(NA, 4, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Figure 7. Quadratic approximation\n of binomial log-likelihood ratio\n 100 out of 10000 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=2)

legend(x=0.0084, y= -5.5 ,xpd = TRUE,  legend=c("logLR","Quadratic"), bty = "n",
       col=c("black","red"), lty=c(1,1), horiz = TRUE) #the legend is below the graph
```


6. $\hat\pi=0.99, n=100, k=99$


注意此圖中紅線提示的近似二次曲線，信賴區間的上限已經大於1，和上面的 Figure 5. 一樣也是無法接受的近似。

```{r qua-apprx-bin99-100, fig.asp=.7, fig.width=6, fig.cap='Quadratic approximation of binomial log-likelihood ratio 99 out of 100 subjects', fig.align='center', out.width='90%', warning=FALSE, cache=TRUE, echo=FALSE}
par(mai = c(1.2, 0.5, 1, 0.7))
pi <- seq(0.96, 1.0, by=0.0001)
L <- (pi^99)*((1-pi)^1)
logLR <- log(L/max(L))

quad <- -(pi-0.99)^2/(2*0.01*0.99/100)

plot(pi, quad, type = "l", ylim = c(-4, 0),yaxt="n", col="red",
     frame.plot = FALSE, ylab = "", xlab = "\U03C0")
lines(pi, logLR, col="black")
grid(NA, 4, lwd = 1)
axis(2, at=seq(-4,0,1), las=2)
#title(main = "Figure 8. Quadratic approximation\n of binomial log-likelihood ratio\n 99 out of 100 subjects")
abline(h=-1.92, lty=1, col="red")
axis(4, at=-1.92, las=2)

legend(x=0.968, y= -5.5 ,xpd = TRUE,  legend=c("logLR","Quadratic"), bty = "n",
       col=c("black","red"), lty=c(1,1), horiz = TRUE) #the legend is below the graph
```



總結： 二次方程近似時，在二項分佈的情況下，隨着 $n, k$ 增加，近似越理想。


# 假設檢驗的構建 Construction of a hypothesis test


## 什麼是假設檢驗 Hypothesis testing {#null-and-alter}

一般來說，我們的**假設**(或者叫**假說**) 是對與我們實驗觀察數據來自的總體(或人羣) 的**概率分佈**的描述。在參數檢驗的背景下，就是要檢驗描述這個總體(或人羣) 的**概率分佈**的參數 (parameters)。最典型的情況是，我們提出兩個互補的假設，一個叫作**零假設**(或者叫**原假設**) ，null hypothesis ($H_0$)；另一個是與之對應的(互補的) 替代假設，althernative hypothesis ($H_1/H_A$)。

例如，若 $X$ 是一個服從二項分佈的隨機離散變量 $X\sim Bin(5, \theta)$。可以考慮如下的零假設和替代假設：$H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}$。

當建立了零假設和替代假設以後，假設檢驗就是要建立如下的規則以確定：

1. 從樣本中計算所得的參數估計值爲多少時，拒絕零假設。(接受替代假設爲“真”)
2. 從樣本中計算所得的參數估計值爲多少時，零假設不被拒絕。(接受零假設爲“真”)

注意：(這一段很繞)

上面的例子是零假設和替代假設均爲簡單假設的情況，實際操作中常常會設計更加複雜的(不對稱的) 假設：即簡單的 $H_0$，複雜的 $H_1$。如此一來當零假設 $H_0$ 不被拒絕時，我們並不一定就接受之。因爲無證據證明 $H_1$ 不等於有證據證明 $H_0$。_**(Absence of evidence is not evidence of absence).**_ 換句話說，無證據讓我們拒絕 $H_0$ 本身並不成爲支持 $H_0$ 爲“真”的證據。因爲在實際操作中，當我們設定的簡單的零假設沒有被拒絕，可能還存在其他符合樣本數據的零假設；相反地，當樣本數據的計算結果拒絕了零假設，我們只能接受替代假設。所以，反對零假設的證據，同時就是支持替代假設的證據。

在樣本空間 sample space 中，決定了零假設 $H_0$ 會被拒絕的子集 subset，被命名爲拒絕域 rejection region 或者 判別區域 critical region，用 $\mathfrak{R}$ 來標記。

## 錯誤概率和效能方程 error probabilities and the power function

這一部分也可以參考本書臨牀試驗樣本量計算 (Section \@ref(sample-size)) 部分。

```{r echo=FALSE, eval=FALSE}
library(knitr)
library(kableExtra)
dt <- read.csv("/home/ccwang/Documents/full-website-content/static/files/type12errorInfer.csv", header = T)
kable(dt, "html",align = "c",caption = "Definition of Type I and Type II error") %>%
  kable_styling(bootstrap_options = c("striped", "bordered"))  %>%
  collapse_rows(columns = c(1)) %>%
  add_header_above(c(" " = 2, "SAMPLE" = 2))
```


<table class="table table-striped table-bordered" style="margin-left: auto; margin-right: auto;">
<caption> 表 15.1 : Definition of Type I and Type II error</caption>
 <thead>
<tr>
<th style="border-bottom:hidden" colspan="2"></th>
<th style="text-align:center; border-bottom:hidden; padding-bottom:0; padding-left:3px;padding-right:3px;" colspan="2"><div style="border-bottom: 1px solid #ddd; padding-bottom: 5px;">SAMPLE</div></th>
</tr>
<tr>
<th style="text-align:center;">  </th>
   <th style="text-align:center;">  </th>
   <th style="text-align:center;"> $\underline{x} \notin \mathfrak{R}$ Accept $H_0$ </th>
   <th style="text-align:center;"> $\underline{x} \in \mathfrak{R}$ Reject $H_0$ </th>
  </tr>
</thead>
<tbody>
<tr>
<td style="text-align:center;vertical-align: middle !important;" rowspan="2"> TRUTH </td>
   <td style="text-align:center;"> $H_0$ is true </td>
   <td style="text-align:center;"> $\checkmark$ </td>
   <td style="text-align:center;"> $\alpha$ <br> Type I error </td>
  </tr>
<tr>
<td style="text-align:center;"> $H_1$ is true </td>
   <td style="text-align:center;"> $\beta$ <br> Type II error </td>
   <td style="text-align:center;"> $\checkmark$ </td>
  </tr>
</tbody>
</table>


假如一個假設檢驗是關於總體參數 $\theta$ 的：

$$H_0: \theta=\theta_0 \text{ v.s. } H_1: \theta=\theta_1 $$

這個檢驗的效能被定義爲當替代假設爲“真”時，拒絕零假設的概率(該檢驗方法能夠檢驗出有真實差別的能力) ：

$$\text{Power}=\text{Prob}(\underline{x}\in\mathfrak{R}|H_1\text{ is true}) = 1-\text{Prob}(\text{Type II error})$$

觀察數據只有兩種可能：落在拒絕域內，或者落在拒絕域之外。第二類錯誤我們常常使用 $\beta$ 來表示，所以 $\text{Power}=1-\beta$。

檢驗的顯著性水平用 $\alpha$ 來表示。$\alpha$ 的直觀意義就是，檢驗結果錯誤的拒絕了零假設 $H_0$，接受了替代假設 $H_1$，即假陽性的概率。

$$\text{Prob}(\underline{x}\in \mathfrak{R} |H_0 \text{ is true})=\text{Prob(Type I error)}$$

### 以二項分佈爲例

用本文開頭的例子： $X\sim Bin(5,\theta)$。和我們建立的零假設和替代假設：$H_0: \theta=\frac{1}{2}; H_1: \theta=\frac{2}{3}$：

考慮兩種檢驗方法：

1. A 方法：當且僅當5次觀察都爲“成功”時才拒絕 $H_0 (\text{i.e.}\; X=5)$。所以此時判別區域 $\mathfrak{R}$ 爲 $5$。檢驗效能 $\text{Power}=1-\beta$ 爲：$Prob(X=5|H_1 \text{ is true})=(\frac{2}{3})^5=0.1317$。顯著性水平 $\alpha$ 爲 $Prob(X=5|H_0 \text{ is true})=(\frac{1}{2})^5=0.03125$。
2. B 方法：當觀察到3,4,5次“成功”時，拒絕 $H_0 (\text{i.e.} X=3,4,5)$。此時判別區域  $\mathfrak{R}$ 爲 $3,4,5$。檢驗效能 $Power$ 爲：$Prob(X=3,4,\text{ or }5|H_1 \text{ is ture})=\sum_{i=3}^5(\frac{2}{3})^i(\frac{1}{3})^{5-i}\approx0.7901$；顯著性水平 $\alpha$ 爲：$Prob(X=3,4,5|H_0 \text{ is true})=\sum_{i=3}^5(\frac{1}{2})^i(\frac{1}{2})^{5-i}=0.5$

```{r}
# the power in test B
dbinom(3,5,2/3)+dbinom(4,5,2/3)+dbinom(5,5,2/3)
# the size in test B
dbinom(3,5,0.5)+dbinom(4,5,0.5)+dbinom(5,5,0.5)
```


比較上面兩種檢驗方法，可以看到，用B方法時，我們有更高的概率獲得假陽性結果(犯第一類錯誤，錯誤地拒絕 $H_0$，接受 $H_1$)，但是也有更高的檢驗效能 $1-\beta$(真陽性更高) 。這個例子就說明了，試圖提高檢驗效能的同時，會提高犯第一類錯誤的概率。實際操作中我們常常將第一類錯誤的概率固定，例如 $\alpha=0.05$，然後儘可能選擇檢驗效能最高的檢驗方法。

## 如何選擇要檢驗的統計量 {#Neyman-Pearson}

在上面的二項分佈的實驗中，“成功的次數” 是我們感興趣的要檢驗的統計量。但也可能是第一次出現 “成功” 之前的實驗次數，或者，任何與假設相關的統計量。相似的，如果觀察不是離散變量而是連續的，可以拿來檢驗的指標就有很多，如均值，中位數，衆數，幾何平均值等。

幸運地是，當明確了零假設和替代假設後，我們可以利用 [Neyman-Pearson lemma](https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma) 似然比公式^[區分與之前討論的對數似然比 (Section \@ref(llr))，之前討論的對數似然比指的是**所有的似然和極大似然**之間的比，此處的似然比只是純粹在探討兩個假設之間的似然比，**與極大似然無關**。]:

來決定使用哪個統計量做檢驗**最有效**：

$$\text{Neyman-Pearson lemma}=\frac{L_{H_0}}{L_{H_1}}$$

這公式很直觀，因爲當觀察數據更加支持 $H_1$ 時 ($L_{H_1}$ 更大)，$H_0$ 的可能性相對更小，就更應該被拒絕。而且，由於似然比越小，他的對數就越小，實際計算時我們常使用對數似然比：$\ell_{H_0}-\ell_{H_1}$。

問題來了，那到底要多小才算小？這個進入拒絕域的閾值由兩個指標來決定：

1. 被檢驗統計量的樣本分佈 (the sampling distribution of the test statistic)
2. 第一類錯誤概率 $\alpha$ (the required value of $\alpha$)

### 以已知方差的正態分佈爲例

假如已知 $X_1, \cdots, X_n \stackrel{i.i.d}{\sim} N(\mu, \sigma^2)$  而且方差 $\sigma^2$ 也是已知的。如果令 $H_0: \mu=5\; ;H_1: \mu=10$  可以通過如下的方法找到我們需要的最佳檢驗統計量 <u>best statistic</u> 根據之前的推導 (Section \@ref(llr)) 可知正態分佈的似然方程如下：

$$\ell(\mu|\underline{x}) =-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2$$

所以已知 $\sigma^2$ 時，我們的零假設和替代假設之間的對數似然比 $\ell_{H_0}-\ell_{H_1}$ 爲:

$$\ell_{H_0}-\ell_{H_1}=-\frac{1}{2\sigma^2}(\sum_{i=1}^n(x_i-5)^2-\sum_{i=1}^n(x_i-10)^2)$$

然而，我們只需要考慮隨着數據變化的部分，所以忽略掉不變的部分^[Rememer that $\ell_{H_0}-\ell_{H_1}$ is a random variable: the data varies **each time** we sample, with consequently varying relative support for the hypotheses, and so we are only interested in that part of  $\ell_{H_0}-\ell_{H_1}$ which depends on the results, the data, which vary with each sample (i.e. which contains the random part); the constant part provides no information on the relative support the data give to the hypotheses, so we ignore it.]：



$$
\begin{aligned}
\ell_{H_0}-\ell_{H_1} & = -(\sum_{i=1}^n(x_i-5)^2-\sum_{i=i}^n(x_i-10)^2)\\
                & = 75n - 2\times(10-5)\sum_{i=1}^nx_i \\
\end{aligned}
$$

所以只要樣本和 (sum of sample) $\sum_{i=1}^nx_i$ <u>(最佳統計量 best statistic)</u> 足夠大，零假設就會被拒絕。而且注意到最佳統計量可以乘以任何常數用作新的最佳統計量。爲了方便我們就用樣本均數 $\frac{1}{n}\sum_{i=1}^nx_i$ 作此處的最佳統計量。所以此時，我們的最佳檢驗就是當樣本均值足夠大，超過某個閾值時，我們拒絕零假設。而且，樣本均值的樣本分佈是可以知道的，這樣就便於我們繼續計算下一步：拒絕域 (判別區域) 。



## 複合假設 composite hypotheses

目前爲止我們討論的假設檢驗限制太多，實際操作時，我們多考慮類似如下的假設：

1.  $H_0: \theta=\theta_0 \;\text{v.s.}\; H_1: \theta>\theta_0$ [**單側**的替代假設]
2.  $H_0: \theta=\theta_0 \;\text{v.s.}\; H_1: \theta\neq\theta_0$ [**雙側**的替代假設]

所以我們面臨的問題是簡單假設中用於判定的最佳統計量，是始終如一地適用？我們一一來看：

### 單側替代假設

本章目前爲止的推導中我們發現，樣本均值越大，零假設和替代假設的對數似然比 $\ell_{H_0}-\ell_{H_1}$ 越小。所以我們在樣本均值較大時，拒絕零假設，那麼就可以把原來使用的簡單替代假設 $H_1: \mu=10$ 擴展爲，任意大於 $5$ 的 $\mu$ ，即 $\mu>5$ 。因爲大於 $5$ 的任何均值，都提供了更小的對數似然比，都會讓我們拒絕零假設。所以在正態分佈時，單側替代假設的最佳檢驗統計量還是**樣本均值**。

### 雙側替代假設

雙側替代假設的情況下，我們無法繼續使用樣本均值作爲最佳統計量。因爲當我們想檢驗：$H_0: \mu=5 \;\text{v.s.}\; H_1: \mu<5$ 時，必須獲得足夠小的樣本均值才能讓我們拒絕零假設。此處暫且先按下不表。

## 爲反對零假設 $H_0$ 的證據定量

重新再考慮複合假設：$H_0: \theta=\theta_0\;\text{v.s.}\;H_1: \theta>\theta_0$ 假如存在一個總是可用的最佳檢驗統計量，用 $T$ 來標記 (或 $T(x)$)， 這個統計量足夠大時，我們拒絕 $H_0$。 別忘了我們還要給事先固定好的顯著性水平 $\alpha$ 定義與之相關的判別區域：

$$\text{Prob}(\underline{x}\in\mathfrak{R}|H_0)=\alpha$$

如果我們知道 $T$ 的樣本分佈，我們就可以使用一個閾值 $c$ 來定義這個判別區域：

$$Prob(T\geqslant c|H_0)=\alpha$$

更加正式的，我們定義判別區域 $\mathfrak{R}$ 爲：

$$\{\underline{x}:\text{Prob}(T(x)\geqslant c|H_0)=\alpha\}$$

換句話說，當統計量 $T>c$ 時，我們拒絕 $H_0$ 。如果先不考慮拒絕或不拒絕的二元判定，我們可以用一個連續型測量值來量化反對零假設 $H_0$ 的證據。再考慮從觀察數據中獲得的 $T$ ，即數據告訴我們的 $t$ 。所以，當 $t$ 值越大，說明觀察值相對零假設 $H_0$ 越往極端的方向走。因此我們可以用 $T$ 的樣本分佈來計算觀察值大大於等於這個閾值(極端值) 時的概率：

$$p=\text{Prob}(T\geqslant t|H_0)$$

這個概率公式被稱爲是單側 $p$ 值 **(one-side p-value)**。單側 $p$ 值越小，統計量 $T$ 的樣本空間就有越小比例(越強) 的證據支持零假設 $H_0$。

我們把這以思想用到假設檢驗中時，就可以認爲：

$$p<\alpha \Leftrightarrow t>c$$

所以用我們一貫的設定 $\alpha=0.05$，所以如果計算獲得 $p<0.05$ 我們就認爲獲得了足夠強的拒絕零假設 $H_0$ 的證據。

### 回到正態分佈的均值比較問題上來(單側替代假設) {#normal-mean-compare}

繼續考慮 $X_1,\cdots,X_n\stackrel{i.i.d}{\sim} N(\mu, \sigma^2)$，假設 $\sigma^2=10$，我們要檢驗的是 $H_0: \mu=5 \;\text{v.s}.\; H_1: \mu>5$

1.  確定最佳檢驗統計量：已經證明過，單側替代假設的最佳檢驗統計量是**樣本均值 $\bar{x}$**。
2.  確定該統計量的樣本分佈：已知樣本均數的樣本分佈是 $\bar{X}\sim N(\mu,\sigma^2/n)$ 。<br>$\Rightarrow Z=\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim N(0,1)$，所以在 $H_0$ 條件下，$\Rightarrow Z=\frac{\bar{X}-5}{\sqrt{10}/\sqrt{n}} \sim N(0,1)$
3.  所以當一個檢驗的顯著性水平設定爲 $\alpha=0.05$ 時，我們用判別區域 $\mathfrak{R}$，使統計量據落在該判別區域內的概率爲 $0.05$：<br> $\text{Prob}(\bar{X}\geqslant c|H_0) = 0.05$ <br> 已知在標準正態分佈時，$\text{Prob}(Z\geqslant1.64)=0.05=\text{Prob}(\frac{\bar{X}-5}{\sqrt{10}/\sqrt{n}}\geqslant1.64)$
4.  假設樣本量是 $10$，那麼數據的判別區域 $\mathfrak{R}$ 就是 $\bar{X}\geqslant6.64$。
5.  假設觀察數據告訴我們，$\bar{X}=7.76$ 。那麼這一組觀察數據計算得到的統計量落在了判別區域內，就提供了足夠的證據拒絕接受 $H_0$。
6.  我們可以給這個觀察數據計算相應的單側 $p$ 值：<br> $p=\text{Prob}(\bar{X}\geqslant7.76|H_0)=\text{Prob}(Z+5\geqslant7.76)\\=\text{Prob}(Z\geqslant2.76)=0.003$ <br> 所以，觀察數據告訴我們，在 $H_0$ 的前提下，觀察值出現的概率是 $0.3\%$ 。即，在無數次**重複**取樣實驗中，僅有 $0.3\%$ 的結果可以給出支持 $H_0$ 的證據。因此我們拒絕 $H_0$ 接受 $H_1$。


## 雙側替代假設情況下，雙側 $p$ 值的定量方法


```{r assymmetric, echo=FALSE, fig.asp=.7, fig.width=5, fig.cap='Deliberately use an assymmetrical distribution to highlight the issues', fig.align='center', out.width='90%', cache=TRUE}
x <- rchisq(1000000, 5)
q100 <- quantile(x, 1)
q95 <- quantile(x, .95)
q05 <- quantile(x, 0.05)
q00 <- quantile(x,0)
dens <- density(x)
plot(dens, xlim=c(0,20), frame.plot = FALSE, main=" ", yaxs="i",
     ylim=c(0,0.18), xlab="")
x1 <- max(which(dens$x <= q05))
x2 <- min(which(dens$x > q00))
x3 <- min(which(dens$x >= q95))
x4 <- max(which(dens$x <  q100))
with(dens, polygon(x=c(x[c(x1,x1:x2,x2)]), y= c(0, y[x1:x2], 0), col="gray"))
with(dens, polygon(x=c(x[c(x3,x3:x4,x4)]), y= c(0, y[x3:x4], 0), col="gray"))

axis(1, at=c(1.17,3,11.17), labels = c(expression(t[obs]), expression(paste("E(T|",theta,")", sep = "")), expression(t^"'")))
```

此處故意使用一個左右不對稱的概率密度分佈來解釋。

現在的替代假設是雙側的：

$$H_0: \theta=\theta_0 \;\text{v.s.}\; H_1:  \theta\neq\theta_0$$

正常來說，雙側的假設檢驗應該分成兩個單側檢驗。即：

1. $H_1: \theta>\theta_0$;
2. $H_1: \theta<\theta_0$.

每個單側檢驗都有自己的最佳檢驗統計量。令 $T$ 是 1. 的最佳檢驗統計量，該統計量的樣本分佈如上圖 \@ref(fig:assymmetric) 所示(左右不對稱) 。假如觀察數據給出的統計量爲 $t_{\text{obs}}$，那麼在概率上反對零假設的情況可以有兩種：

1. $T\geqslant t_{\text{obs}}$ 其中， $\text{Prob}(T\geqslant t_{\text{obs}}|H_0)=\tilde p$;
2. $T\leqslant t^\prime$ 其中，$t^\prime$ 滿足： $\text{Prob}(T\leqslant t^\prime|H_0) =\tilde p$。(圖\@ref(fig:assymmetric))

所以概率密度分佈兩側的距離可以不對稱，但是只要左右兩側概率密度分佈的面積($=\tilde p$)相同，那麼就可以直接認爲，雙側 $p$ 值是兩側面積之和 ($p=2\times \tilde p$)，且觀察數據提供的統計量落在這兩個面積內的話，都足以提供證據拒絕零假設 $H_0$。


注意：

- 被選中的 $t^\prime$ 值大小不大可能滿足：$|t^\prime - E(T|\theta_0)|=|t_{obs}-E(T|\theta_0)|$。因爲那只有在完全左右對稱的分佈中才會出現。但是，此處我們關心的是面積左右兩邊的尾部要相等即可，所以我們只需要知道右半邊，較大的那個 $t_{obs}$ 就完全足夠了。

回到上面的均值比較問題 (Section \@ref(normal-mean-compare))。現在我們要進行雙側假設檢驗，即： $H_0: \mu=5 \text{ v.s. } H_1: \mu\neq5$，最佳統計量依然還是樣本均數 $\bar{X}$。數據告訴我們說 $\bar{X}=7.76$，因此雙側 $p$ 值就是將已求得的單側 $\tilde p$ 值乘以 $2$： $\text{two-sided } p=2\tilde p= 0.006$

當然，實際操作中我們很少進行這樣繁瑣的論證，多數情況下就直接報告雙側 $p$ 值。


## 假設檢驗構建之總結 {#test-summary}

按照如下的步驟一一構建我們的假設檢驗過程：

1. 先建立**零假設，和替代假設** (Section \@ref(null-and-alter))；
2. 定義**最佳檢驗統計量** (用 Neyman-Pearson lemma) (Section \@ref(Neyman-Pearson))；
3. 取得零假設條件下，最佳統計量的樣本分佈(通常都較爲困難，有時候我們會傾向於使用“不太理想”，但是計算較爲簡便的過程。) ；
4. 定義**拒絕域(判別區域) ** (常用 $\alpha=0.05$) ；
5. 計算**觀察數據**的檢驗統計量；
6. 如果觀察數據的檢驗統計量落在了提前定義好的拒絕域內，那麼我們的檢驗結論就是：觀察數據**拒絕了零假設支持替代假設**。然而在實際操作時，如果發現數據的檢驗統計量不在拒絕域內，我們僅僅只能下結論說：觀察數據**無法拒絕零假設**(**而不是接受零假設！**) ；
7. 報告計算得到的反對零假設的定量 $p$ 值。

作爲統計學家，我們的任務是評價數據提供的證據，而不是簡單的去接受或者拒絕一個假設。

## 練習題

### Q1

某種藥物有兩種使用方法：可以口服，也可以注射。兩種方法都被認爲可以使血漿中藥物濃度在24小時候達到相似的平均水平，$3 \mu \text{g/L}$。已知口服該藥物後，濃度的方差爲 $1$，而如果是注射的話方差只有 $1/4$。因此設計了一個口服臨牀實驗，觀察到24小時後血漿中藥物濃度數據爲：2.54, 0.93, 2.75, 4.51, 3.71, 1.62, 3.01, 4.13, 2.08, 3.33。假設這組觀察數據獨立同分佈 $\stackrel{i.i.d}{\sim} N(3, \sigma^2)$

1. 證明以下的假設的最佳檢驗統計量是 $\sum_{i=1}^{10}(x_i-3)^2$：
    $$H_0: \sigma^2=1/4 \text{ v.s. } H_1: \sigma^2=1$$

**解**

根據 Neyman-Pearson lemma (Section \@ref(Neyman-Pearson)) 來判斷最佳檢驗統計量：

下面用 $\sigma^2_0, \sigma^2_1$ 分別標記零假設和替代假設時的方差。

$$
\begin{aligned}
L(\sigma^2|\underline{x},\mu=3) &= \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}}\text{exp}(-\frac{1}{2}(\frac{x_i-3}{\sigma})^2) \\
\Rightarrow \ell(\sigma^2) &=-\frac{1}{2}\sum_{i=1}^n\text{log}\sigma^2-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-3)^2 \\
  &= -\frac{n}{2}\text{log}\sigma^2-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-3)^2 \\
\Rightarrow \ell(\sigma_0^2)-\ell(\sigma_1^2)&= \frac{n}{2}\text{log}\sigma_1^2+\frac{1}{2\sigma_1^2}\sum_{i=1}^n(x_i-3)^2\\
&\;\;\;\;\;\;-\frac{n}{2}\text{log}\sigma_0^2-\frac{1}{2\sigma_0^2}\sum_{i=1}^n(x_i-3)^2\\
&=\frac{n}{2}(\text{log}\sigma_1^2-\text{log}\sigma_0^2)+\frac{1}{2}(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})\sum_{i=1}^n(x_i-3)^2\\
&=\frac{n}{2}\text{log}\frac{\sigma_1^2}{\sigma_0^2}+\frac{1}{2}(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})\sum_{i=1}^n(x_i-3)^2
\end{aligned}
$$

觀察上面的式子就會發現，當實驗重複後唯一會發生變化的就是後面的 $\sum_{i=1}^n(x_i-3)^2$。
由於，$\sigma_0^2=1/4, \; \sigma_1^2=1$，所以 $(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})<0$。那麼當 $\sum_{i=1}^n(x_i-3)^2$ 越大，$\ell(\sigma_0^2)-\ell(\sigma_1^2)$ 就越小。因此，這就是我們尋找的最佳檢驗統計量。

2. 證明上面的檢驗統計量總是可以作爲最佳檢驗統計量，用於檢驗單側替代假設：$H_1: \sigma^2>1/4$。

上面的替代假設中 $\sigma_1^2=1$，如果將替代假設改成 $\sigma_1^2>1/4$，那麼 $(\frac{1}{\sigma_1^2}-\frac{1}{\sigma_0^2})<0$ 依然成立。所以，$\sum_{i=1}^n(x_i-3)^2$，或者這部分乘以任何一個不變的常數依然是替代假設爲 $H_1: \sigma^2>1/4$ 時的最佳檢驗統計量。

3. 在 $H_0$ 條件下，樣本分佈 $\sum_{i=1}^{10}(x_i-3)^2$ 是怎樣的分佈？利用這個分佈來定義顯著性水平爲 $\alpha=0.05$ 時的拒絕域。

在$H_0$ 條件下，有：
$$X_1,\cdots,X_n\stackrel{i.i.d}{\sim}N(3,1/4)\\
\Rightarrow \frac{X_i-3}{\sqrt{1/4}}\sim N(0,1)\\
\Rightarrow (\frac{X_i-3}{\sqrt{1/4}})^2 \sim \mathcal{X}_1^2\\
\Rightarrow \sum_{i=1}^{10}(\frac{X_i-3}{\sqrt{1/4}})^2 \sim \mathcal{X}_{10}^2\\
\Rightarrow 4\sum_{i=1}^{10}(X_i-3)^2\sim \mathcal{X}_{10}^2\\
\text{Let } T=\sum_{i=1}^{10}(X_i-3)^2\\
\Rightarrow 4T \sim \mathcal{X}_{10}^2$$

拒絕域被定義爲檢驗統計量取大於等於某個臨界值時概率爲 $0.05$，即 $\text{Prob}(T\geqslant t)=0.05$

$$\text{Prob}(4T\geqslant \mathcal{X}^2_{10,0.95})=0.05\\
\Rightarrow \text{Prob}(T\geqslant 1/4\mathcal{X}^2_{10,0.95})=0.05$$

所以，此處當顯著性水平定爲 $\alpha=0.05$ 時，拒絕域就是要大於自由度爲 $10$ 的卡方分佈的 $95\%$ 分位點。

4. 在 $H_0$ 條件下，該檢驗統計量的正態分佈模擬是怎樣的？

根據**中心極限定理**(Section \@ref(CLT)) 和 **卡方分佈的性質** (Section \@ref(chi-square-distribution))

$$n\rightarrow \infty, X_n^2\sim N(n, 2n)$$

所以近似地，

$$\mathcal{X}_{10}^2\sim N(\text{E}(\mathcal{X}_{10}^2)=10,\text{Var}(\mathcal{X}_{10}^2)=20)\\
\Rightarrow 4T\sim \text{approx} N(10,20)\\
\Rightarrow \frac{4T-10}{\sqrt{20}} \stackrel{\cdot}{\sim} N(0,1)$$


5. 用上面的正態分佈模擬，和觀察嘗試對單側替代假設作統計檢驗並依據所得結果作出結論：$$H_0: \sigma^2=1/4 \text{ v.s. } H_1: \sigma^2>1/4$$

用上面的正態分佈近似法，我們可以計算拒絕域：

$$\text{Prob}(\frac{4T-10}{\sqrt{20}}\geqslant Z_{0.95})=0.05$$

已知標準正態分佈的 $95\%$ 分位點取值 $1.64$，所以拒絕域：

$$\frac{4T-10}{\sqrt{20}}\geqslant 1.64\\
\Rightarrow T\geqslant1/4(10+1.64\sqrt{20})=1/4\times17.33$$

由觀察數據可得：$T=11.5$ ，所以觀察數據的檢驗統計量落在了拒絕域內。我們的結論是：觀察數據提供了極強的證據證明在顯著性水平爲 $5\%$ 時，口服該藥物24小時後的血漿藥物濃度的方差大於 $1/4$。


# 假設檢驗的近似方法

本章教你怎麼徒手搞似然比檢驗 (likelihood ratio test)，Wald 檢驗 (Wald test)，和 Score 檢驗 (Score test)。

## 近似和精確檢驗 approximate and exact tests

前一章描述了如何用對數似然比尋找最佳檢驗統計量 (Section \@ref(Neyman-Pearson))。一旦找到並確定了最佳檢驗統計量，接下去還需要確定這個最佳檢驗統計量的樣本分佈，用定好的顯著性水平($\alpha=0.05$)確定拒絕域，再使用觀察數據計算數據本身的統計量，然後對反對零假設的證據定量(計算 $p$ 值) 。前一章用的例子均來自於正態分佈，所以我們都能夠不太複雜地獲得樣本均值，樣本方差等較容易取得樣本分佈的檢驗統計量。正如我們在前一章最後部分 (Section \@ref(test-summary)) 總結的那樣，**大多數情況下我們沒有那麼幸運**。最佳檢驗統計量的樣本分佈會很難確定。所以另一個進行假設檢驗的途徑就是近似檢驗法 (approximate tests)。

## 精確檢驗法之 -- 似然比檢驗法 Likelihood ratio test

記得我們之前說到，簡單假設 $H_0: \theta=\theta_0\text{ v.s. } H_1: \theta=\theta_1$ 的檢驗的最佳檢驗統計量可以使用 Neyman-Pearson lemma (尼曼皮爾森輔助定理) (Section \@ref(Neyman-Pearson)) 來確定：

$$\ell_{H_0}-\ell_{H_1} = \ell(\theta_0)-\ell(\theta_1)$$

如果假設變成了複合型假設：$H_0: \theta\in\omega_0 \text{ v.s. } H_1: \theta\in\omega_1$。此時，$\omega_0, \omega_1$ 分別指兩種假設條件下我們關心的總體參數的可能取值範圍。那麼可以把上面的定理擴展成，在 $\omega_0, \omega_1$ 兩個取值範圍內，零假設和對立假設在給出的觀察數據條件下的極大似然之比：

$$\text{log}\frac{\text{max}_{H_0}[L(\theta|data)]}{\text{max}_{H_1}[L(\theta|data)]}=\text{max}_{H_0}[\ell(\theta|data)]-\text{max}_{H_1}[\ell(\theta|data)]\\
=\text{max}_{\theta\in\omega_0}[\ell(\theta|data)]-\text{max}_{\theta\in\omega_1}[\ell(\theta|data)]$$

典型的假設檢驗情況下，我們面對的是簡單的零假設和複合型的替代假設：

$$H_0: \theta=\theta_0 \text{ v.s. } H_1: \theta\neq\theta_0$$

所以在這個情況下，套用擴展以後的 Neyman-Pearson lemma：

$$\text{max}_{H_0}[\ell(\theta)]-\text{max}_{H_1}[\ell(\theta)]=\ell(\theta_0) - \ell(\hat\theta)=llr(\theta_0)$$

之前討論對數似然比 (Section \@ref(llr-chi)) 時我們已知：

$$\text{Under }H_0: \theta=\theta_0\Rightarrow -2llr(\theta_0)\stackrel{\cdot}{\sim}\mathcal{X}_1^2$$

於是利用自由度爲 $1$ 的卡方檢驗的特徵我們就可以爲反對零假設的證據定量，計算關鍵的拒絕域。如果說顯著性水平爲 $\alpha$ 那麼，我們拒絕零假設 $H_0:\theta=\theta_0$ 的拒絕域是：

$$-2llr(\theta_0)>\mathcal{X}^2_{1,1-\alpha}$$

當使用 $\alpha=0.05$ 時，這個關鍵的拒絕域就是：$-2llr(\theta_0)>3.84$。

這就是傳說中的 (對數) 似然比檢驗，(log-)Likelihood ratio test (LRT)。

LRT 的優點：

1. 簡單；
2. $p$ 值不會被參數尺度 (parameter scale) 左右，也就是說如果我們對參數進行了數學轉換 (Section \@ref(para-trans)) 也不會影響似然比檢驗計算得到的 $p$ 值大小。

LRT 的缺點：

1. 非正態分佈的數據時，LRT 只能算是漸進有效 (asymptotic valid)，即樣本量要足夠大時結果才能令人滿意；
2. 無法總是保證這是最佳檢驗統計量；
3. 需要計算兩次對數似然 (MLE 和 零假設時)。

## 練習題
假設有在觀察對象 $n=100$ 人中發生了 $k=40$ 個事件。假定數據服從二項分佈，已知人羣中每個人發生該事件的概率爲 $\pi_0=0.5$。嘗試計算似然比檢驗統計量：$-2llr(\pi_0)$，並進行顯著性水平爲 $\alpha=0.05$ 的假設檢驗：$H_0: \pi=\pi_0 \text{ v.s. }H_1: \pi\neq\pi_0$

**解**

$$
\begin{aligned}
&\because f(k=40|\pi) = \binom{100}{40}\pi^{40}(1-\pi)^{100-40} \\
&\text{Ignoring terms}  \text{ not with }  \pi \\
&\therefore \ell(\pi|k=40) = 40\text{log}\pi+60\text{log}(1-\pi) \\
&\Rightarrow \ell^\prime(\pi|k=40) = \frac{40}{\pi}-\frac{60}{1-\pi} \\
&\text{Let }   \ell^\prime(\pi|k=40) = 0 \\
&\Rightarrow   \frac{40}{\pi}-\frac{60}{1-\pi} =0 \\
&\Rightarrow  \text{ MLE } \hat\pi=0.4 \\
&\Rightarrow llr(\pi_0)=\ell(\pi_0)-\ell(\hat\pi) \\
&\;\;\;\;\;\;\;\;\;=40\text{log}0.5+60\text{log}(1-0.5)-40\text{log}0.4-60\text{log}(1-0.4)\\
&\;\;\;\;\;\;\;\;\;=-2.013\\
&\Rightarrow -2llr=4.026 > \text{Pr}(\mathcal{X}^2_{1,0.95})=3.84
\end{aligned}
$$

所以當顯著性水平爲 $\alpha=0.05$ 時，數據提供了足夠拒絕零假設的證據。該事件在此人羣中發生的概率要低於人羣的 $0.5$。

## 近似檢驗法之 -- Wald 檢驗 {#Wald}

和 LRT 一樣， Wald 檢驗也適用於檢驗 $H_0: \theta=\theta_0 \text{ v.s. } H_1: \theta\neq\theta_0$。但是本方法其實是使用對數似然比方程的近似二次方程 (Section \@ref(quadratic-llr))。相比之下，LRT 使用的是精確的對數似然比，只對檢驗統計量 $-2llr$ 進行了自由度爲 $1$ 的卡方分佈 $\mathcal{X}_1^2$ 近似。本節介紹的 Wald 檢驗過程中使用了兩次近似，一次是計算對數似然比時使用了二次方程，一次則是和 LRT 一樣對檢驗統計量進行 $\mathcal{X}_1^2$ 近似。

根據之前的對數似然比近似結論 (Section \@ref(quadratic-llr2)) ：

$$llr(\theta)\approx-\frac{1}{2}(\frac{M-\theta}{S})^2\text{ asymptotically}$$

其中，$M$ 是 $\text{MLE }\hat\theta$，$S=\sqrt{\left.-\frac{1}{\ell^{\prime\prime}(\theta)}\right\vert_{\theta=\hat{\theta}}}$

而且前一節我們也看到，

$$
\text{Under }H_0: \theta=\theta_0\Rightarrow -2llr(\theta_0) \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
\Rightarrow -2\times-\frac{1}{2}(\frac{M-\theta_0}{S})^2 \stackrel{\cdot}{\sim}\mathcal{X}_1^2 \\
\Rightarrow (\frac{M-\theta_0}{S}) \stackrel{\cdot}{\sim} N(0,1)\\
\text{Let } W=(\frac{M-\theta_0}{S})
$$

$W$ 就是我們在 Wald 檢驗中用到的檢驗統計量。接下來就可以計算給定顯著水平 $\alpha$ 時的拒絕域，給 $p$ 值定量：

當 $W>N(0,1)_{1-\alpha/2}$ 或 $W<N(0,1)_{\alpha/2}$時，拒絕 $H_0: \theta=\theta_0$；

或者，當 $W^2>\mathcal{X}^2_{1,1-\alpha}$ 時，拒絕 $H_0: \theta=\theta_0$。

這就是我們心心念念的 Wald 檢驗。

```{r llr-wald, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Likelihood ratio and Wald tests: solid (green) line is log-likelihood ratio, dashed (red) is quadratic approximation', fig.align='center', out.width='90%'}
knitr::include_graphics("img/Selection_083.png")
```
上圖 \@ref(fig:llr-wald) 解釋了 LRT 和 Wald 檢驗的不同之處。紅色虛線是二次方程，用於近似似然比方程(綠色實線) 。二者在 $\text{MLE}=\hat\theta$ 時同時取極大值。Wald 檢驗的是，數據提供的 $\hat\theta$ 和我們想要比較的零假設 $\theta_0$ 之間的橫軸差距。在檢驗量 $W$ 中我們還把這個差除以觀察數據均值的標準差(數據的標準誤) 。 如果數據本身波動大，$W$ 的分母(標準誤) 較大，那麼即使 $\hat\theta - \theta_0$ 保持不變，統計量變小，反對零假設的證據也就越小。反觀，LRT 檢驗的檢驗統計量就是上圖 \@ref(fig:llr-wald) 顯示的縱軸差 $\ell(\theta_0)-\ell(\hat\theta)$ 的大小。二者之間的關係被直觀的顯示在圖中。

Wald 檢驗優點：

1. 比 LRT 略簡單；
2. 不必再計算零假設時的對數似然，只需要 $MLE$ 和它的標準誤。

Wald 檢驗缺點：

1. 兩次近似(LRT只用了一次近似) ；
2. 無法總是保證這是最佳檢驗統計量；
3. 參數如果被數學轉換 (Section \@ref(para-trans))，$p$ 值會跟着變化。



### 再以二項分佈爲例

在 $n$ 個實驗對象中觀察到 $k$ 個事件，使用參數爲 $\pi$ 的二項分佈模型來模擬。使用 Wald 檢驗法對下列假設做出統計檢驗： $H_0: \pi=\pi_0 \text{ v.s. } H1: \pi\neq\pi_0$。將參數 logit 轉換 (log-odds) 之後，對轉換後的新參數再做一次 Wald 檢驗。

**解**

根據之前的二次方程近似法推導 (Section \@ref(quadratic-binomial-approx))：

$$
\begin{aligned}
& M=\text{MLE}=\hat\pi=\frac{k}{n}=p\\
& S=se(\hat\pi)=\sqrt{\frac{p(1-p)}{n}}\\
& \Rightarrow \text{Under } H_0: \pi=\pi_0\\
& W=(\frac{p-\pi_0}{\sqrt{\frac{p(1-p)}{n}}})\stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

根據參數數學轉換的性質 (Section \@ref(para-trans))

$$
\begin{aligned}
&\text{New parameter } \beta=g(\pi)=\text{logit}(\pi)=\text{log}\frac{\pi}{1-\pi}\\
& \text{MLE}=\text{logit}(\hat\pi)=\text{log}\frac{\hat\pi}{1-\hat\pi} \\
& \text{Here we need to use delta-method to approximate standard error of } g(\pi)\\
& S=se[g(\hat\pi)]\approx g^\prime(\pi)\times se(\hat\pi) \\
& = \frac{1}{\hat\pi(1-\hat\pi)}\sqrt{\frac{p(1-p)}{n}}\\
& =\sqrt{\frac{1}{k}+\frac{1}{n-k}} \\
& \text{So the Wald test becomes}\\
& H_0: \beta=\beta_0\\
& \Rightarrow W=\frac{\text{log}(\frac{\hat\pi}{1-\hat\pi})-\text{log}(\frac{\pi_0}{1-\pi_0})}{\sqrt{\frac{1}{k}+\frac{1}{n-k}}}\stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

可見對參數進行了數學轉換之後，檢驗統計量的計算式發生了變化。因此 $p$ 值也會不同。

## 近似檢驗法之 -- Score 检验

注意到 Wald 檢驗使用的近似二次方程是在 MLE， 也就是極大似然比時的點 $\hat\theta$ 和對數似然比方程取相同的值和相同曲率 (二次導數)。
可以類比的是，Score 检验是基于另一種二次方程模擬，Score 檢驗的近似二次方程和對數似然比方程在零假設 ($\theta_0$) 時取相同的曲率。所以，Score 檢驗使用的近似方程在 $\theta_0$ 時和對數似然比方程在相同位置時的傾斜度 (一階導數)，和曲率 (坡度的變化程度，二階導數) 相同。所以令 $U$ 爲對數似然比方程在 $\theta_0$ 時的坡度，定義 $V$ 是對數似然比方程在 $\theta_0$ 時的曲率的負數：

$$
\begin{aligned}
& U=\ell^\prime(\theta)|_{\theta=\theta_0}=\ell^\prime(\theta_0)\\
& V=-E[\ell^{\prime\prime}(\theta)]|_{\theta=\theta_0}=-E[\ell^{\prime\prime}(\theta_0)]
\end{aligned}
$$

注：此處的 $V=-E[l^{\prime\prime}(\theta_0)]$ 又常常被叫做 Expected Fisher information。

記得在 Wald 檢驗中使用的近似方程：
$$llr(\theta)\approx-\frac{1}{2}(\frac{M-\theta}{S})^2\text{ asymptotically}$$

令 $q(\theta)=-\frac{1}{2}(\frac{M-\theta}{S})^2$
就有：

$$
\begin{aligned}
& q^\prime(\theta)                      =\frac{M-\theta}{S^2}\\
& \Rightarrow q^\prime(\theta_0)        =\frac{M-\theta_0}{S^2}\\
& q^{\prime\prime}(\theta)              =-\frac{1}{S^2}\\
& \Rightarrow q^{\prime\prime}(\theta_0)=E[l^{\prime\prime}(\theta_0)]\\
& \Rightarrow \frac{1}{S^2}             =-E[l^{\prime\prime}(\theta_0)]\\
& q^\prime(\theta_0)                    = \frac{M-\theta_0}{S^2} = -E[l^{\prime\prime}(\theta_0)](M-\theta_0)\\
&                                       = \ell^\prime(\theta_0)\\
& \Rightarrow     M-\theta_0  = -\frac{\ell^\prime(\theta_0)}{E[l^{\prime\prime}(\theta_0)]}\\
& \Rightarrow     M  =  -\frac{\ell^\prime(\theta_0)}{E[l^{\prime\prime}(\theta_0)]}+\theta_0\\
& q(\theta)=-\frac{1}{2}(\frac{M-\theta}{S})^2=\frac{E[l^{\prime\prime}(\theta_0)]}{2}(-\frac{\ell^\prime(\theta_0)}{E[l^{\prime\prime}(\theta_0)]}+\theta_0-\theta)^2\\
& q(\theta)=-\frac{V}{2}(\frac{U}{V}+\theta_0-\theta)^2\\
& \Rightarrow \text{ Under } H_0: \theta=\theta_0\\
& \Rightarrow q(\theta_0)=-\frac{V}{2}(\frac{U}{V})^2=-\frac{U^2}{2V}\\
& \Rightarrow -2q(\theta_0)=\frac{U^2}{V} \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
& \text{Or equivalently} \frac{U}{\sqrt{V}} \stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

這就是 Score 檢驗時使用的檢驗統計量。相應的拒絕域就可以被定義爲：
當 $\frac{U^2}{V}>\mathcal{X}_{1,1-\alpha}^2$ 時，拒絕 $H_0$

如下面的示意圖 \@ref(fig:score-test) 所示，Score 檢驗，比較的是 $\theta_0$ 時的校正後似然方程的坡度 (一階導數/二階導數)，和極大似然時的坡度 (一階導數=0) 的差別。如果這個值越大，說明零假設時的似然和極大似然 (觀察數據的信息) 的距離越遠，拒絕零假設的證據就越有力。


```{r score-test, echo=FALSE, fig.asp=.7, fig.width=4, fig.cap='Score test: solid (green) line is log-likelihood ratio, dashed (red) is quadratic approximation', fig.align='center', out.width='90%'}
knitr::include_graphics("img/Selection_084.png")
```

Score 檢驗優點：

1. 比 LRT 簡單；
2. 不需要計算 MLE，只需要計算零假設時的對數似然比方程之坡度和曲率；
3. 在流行病學用到的檢驗方法中最常用，也最容易擴展 (Mantel-Haenszel test, log rank test, generalised linear models such as logistic, Poisson, Cox regressions)。

Score 檢驗缺點：

1. 和 Wald 檢驗一樣用到了兩次近似；
2. 無法總是保證這是最佳檢驗統計量；
3. 參數如果被數學轉換 (Section \@ref(para-trans))，$p$ 值會跟着變化。



### 再再以二項分佈爲例

$K\sim Bin(n, \pi)$ 假如已知人羣中事件發生的概率是 $\pi_0$。試推導此時的 Score 檢驗的檢驗統計量。

**解**

對二項分佈數據進行 Score 檢驗的時候我們需要計算 $U, V$，然後計算統計量 $\frac{U^2}{V}$ 和 $\mathcal{X}_1^2$ 比較即可。

$$
\begin{aligned}
& \text{Let } p=\frac{k}{n} \\
& \ell(\pi|k) = k\text{log}(\pi)+(n-k)\text{log}(1-\pi)\\
& \ell^\prime(\pi)=\frac{k}{\pi}-\frac{n-k}{1-\pi}=\frac{k-n\pi}{\pi(1-\pi)}\\
& = \frac{p-\pi}{\pi(1-\pi)/n}\\
& \Rightarrow U = \ell^\prime(\pi_0)=\frac{p-\pi_0}{\pi_0(1-\pi_0)/n}\\
& \ell^{\prime\prime}(\pi|K)=-\frac{K}{\pi^2}-\frac{n-K}{(1-\pi)^2}\\
& \Rightarrow -\ell^{\prime\prime}(\pi|K)=\frac{K}{\pi^2}+\frac{n-K}{(1-\pi)^2}\\
& \because E(K)=n\pi\\
& \therefore -E[\ell^{\prime\prime}(\pi|K)]=\frac{n\pi}{\pi^2}+\frac{n-n\pi}{(1-\pi)^2}\\
& =\frac{n}{\pi}+\frac{n}{1-\pi}=\frac{n}{\pi(1-\pi)}\\
& \text{ Under } H_0: \pi=\pi_0 \Rightarrow V=-E[\ell^{\prime\prime}(\pi_0)]=\frac{n}{\pi_0(1-\pi_0)}\\
& \Rightarrow \frac{U^2}{V}=\frac{(p-\pi_0)^2}{\pi_0(1-\pi_0)/n} \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
& \text{OR } \frac{U}{\sqrt{V}} = \frac{p-\pi_0}{\sqrt{\pi_0(1-\pi_0)/n}} \stackrel{\cdot}{\sim} N(0,1)
\end{aligned}
$$

## LRT, Wald, Score 檢驗三者的比較

1. LRT 比較的是對數似然方程在零假設 $H_0$ 和極大似然估計 (MLE) 時之間的縱軸差 (圖 \@ref(fig:llr-wald))；Wald 檢驗試圖直接比較 MLE 和 $H_0$ 的橫軸差 (二次方程近似法，並用標準誤校正) (圖 \@ref(fig:llr-wald))；Score 檢驗比較的是對數似然方程在 $H_0$ 時的切線斜率 (二次方程近似法，用曲率也就是二階導數校正) (圖 \@ref(fig:score-test))。三種檢驗比較的東西各不相同，但是這種差距大到進入拒絕域時，數據就會拒絕零假設。其中 Score 檢驗的計算過程最爲簡便，只需要計算 $H_0$ 時對數似然方程的一階和二階導數，而不用去計算 MLE，因此更多的被應用在流行病學數據計算中。

2. 如果對數似然方程本身就是左右對稱的 (正態分佈的情況下)，這三個檢驗方法計算的所有結果都是完全一致的。如果對數似然方程只是近似左右對稱，那麼三者的計算結果會十分接近。可以說，三種檢驗方法是漸進等價的。

3. 如果對參數進行了數學轉換，三者中只有 LRT 的計算結果保持不變。如果對參數的數學轉換使得對數似然方程更加接近左右對稱的二次方程，那麼 Wald 和 Score 檢驗的計算結果可以得到改善。

4. 如果說，MLE 和 零假設之間的差距很大，那麼 Wald 或者 Score 檢驗所使用的二次方程近似法的誤差會增加，此時傾向於使用 LRT 來進行精確檢驗。當然如果當樣本量較大，要檢驗的差距也很大，三種檢驗方案都能夠提供證據拒絕零假設 ($p$ 值都會很小)。

5. 如果三種檢驗方案給出的計算結果迥異，即使使用了數學轉換結果也沒有明顯改善的話，那麼最大的問題是樣本量太小。這時候還是老老實實用 LRT 吧。

6. 幾乎所有的參數檢驗都歸類與這章節介紹的三種檢驗方法。比如說 $Z$ 檢驗， $t$ 檢驗， $F$ 檢驗都是 LRT。在流行病學研究中最常用的還是 Score 檢驗。

我們的結論是，當條件允許的情況下，統計檢驗都推薦儘量使用精確檢驗 LRT。

## 練習題

### Q1

在對數似然比章節 (Section \@ref(llr-chi1))，我們曾經證明過，已知方差時：

$$
\begin{aligned}
& llr(\mu|\underline{x})=\ell(\mu|\underline{x})=-\frac{1}{2}(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2\\
& \Rightarrow -2llr(\mu|\underline{x})=-2\ell(\mu|\underline{x})=(\frac{\bar{x}-\mu}{\sigma/\sqrt{n}})^2
\end{aligned}
$$

當觀察數據 $X_1,\cdots,X_n\sim N(\mu,1^2)$ ，求 LRT, Wald, Score 三種檢驗方法對下列假設進行檢驗時的檢驗統計量：
$H_0: \mu=\mu_0 \text{ v.s. } H_1: \mu\neq\mu_0$

**解**

$$
\begin{aligned}
& \text{Model: } X_1, \cdots, X_n \stackrel{i.i.d}{\sim} N(\mu, 1)\\
& H_0: \mu=\mu_0 \text{ v.s. } H_1: \mu\neq\mu_0\\
& \text{Model } \Rightarrow \bar{X} \sim N(\mu, \frac{1}{n}) \\
& \text{If we observe } \bar{X} = \bar{x}\\
& \ell(\mu|\bar{x})=-\frac{1}{2}(\frac{\bar{x}-\mu}{1/\sqrt{n}})^2\\
& \textbf{For LRT, under } H_0: \mu=\mu_0 \Rightarrow -2llr(\mu_0) \stackrel{\cdot}{\sim}\mathcal{X}_1^2\\
& \Rightarrow \frac{\bar{x}-\mu}{1/\sqrt{n}} \sim N(0,1)\\
& \textbf{For Wald test, under } H_0: \mu=\mu_0 \Rightarrow \frac{M-\mu_0}{S}\sim N(0,1) \\
& \Rightarrow \frac{\bar{x}-\mu}{1/\sqrt{n}} \sim N(0,1)\\
& \textbf{For Score test, under } H_0: \mu=\mu_0 \Rightarrow U=\ell^\prime(\mu_0), V=-E[\ell^{\prime\prime}(\mu_0)]\\
& U=\ell^\prime(\mu_0)=(\frac{\bar{x}-\mu_0}{1/\sqrt{n}})\sqrt{n}=\frac{\bar{x}-\mu_0}{1/n}\\
& \ell^{\prime\prime}(\mu_0)=-\frac{1}{1/n}=-n \Rightarrow V=-E[n]=n\\
& \frac{U^2}{V}=(\frac{\bar{x}-\mu_0}{1/n})^2/n=(\frac{\bar{x}-\mu_0}{1/\sqrt{n}})^2\\
& \Rightarrow \frac{U^2}{V} \sim \mathcal{X}_1^2 \Rightarrow \frac{U}{\sqrt{V}}=\frac{\bar{x}-\mu_0}{1/\sqrt{n}} \sim N(0,1)
\end{aligned}
$$

**本題證明了，當數據服從正態分佈時，三種檢驗方法使用的檢驗統計量，是完全一致的。**

### Q2

根據醫生的觀察，某種癌症患者的生存時間服從平均值爲 $1/\beta_0$ 的指數分佈 (exponentially distributed)。有一種新藥物可以改善平均生存時間 (仍然服從指數分佈)。已知指數分佈的密度方程是：$f(x|\beta)=\beta \text{exp} (-\beta x), \text{ where } \beta, x>0$。

1. 證明指數分佈的均值是 $1/\beta$

**解**

$$
\begin{aligned}
& X\sim f(x|\beta), x>0 \Rightarrow E(X)=\int_0^\infty x\cdot f(x)\text{d} x = \int_0^\infty x\cdot \beta \cdot e^{-\beta x} \text{d}x\\
& E(x)= - \int_0^\infty x\cdot \frac{\text{d}e^{-\beta x}}{\text{d}x} \cdot \text{d}x\\
& \text{We can now integrate by parts, using } \int_a^b u \frac{\text{d}v}{\text{d}x} \text{d}x = [uv]_a^b-\int_a^b v \frac{\text{d}u}{\text{d}x} \text{d}x \\
& E(X) = -[x\cdot e^{-\beta x}]_0^\infty + \int_0^\infty e^{-\beta x} \text{d} x \\
& \;\;\;\; = -0+\int_0^\infty e^{-\beta x} \text{d} x\\
& \;\;\;\; = \int_0^\infty\frac{\text{d}}{\text{d}x} \frac{e^{-\beta x}}{-\beta} \text{d} x\\
& \;\;\;\; = [\frac{e^{-\beta x}}{-\beta}]_0^\infty = \frac{1}{-\beta}[0-1]=\frac{1}{\beta}
\end{aligned}
$$

2. 請寫下本題設定條件下的數學模型，零假設和替代假設

**解：** 假設患者人數爲 $n$，他們的生存時間爲相互獨立的隨機變量： $X_1,\cdots,X_n$。那麼本例中的數學模型爲：$\text{Model: } X_1,\cdots,X_n\stackrel{i.i.d}{\sim}f(x|\beta)=\beta e^{-\beta x}$。我們可以提出如下的零假設和替代假設：$H_0: \beta=\beta_0 \text{ v.s. } H_1: \beta\neq\beta_0$。

3. 推導此模型參數 $\beta$ 的極大似然估計 (MLE)，試使用似然比檢驗法來推導進行假設檢驗時使用的檢驗統計量。

**解**

$$
\begin{aligned}
& L(\beta|\underline{x}) = \prod_{i=1}^n f(x_i|\beta)=\prod_{i=1}^n\beta e^{-\beta x_i} \\
& \ell(\beta)=\sum_{i=1}^n\text{log}(\beta e^{-\beta x_i})=\sum\text{log}\beta-\sum\beta x_i=n\text{log}\beta-\beta\sum x_i \\
& \;\;\;\; = n\text{log}\beta-\beta n \bar{x} \\
& \Rightarrow \ell^\prime(\beta)=\frac{n}{\beta}-n\bar{x}\text{ MLE solves } \ell^\prime(\beta)=0 \text{ when }\ell^{\prime\prime}(\beta) < 0 \\
& \ell^\prime(\beta)=0 \Rightarrow \hat\beta=\frac{1}{\bar{x}}, \text{ and } \ell^{\prime\prime}(\beta)=-n\frac{1}{\beta^2} < 0\\
& \Rightarrow \text{ LRT test statistic: Under } H_0: \beta=\beta_0 \Rightarrow -2llr(\beta_0) \sim \mathcal{X}_1^2\\
& llr(\beta_0)=\ell(\beta_0)-\ell(\hat\beta)=n\text{log}\beta_0-\beta_0n\bar{x}-n\text{log}\hat\beta+\hat\beta n \bar{x}\\
& \text{ Substituting with MLE } \hat\beta=\frac{1}{\bar{x}}\\
& \;\;\;\;\;\;\;\;\;\; = n\text{log}\beta_0-\beta_0n\bar{x}+n\text{log}\bar{x}+ n\\
& \;\;\;\;\;\;\;\;\;\; = n(\text{log}\beta_0\bar{x}-\beta_0\bar{x}+1) \textbf{ this is the statistic for LRT}
\end{aligned}
$$

4. 推導  Score 和 Wald 檢驗法時的檢驗統計量

**解**

$$
\begin{aligned}
& \textbf{Score test: under } H_0 \Rightarrow \frac{U^2}{V}\sim \mathcal{X}_1^2 \text{ where } U=\ell^\prime(\beta_0), V=-E[\ell^{\prime\prime}(\beta_0)]\\
& \Rightarrow U=\frac{n}{\beta_0}-n\bar{x}; V = -E[-n\frac{1}{\beta_0^2}] = n\frac{1}{\beta_0^2} \\
& \Rightarrow \frac{U^2}{V}=(\frac{n}{\beta_0}-n\bar{x})^2\cdot\frac{\beta_0^2}{n} = (\frac{(\frac{n}{\beta_0}-n\bar{x})\beta_0}{\sqrt{n}})^2\\
& \;\;\;\;\;\;\;\;\; = n(1-\bar{x}\beta_0)^2\\
& \textbf{This is the statistic for Score test}\\
& \textbf{Wald test: under } H_0: \beta=\beta_0 \Rightarrow W=(\frac{M-\beta_0}{S})^2 \sim \mathcal{X}_1^2, \\
& \text{ where } M=\hat\beta=\frac{1}{\bar{x}}, \text{ and } S^2=-\frac{1}{\ell^{\prime\prime}(\hat\beta)}\\
& \ell^{\prime\prime}(\beta)=-n\frac{1}{\beta^2}\Rightarrow \ell^{\prime\prime}(\hat\beta)=-n\bar{x}^2\Rightarrow S^2=\frac{1}{n\bar{x}^2}\\
& \Rightarrow W=(\frac{M-\beta_0}{S})^2=\frac{(\frac{1}{\bar{x}}-\beta_0)^2}{\frac{1}{n\bar{x}^2}}=n(1-\beta_0\bar{x})^2\\
& \textbf{This is the statistic for Wald test}
\end{aligned}
$$

注意到在這個特例中， Score 和 Wald 檢驗的統計量竟然不謀而合。

5. 觀察5名患者，獲得診斷後的生存數據 (年)： $0.5,1,1.25,1.5,0.75$。用上面推導的統計量對這個數據進行假設檢驗：$H_0: \beta=0.5 \text{ v.s. } \beta\neq0.5$，你如何下結論？

**解**

$$
\begin{aligned}
&\text{Data: } x_1,\cdots,x_n=0.5,1,1.25,1.5,0.75. \Rightarrow \bar{x}=1\\
&H_0: \beta=0.5 \text{ v.s. } \beta\neq0.5\\
&\textbf{LRT test: } \\
& llr(\beta_0) = n(\text{log}\beta_0\bar{x}-\beta_0\bar{x}+1) = 5\times(\text{log}0.5-0.5\times1+1) = -0.966\\
&\Rightarrow -2llr=1.93 < \text{Prob}(\mathcal{X}^2_{1,0.95}) = 3.84 \\
& \text{There is no evidence that } \beta\neq0.5.\\
&\textbf{Score test: } \\
& \frac{U^2}{V} = n(1-\bar{x}\beta_0)^2 = 5\times(1-1\times0.5)^2=1.25 < \text{Prob}(\mathcal{X}^2_{1,0.95}) = 3.84 \\
& \text{There is no evidence that } \beta\neq0.5.\\
&\textbf{Wald test: } \\
& W=n(1-\beta_0\bar{x})^2=5\times(1-0.5\times1)^2=1.25< \text{Prob}(\mathcal{X}^2_{1,0.95}) = 3.84 \\
& \text{There is no evidence that } \beta\neq0.5.\\
\end{aligned}
$$

### Q3

隨機變量 $X_1,\cdots,X_n$ 互相獨立且在區間 $[0,\alpha]$ 內服從相同的恆定概率分佈 (identical uniform distribution)。
試着畫出參數 $\alpha$ 的似然方程示意圖。不進行任何數學計算，試着想象一下如果對 $\alpha$ 進行某種假設檢驗會出現什麼問題嗎？

# 誤差模型  Normal error
